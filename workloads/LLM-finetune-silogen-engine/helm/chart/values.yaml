### General chart values ### 
ioImage: ghcr.io/silogen/logistics:v0.1
finetuningImage: ghcr.io/silogen/rocm-silogen-finetuning-worker:v0.1

# Use these to add e.g. kueue labels
labels: []  
  # Example:
  # labels:
  #   - label: kueue.x-k8s.io/queue-name
  #     value: kaiwo

# Extra annotations such as an imagePullSecrets
imagePullSecrets: []
  # Example:
  # imagePullSecrets:
  #   - "regcred"

# Configure these to match the credentials in your cluster:
bucketStorageHost: 
bucketCredentialsSecret:
  name: bucket-storage-credentials
  accessKeyKey: access_key
  secretKeyKey: secret_key

# Resources:
downloadsReservedSize: 64Gi
checkpointsReservedSize: 512Gi
storageClass:  # Optionally set this to use a specific storageClass for the storage.
finetuningGpus: 1 # 1 or 8, values in between are not robust at the moment

# Runtime configuration:
distributedType: "auto-deepspeed-stage1"  # Alternatives: "auto-single-process", "custom-accelerate-config"
customAccelerateConfig:  # Can contain an accelerate config in YAML format, used with distributed_type: "custom-accelerate-config".
mergeAdapter: true

### Model input and output, required args ###
checkpointsRemote:
basemodel:

### Finetuning config section ###
finetuning_config:
  method: sft
  data_conf:
    training_data:
      type: CONCATENATION # Alternative: PRECOMPUTE_WEIGHTED_MIX
      datasets: [] 
    validation_data:
      type: AUTO_SPLIT # Alternatives: CONCATENATION, NONE
      ratio: 0.1
    chat_template_name: "keep-original"  # Alternatives: "simplified-llama31" / "chat-ml"
    missing_pad_token_strategy: "bos-repurpose"  # Alternatives: "unk-repurpose"
  training_args: {}
    # training_args is like Huggingface TrainingArguments, but certain inputs are not allowed:
    # - Do not set output_dir, the checkpoints should be saved to the default location to be uploaded
    # - Do not set per_device_train_batch_size nor gradient_accumulation, they are automatically set
    #   based on the batchsize_conf (below)
  batchsize_conf:
    total_train_batch_size:  # By default, this gets set to the number of GPUs
    max_per_device_train_batch_size: 1
    per_device_eval_batch_size: # By default is the same as per_device_train_batch_size
  peft_conf:
    peft_type: NO_PEFT  # This can be any HF peft_type or NO_PEFT.
    # If setting peft_type for any actual PEFT, also set:
    # task_type: "CAUSAL_LM", or some other task type, but usually this.
    # peft_kwargs:, the huggingface kwargs for the that PEFT model type
  quant_conf:
    quantization_type: no-quantization  # This can also be bitsandbytes
    # If bitsandbytes, you can also set any keys from BitsAndBytesConfig here
  run_conf:
    model: /local_resources/basemodel
    model_args: {}  # These are HuggingFace model .from_pretrained() kwargs
    resume_from_checkpoint: auto
  sft_args:
    max_seq_length: 2048

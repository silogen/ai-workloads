apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "release.fullname" . }}
  {{- if .Values.metadata.labels }}
  labels:
    {{- range $key, $value := .Values.metadata.labels }}
    {{ $key }}: {{ $value | quote }}
    {{- end }}
  {{- end }}
spec:
  ttlSecondsAfterFinished: 3600
  backoffLimit: {{ default 0 .Values.backOffLimit }}
  template:
    spec:
      restartPolicy: Never
      volumes:
        {{- include "container.volumes" . | nindent 8 }}
      initContainers:
        - name: download
          image: "{{ .Values.logistics.image }}"
          imagePullPolicy: Always
          command: ["sh", "-euc"]
          args:
            - |
              # Setup MinIO, Download resources:
              mc alias set minio-host $${BUCKET_STORAGE_HOST} $${BUCKET_STORAGE_ACCESS_KEY} $${BUCKET_STORAGE_SECRET_KEY};
              echo "Copying data to container...";
              mc cp -r minio-host/'{{ .Values.remoteDataDirPath | trimSuffix "/" | replace  "'" "'\\''"  }}'/'{{ .Values.remoteDataNamePrefix }}' /local-resources/data;
              echo "Copying tokenizer to container...";
              mc cp -r minio-host/'{{ .Values.remoteTokenizerPath | trimSuffix "/" | replace  "'" "'\\''"  }}'/ /local-resources/tokenizer;
              echo "Copying model checkpoint to container...";
              {{- $remotePath := printf "minio-host/'%s'/" (.Values.remoteCheckpointsPath | trimSuffix "/" | replace  "'" "'\\''") }}
              if last_ckpt=$(mc cat {{ $remotePath }}/latest_checkpointed_iteration.txt); then
                last_ckpt=$(printf 'iter_%07d' "$last_ckpt")
                echo "Found checkpoint at iteration $last_ckpt. Downloading ..."
                mc mirror {{ $remotePath }}/$last_ckpt/ /local-resources/basemodel/$last_ckpt
                mc cp {{ $remotePath }}/latest_checkpointed_iteration.txt /local-resources/basemodel/latest_checkpointed_iteration.txt
              else
                echo "No checkpoints found yet. Downloading basemodel ..."
                mc cp -r minio-host/'{{ .Values.remoteBaseModelPath | trimSuffix "/" | replace  "'" "'\\''" }}'/ /local-resources/basemodel;
              fi
          resources:
            limits:
              memory: "1Gi"
              cpu: "1"
            requests:
              memory: "1Gi"
              cpu: "1"
          env:
            {{- include "logistics.container.env" . | nindent 12 }}
          volumeMounts:
            - name: ephemeral-storage
              mountPath: /local-resources

        - name: upload
          image: "{{ .Values.logistics.image }}"
          imagePullPolicy: Always
          restartPolicy: Always
          command: ["sh", "-euc"]
          args:
            - |
              bash /local-resources/mount/checkpoints-upload.sh {{ .Values.remoteCheckpointsPath | trimSuffix "/" }}
          resources:
            limits:
              memory: "1Gi"
              cpu: "1"
            requests:
              memory: "1Gi"
              cpu: "1"
          env:
            {{- include "logistics.container.env" . | nindent 12 }}
          volumeMounts:
            - name: ephemeral-storage
              mountPath: /local-resources
            - name: workload-mount
              mountPath: /local-resources/mount

      containers:
        - name: training
          image: "{{ .Values.mainWorkloadImage }}"
          imagePullPolicy: Always
          command: ["sh", "-euc"]
          args:
            - |
              # Print GPU Info:
              rocm-smi;

              git apply /local-resources/mount/Megatron-LM.patch;

              # Run training:
              {{- range $k, $v := .Values.pretrainingSettings }}
              {{ $k }}={{ $v }} \
              {{- end }}
              LOAD_CKPT_PATH=/local-resources/basemodel \
              DATA_PATH=/local-resources/data/{{ .Values.remoteDataNamePrefix }} \
              TOKENIZER_MODEL=/local-resources/tokenizer \
              SAVE_CKPT_PATH=/local-resources/checkpoints \
              bash /local-resources/mount/{{ .Values.pretrainingScript }} \
              {{- range .Values.megatronArguments }}
              {{ . }} \
              {{- end }}
              ;

              # Exchange signals with upload container to wrap up the run
              touch /local-resources/done_training;
              while [ ! -f /local-resources/done_uploading ]; do
                sleep 15
              done
          resources:
            {{- with .Values.resources }}
            limits:
              memory: "{{ .memory }}"
              cpu: "{{ .cpu }}"
              amd.com/gpu: "{{ .gpu }}"
            requests:
              memory: "{{ .memory }}"
              cpu: "{{ .cpu }}"
              amd.com/gpu: "{{ .gpu }}"
            {{- end }}
          env:
            - name: PYTORCH_HIP_ALLOC_CONF # to avoid OOM errors caused by Pytorch memory fragmentation ref: https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management
              value: "expandable_segments:True"
            - name: HIP_VISIBLE_DEVICES
              value: "{{ int .Values.resources.gpu | until | join "," }}"
          volumeMounts:
            - name: dshm # Increase SHM size for the container by mounting /dev/shm, for Pytorch parallel processing
              mountPath: /dev/shm
            - name: ephemeral-storage
              mountPath: /local-resources
            - name: workload-mount
              mountPath: /local-resources/mount

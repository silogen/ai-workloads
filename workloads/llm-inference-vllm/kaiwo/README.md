# LLM Inference with vLLM

> [!NOTE]
> This guide demonstrates how to deploy the LLM Inference vLLM workload using [Kaiwo](https://github.com/silogen/kaiwo), a Kubernetes-native AI Workload Orchestrator designed to accelerate GPU workloads, and has been tested to work with Kaiwo v0.0.5.

## Prerequisites

Ensure the following prerequisites are met before deploying any of the workloads:

1. **Kaiwo CLI**: Install the Kaiwo CLI tool. Refer to the [Kaiwo Installation Guide](https://github.com/silogen/kaiwo) for instructions.
2. **Secret**: A secret named `hf-token` must exist in the namespace.

## Deploying the Workloads

See the `README.md` in the respective workload directories.

metadata:
  labels:
    pipeline_tag: ""
    chat: "true"

model: "mistralai/Mistral-Large-Instruct-2411"
served_model_name: "mistralai/Mistral-Large-Instruct-2411"
gpus: 2

env_vars:
  HF_TOKEN:
    key: hf-token
    name: hf-token

storage:
  ephemeral:
    quantity: 512Gi  # observed peak usage: ~457Gi (2 copies of the model)
    storageClassName: mlstorage
    accessModes:
      - ReadWriteOnce
  dshm:
    sizeLimit: 64Gi

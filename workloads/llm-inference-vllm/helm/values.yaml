metadata:
  labels: {}

image: "rocm/vllm-dev:20241205-tuned"
imagePullPolicy: Always

model: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
gpus: 1
memory_per_gpu: 64  # Gi
cpu_per_gpu: 4

# vllm engine args (ref: https://docs.vllm.ai/en/latest/serving/engine_args.html)
vllm_engine_args: {}
  # tensor-parallel-size: "{{ .Values.gpus }}"  # taken care of by the template
  # model: "{{ .Values.model }}"  # taken care of by the template
  # served-model-name: "{{ .Values.model }}"
  # enforce-eager: "true"
  # gpu-memory-utilization: 0.9

# env vars (vllm ref: https://docs.vllm.ai/en/latest/serving/env_vars.html)
env_vars:
  VLLM_DO_NOT_TRACK: "1"
  BUCKET_STORAGE_HOST: http://minio.minio-tenant-default.svc.cluster.local:80
  BUCKET_STORAGE_ACCESS_KEY:
    name: minio-credentials
    key: minio-access-key
  BUCKET_STORAGE_SECRET_KEY:
    name: minio-credentials
    key: minio-secret-key
  HF_HOME: /workload/.cache/huggingface
  # HF_TOKEN:
  #   key: hf-token
  #   name: hf-token

storage:
  ephemeral:
    quantity: 100Gi
    storageClassName: mlstorage
    accessModes:
      - ReadWriteOnce
  dshm:
    sizeLimit: 32Gi

deployment:
  port: 8080

metadata:
  labels: {}
  project_id: silogen
  user_id: user
  workload_id: # defaults to the release name

image: "rocm/vllm:rocm6.4.1_vllm_0.9.0.1_20250605"
imagePullPolicy: Always
replicas: 1

model: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
gpus: 1
memory_per_gpu: 64 # Gi
cpu_per_gpu: 4

# vllm engine args (ref: https://docs.vllm.ai/en/latest/serving/engine_args.html)
# allows flag arguments to be passed to the cli by leaving empty. for example:
# example-flag:
vllm_engine_args:
  # tensor-parallel-size: "{{ .Values.gpus }}"  # taken care of by the template
  # model: "{{ .Values.model }}"  # taken care of by the template
  # served-model-name: "{{ .Values.model }}"
  # enforce-eager: "true"
  gpu-memory-utilization: "0.95"
  distributed_executor_backend: "mp"
  no-enable-chunked-prefill:

# env vars (vllm ref: https://docs.vllm.ai/en/latest/serving/env_vars.html)
env_vars:
  BUCKET_STORAGE_HOST: http://minio.minio-tenant-default.svc.cluster.local:80
  BUCKET_STORAGE_ACCESS_KEY:
    name: minio-credentials
    key: minio-access-key
  BUCKET_STORAGE_SECRET_KEY:
    name: minio-credentials
    key: minio-secret-key
  HF_HOME: /workload/.cache/huggingface

  # vLLM optimized settings: https://blog.vllm.ai/2024/10/23/vllm-serving-amd.html
  VLLM_DO_NOT_TRACK: "1"
  VLLM_USE_V1: "0"
  VLLM_USE_TRITON_FLASH_ATTN: "0" # Use Composable Kernel (CK) Flash Attention instead of Triton Flash Attention
  HIP_FORCE_DEV_KERNARG: "1"
  NCCL_MIN_NCHANNELS: "112"
  TORCH_BLAS_PREFER_HIPBLASLT: "1"
  PYTORCH_TUNABLEOP_ENABLED: "1"
  PYTORCH_TUNABLEOP_VERBOSE: "1"
  PYTORCH_TUNABLEOP_TUNING: "0"

storage:
  ephemeral:
    quantity: 256Gi
    storageClassName: mlstorage
    accessModes:
      - ReadWriteOnce
  dshm:
    sizeLimit: 32Gi

deployment:
  ports:
    http: 8080

# startupProbe - checks if the container has successfully started. It disables liveness and readiness probes until it succeeds, useful for slow-starting applications.
# livenessProbe - checks if the container is still alive. If it fails, Kubernetes restarts the container to recover from failure.
# readinessProbe - checks if the container is ready to serve traffic. If it fails, the container is removed from the service's endpoints but remains running.
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
startupProbe:
  httpGet:
    path: /v1/models
    port: http
  periodSeconds: 10
  failureThreshold: 360 # 360 x 10s => allow for 60 minutes startup time
livenessProbe:
  httpGet:
    path: /health
    port: http
readinessProbe:
  httpGet:
    path: /v1/models
    port: http

# kaiwo settings (if enabled, use kaiwo CRDs to have kaiwo operator manage the workload)
kaiwo:
  enabled: false

http_route:
  enabled: false

ingress:
  enabled: false

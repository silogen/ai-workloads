# general
name: "llm-inference-vllm"
labels: {}

# image/model
image: "rocm/vllm-dev:20241205-tuned"
model: "s3://default-bucket/meta-llama/Llama-3.1-8B-Instruct"
gpus: 1

# vllm engine args (ref: https://docs.vllm.ai/en/latest/serving/engine_args.html)
vllm_engine_args:
  # tensor-parallel-size: "{{ .Values.gpus }}"
  # model: "{{ .Values.model }}"
  # served-model-name: "{{ .Values.model }}"
  # enforce-eager: "true"
  # gpu-memory-utilization: 0.9

# vllm env vars (ref: https://docs.vllm.ai/en/latest/serving/env_vars.html)
env_vars:
  VLLM_DO_NOT_TRACK: 1

# secrets: bucket storage
bucket_storage:
  host: https://default-minio-tenant-hl.minio-tenant-default.svc.cluster.local:9000
  credentials:
    secret_name: minio-credentials
    secret_key_key: minio-secret-key
    access_key_key: minio-access-key

# secrets: huggingface token
hf_token:
  secret_name: hf-token
  key: hf-token

# storage
storage:
  quantity: "100Gi"
  storageClassName: "longhorn"
dshm:
  sizeLimit: "200Gi"

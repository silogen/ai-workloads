### Model ###
modelName: "Qwen/Qwen2-7B-Instruct"

### Data ###
dataset: "gsm8k"

# Resources:
checkpointsReservedSize: 512Gi
storageClass: mlstorage
finetuningGpus: 2
memoryPerGpu: 64
cpusPerGpu: 8

### Model output path ###
checkpointsRemote: "default-bucket/experiments/Qwen2_7B_Instruct_PPO_gsm8k_verl"

verlConfig:
  data:
    train_batch_size: 1024
    max_prompt_length: 1024
    max_response_length: 512
  actor_rollout_ref:
    model:
      use_remove_padding: True
      enable_gradient_checkpointing: True
    actor:
      ppo_micro_batch_size_per_gpu: 16
    rollout:
      log_prob_micro_batch_size_per_gpu: 40
      tensor_model_parallel_size: 2
      gpu_memory_utilization: 0.6
    ref:
      log_prob_micro_batch_size_per_gpu: 40
      fsdp_config:
        param_offload: True
  critic:
    optim:
      lr: 1e-5
    model:
      use_remove_padding: True
      enable_gradient_checkpointing: True
    ppo_micro_batch_size_per_gpu: 32
    fsdp_config:
      param_offload: False
      optimizer_offload: False
  algorithm:
    kl_ctrl:
      kl_coef: 0.001
  trainer:
    total_epochs: 10

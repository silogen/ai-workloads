# Values file for running bertscore evaluation for Llama-3.2-3B on the CNN-Dailymail summarization dataset
# Publishes results (incl. score distributions) to MLflow.
# Note that an up and running MLFlow service needs to be set up by the user.
general:
  job_name: evaluation-metrics-llama-3-2-3b-mlflow
model_inference_container:
  gpu_count: 1
  model: Llama-3.2-3B
  model_path: hf://meta-llama/Llama-3.2-3B
evaluation_container:
  image: ghcr.io/silogen/evaluation-workloads-metrics:v0.1
  use_data_subset: 0 # Edit this for quick testing. Defines the number of documents used for evaluation. 0 means using the whole dataset
storage:
  mlflow:
    server_uri: http://xxx.xxx.xxx.xxx:<port> #replace with your MLflow server URI
    experiment_name: evaluation-metrics
    run_name: metrics-llama-3-2-3b

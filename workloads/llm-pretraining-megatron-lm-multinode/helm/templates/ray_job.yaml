apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: {{ include "release.fullname" . }}
  {{- if .Values.metadata.labels }}
  labels:
    {{- range $key, $value := .Values.metadata.labels }}
    {{ $key }}: {{ $value | quote }}
    {{- end }}
  {{- end }}
spec:
  backoffLimit: 0
  shutdownAfterJobFinishes: true
  submissionMode: K8sJobMode
  ttlSecondsAfterFinished: 3600
  entrypoint: >
    python /local_resources/mount/ray_entrypoint.py --num-nodes {{ .Values.workers.replicas }} --gpus-per-node {{ .Values.workers.resources.gpu }}
    {{- range $index, $arg := .Values.megatron_arguments }}
    {{- $arg | nindent 4 }}
    {{- end }}
  rayClusterSpec:
    enableInTreeAutoscaling: false
    headGroupSpec:
      rayStartParams:
      {{- .Values.workers.start_params | default dict | toYaml | nindent 8 }}
      template:
        spec:
          restartPolicy: Never
          volumes:
            {{- include "container.volumes" . | nindent 12 }}
          containers:
            - image: "{{ .Values.main_workload_image }}"
              imagePullPolicy: Always
              name: ray-head
              env:
                {{- include "worker.container.env" . | nindent 14 }}
              - name: RAY_ENABLE_RECORD_ACTOR_TASK_LOGGING
                value: "1"
              - name: PYTORCH_HIP_ALLOC_CONF
                value: "expandable_segments:True"
              - name: HIP_VISIBLE_DEVICES
                value: "{{ int .Values.workers.resources.gpu | until | join "," }}"
              resources:
              {{- with .Values.workers.resources }}
                limits:
                  memory: "{{ .memory }}"
                  cpu: "{{ .cpu }}"
                  amd.com/gpu: "{{ .gpu }}"
                requests:
                  memory: "{{ .memory }}"
                  cpu: "{{ .cpu }}"
                  amd.com/gpu: "{{ .gpu }}"
              {{- end }}
              volumeMounts:
              - name: dshm
                mountPath: /dev/shm
              - name: ephemeral-storage
                mountPath: /local_resources
                readOnly: false
              - mountPath: /local_resources/mount
                name: workload-mount

          initContainers:
          - name: setup
            image: "{{ .Values.logistics.image }}"
            imagePullPolicy: Always
            command: ["sh", "-euc"]
            args:
              - |
                # Handle cleanup of kubernetes resources: ConfigMap, PVC
                bash /local_resources/mount/gc.sh {{ include "release.fullname" . }}
                # Setup MinIO, Download resources:
                mc alias set minio-host $${BUCKET_STORAGE_HOST} $${BUCKET_STORAGE_ACCESS_KEY} $${BUCKET_STORAGE_SECRET_KEY};
                echo "Copying data to container...";
                mc cp -r minio-host/{{ .Values.remote_data_dir_path | trimSuffix "/" }}/{{ .Values.remote_data_name_prefix }} /local_resources/data;
                echo "Copying tokenizer to container...";
                mc cp -r minio-host/{{ .Values.remote_tokenizer_path | trimSuffix "/" }}/ /local_resources/tokenizer;
                echo "Copying model checkpoint to container...";
                mc cp -r minio-host/{{ .Values.remote_base_model_path | trimSuffix "/" }}/ /local_resources/basemodel;
            resources:
              limits:
                memory: "1Gi"
                cpu: "1"
              requests:
                memory: "1Gi"
                cpu: "1"
            env:
              {{- include "logistics.container.env" . | nindent 14 }}
            volumeMounts:
              - name: ephemeral-storage
                mountPath: /local_resources
              - name: workload-mount
                mountPath: /local_resources/mount

          - name: upload
            image: "{{ .Values.logistics.image }}"
            imagePullPolicy: Always
            restartPolicy: Always
            command: ["sh", "-euc"]
            args:
              - |
                bash /local_resources/mount/checkpoints_upload.sh {{ .Values.remote_checkpoints_path | trimSuffix "/" }}
            resources:
              limits:
                memory: "1Gi"
                cpu: "1"
              requests:
                memory: "1Gi"
                cpu: "1"
            env:
              {{- include "logistics.container.env" . | nindent 14 }}
            volumeMounts:
              - name: ephemeral-storage
                mountPath: /local_resources
              - name: workload-mount
                mountPath: /local_resources/mount

    workerGroupSpecs:
    - groupName: "ray-worker"
      maxReplicas: {{ sub .Values.workers.replicas 1 }}
      minReplicas: {{ sub .Values.workers.replicas 1 }}
      replicas: {{ sub .Values.workers.replicas 1 }}
      rayStartParams:
      {{- .Values.workers.start_params | default dict | toYaml | nindent 8 }}
      numOfHosts: 1
      template:
        spec:
          volumes:
            {{- include "container.volumes" . | nindent 12 }}
          containers:
          - name: ray-worker
            env:
              {{- include "worker.container.env" . | nindent 12 }}
            - name: RAY_ENABLE_RECORD_ACTOR_TASK_LOGGING
              value: "1"
            - name: PYTORCH_HIP_ALLOC_CONF
              value: "expandable_segments:True"
            - name: HIP_VISIBLE_DEVICES
              value: "{{ int .Values.workers.resources.gpu | until | join "," }}"
            image: "{{ .Values.main_workload_image }}"
            imagePullPolicy: Always
            resources:
            {{- with .Values.workers.resources }}
              limits:
                memory: "{{ .memory }}"
                cpu: "{{ .cpu }}"
                amd.com/gpu: "{{ .gpu }}"
              requests:
                memory: "{{ .memory }}"
                cpu: "{{ .cpu }}"
                amd.com/gpu: "{{ .gpu }}"
            {{- end }}
            volumeMounts:
            - name: dshm # Increase SHM size for the container by mounting /dev/shm, for Pytorch parallel processing
              mountPath: /dev/shm
            - name: ephemeral-storage
              mountPath: /local_resources
              readOnly: false
            - mountPath: /local_resources/mount
              name: workload-mount
          restartPolicy: Never

# RAG Chat App (Infinity + VLLM + Gradio)
This Helm chart deploys a Retrieval-Augmented Generation (RAG) Chat Application that combines:

Infinity for document embeddings
VLLM for LLM-based text generation
Chroma for vector storage
Gradio for a user-friendly chat interface

## Features
Embed your own .txt and .pdf documents (supports flat and nested folders)
Retrieve and synthesize answers using a VLLM-hosted LLM
Simple Gradio chat UI for interactive Q&A
All application code and knowledge base are packaged and deployed via Kubernetes ConfigMaps

## Deploying the Workload
1. Deploy Infinity and VLLM Services
Make sure you have the Infinity and VLLM services running in your cluster (see your infra repo or use provided Helm charts).
2. Deploy the RAG Chat App
Render and apply the Helm chart:

```bash
helm template my-app . | kubectl apply -f -
```

To use specific embedding or generation model (if you are running non-default infinity and vllm workloads):

```bash
helm template my-app . \
  --set infinity.host="rag-embedding-infinity-e5-base" \
  --set infinity.port=80 \
  --set infinity.model="intfloat/multilingual-e5-base" \
  --set vllm.host="llm-inference-vllm-qwen" \
  --set vllm.port=80 \
  --set vllm.model="Qwen/Qwen2-0.5B-Instruct" \
  | kubectl apply -f -
```

If "host" is a name of the service that is running the corresponding workload you should use port 80 (default).

## User Input Values
See `values.yaml` for all configurable options, such as:

Infinity/VLLM host and port

Model names

Resource requests/limits

## Interacting with the App
1. Verify Deployment


```bash
kubectl get svc
kubectl get deployment
kubectl get pod
```


2. Port Forward to Access Gradio UI
```bash
kubectl port-forward svc/my-app-rag-chat-app 8080:80
```


Then open http://localhost:8080 in your browser.

3. Chat with Your Documents
Upload your documents using UI function.
Use the UI to ask questions about your uploaded documents.
The app will embed, retrieve, and synthesize answers using your configured models.


## How It Works
Documents are chunked and embedded, with results saved into ChromaDB.
Embeddings are generated via the Infinity service.
Relevant chunks are retrieved from a Chroma vectorstore (persisted in an ephemeral volume).
Answers are generated by VLLM and displayed in the Gradio chat UI.


## Cleanup

After using the app, please delete the resources created.

You can run the same `helm template` command, only replacing `kubectl apply` with `kubectl delete`, e.g.:

```bash
helm template my-app . | kubectl delete -f -

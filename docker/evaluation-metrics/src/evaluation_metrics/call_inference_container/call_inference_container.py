import os
from argparse import Namespace
from datetime import datetime
from typing import Any, Dict, List

from datasets import load_dataset
from evaluation_metrics import logger
from evaluation_metrics.argument_parsers import get_inference_parser
from jsonlines import Writer
from openai import APIError, Client
from transformers import AutoTokenizer


def download_dataset(dataset: str, version: str) -> Dict:
    """
    Downloads a specified dataset and version.
    Args:
        dataset (str): The name of the dataset to download.
        version (str): The version of the dataset to download.
    Returns:
        Dict: A dictionary containing the downloaded dataset.
    """

    logger.info(f"Downloading dataset {dataset} version {version}")
    return load_dataset(dataset, version)


def get_llm_client(base_url: str, port: str | None, endpoint: str | None) -> Client:
    """
    Creates and returns a client for interacting with a language model (LLM) service.
    Args:
        base_url (str): The base URL of the LLM service.
        port (str | None): The port number to connect to the LLM service. If None, no port is appended.
        endpoint (str | None): The endpoint path for the LLM service. If None, no endpoint is appended.
    Returns:
        Client: An instance of the Client class configured to interact with the specified LLM service.
    """

    model_url = f"{base_url}{':'+port if port else ''}{'/'+endpoint if endpoint else ''}/"
    logger.info(f"Connecting to model at {model_url}")
    return Client(base_url=model_url, api_key="EMPTY")


def get_llm_inference(message: str, inference_client: Client, model_name: str, llm_parameters: Dict) -> str | None:
    """
    Generates a response from a language model inference container using the provided document,
    prompt template, and model parameters.
    Args:
        document (str): The input document or context to be used in the prompt.
        prompt_template (str): A template string for the prompt, which should include a placeholder
            denoted by `{context}` for the document.
        inference_client (Client): The client object used to interact with the inference container.
        model_name (str): The name of the language model to be used for inference.
        llm_parameters (Dict): Additional parameters to configure the language model, such as
            temperature, max tokens, etc.
    Returns:
        str | None: The content of the response generated by the language model. | If an error occurs during the inference process.
    """
    try:
        result = inference_client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": message}],
            **llm_parameters,
        )
        return result.choices[0].message.content
    except APIError as e:
        logger.error(f"APIException: {e}")
    return None


def save_inference_results(
    results: list, output_dir_path: str, model_name: str, evaluation_dataset: str, evaluation_dataset_version: str
) -> str:
    """
    Saves inference results to a JSONL file in the specified output directory.
    Args:
        results (list): A list of inference results to be saved.
        output_dir_path (str): The directory path where the inference results file will be saved.
        model_name (str): The name of the model used for inference.
        evaluation_dataset (str): The name of the evaluation dataset used.
        evaluation_dataset_version (str): The version of the evaluation dataset used.
    Returns:
        str: The file path of the saved inference results.
    Raises:
        OSError: If the output directory cannot be created or the file cannot be written.
    """

    if not os.path.exists(output_dir_path):
        logger.info(f"Creating path {output_dir_path}")
        os.makedirs(output_dir_path)

    inferences_filepath = os.path.join(
        output_dir_path,
        f"inferences_{model_name}--{evaluation_dataset.replace('/', '_')}--{evaluation_dataset_version}--{datetime.now().isoformat()}.jsonl",
    )

    logger.info(f"Writing inferences to {inferences_filepath}")
    with open(inferences_filepath, "w") as fp:
        writer = Writer(fp)
        writer.write_all(results)

    return inferences_filepath


def read_prompt_template(prompt_template_path: str) -> str:
    """
    Reads a prompt template from a file.
    Args:
        prompt_template_path (str): The file path to the prompt template.
    Returns:
        str: The content of the prompt template as a string.
    Raises:
        FileNotFoundError: If the specified file does not exist.
        IOError: If there is an error reading the file.
    """
    with open(prompt_template_path, "r") as p_file:
        prompt_template = p_file.read()
    return prompt_template


def run(
    dataset: Dict,
    prompt_template: str,
    context_column_name: str,
    gold_standard_column_name: str,
    llm_client: Client,
    model_name: str,
    model_path: str,
    parameters: Dict[str, Any],
    max_context_size: int,
    use_data_subset: int,
    dataset_split: str,
) -> List[Dict[str, Any]]:
    """
    Executes inference on a dataset using a specified language model and returns the results.
    Args:
        dataset (Dict): The dataset containing the input data for inference.
        prompt_template (str): The template used to format the prompt for the language model.
        context_column_name (str): The column name in the dataset containing the context documents.
        gold_standard_column_name (str): The column name in the dataset containing the gold standard answers.
        llm_client (Client): The client used to interact with the language model.
        model_name (str): The name of the language model to use for inference.
        parameters (Dict[str, Any]): Additional parameters to configure the language model.
        max_context_size (int): The maximum number of tokens allowed in the context document.
        use_data_subset (bool): Whether to use only a subset of the dataset for testing purposes.
        dataset_split (str): The split of the dataset to use (e.g., "train", "test", "validation").
    Returns:
        List[Dict[str, Any]]: A list of dictionaries containing the inference results, gold standard answers,
        input documents, and prompts.
    """
    results = list()

    # tokeniser = LlamaTokenizer.from_pretrained("HuggingFaceM4/llama-7b-tokenizer")
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    counter = 0  # Used for running a subset of the dataset for testing purposes
    length_exclusion_counter = 0
    inference_errors_counter = 0
    for datum in dataset[dataset_split]:

        document = datum[context_column_name]
        correct_answer = datum[gold_standard_column_name]

        logger.info(f"Running inference on document: {document}")

        # Format the prompt with the context document
        message = prompt_template.format(context=document)

        # Exclude messages that are longer than the context length
        tokens = tokenizer(message)["input_ids"]
        if len(tokens) > max_context_size:
            logger.warning(f"Message exceeds max content size of {max_context_size} tokens. Skipping...")
            logger.warning(f"Token length: {len(tokens)}, Character length: {len(message)}")
            length_exclusion_counter += 1
            continue

        inference_result = get_llm_inference(
            message=message,
            inference_client=llm_client,
            model_name=model_name,
            llm_parameters=parameters,
        )

        if inference_result is None:
            logger.warning(f"Error during inference for document. Skipping...")
            logger.warning(f"Token length: {len(tokens)}, Character length: {len(message)}")
            inference_errors_counter += 1
            continue

        logger.info(f"Inference result: {inference_result}")
        results.append(
            {
                "inference_result": inference_result,
                "gold_standard_result": [correct_answer],
                "document": document,
                "prompt": prompt_template,
            }
        )
        counter += 1
        if use_data_subset > 0 and counter >= use_data_subset:
            logger.info(f"Ran inference for a subset of data: {use_data_subset} documents.")
            break

    logger.info(f"Total documents: {len(dataset[dataset_split])}")
    logger.info(f"Total documents used for evaluation: {len(results)}")
    logger.info(f"\tDocuments excluded due to length: {length_exclusion_counter}")
    logger.info(f"\tInference errors encountered: {inference_errors_counter}")

    return results


def main(args: Namespace) -> str:

    logger.info(f"Loading dataset {args.evaluation_dataset} version {args.evaluation_dataset_version}")

    ds = download_dataset(dataset=args.evaluation_dataset, version=args.evaluation_dataset_version)

    logger.info("Dataset loaded...")

    prompt_template = read_prompt_template(args.prompt_template_path)

    logger.info(f"Prompt template has been read in from {args.prompt_template_path}")

    parameters: dict = {}

    client = get_llm_client(base_url=args.llm_base_url, port=args.llm_port, endpoint=args.llm_endpoint)

    results = run(
        dataset=ds,
        prompt_template=prompt_template,
        context_column_name=args.context_column_name,
        gold_standard_column_name=args.gold_standard_column_name,
        llm_client=client,
        model_name=args.model_name,
        model_path=args.model_path,
        parameters=parameters,
        max_context_size=args.maximum_context_size,
        use_data_subset=args.use_data_subset,
        dataset_split=args.dataset_split,
    )

    inferences_filepath = save_inference_results(
        results=results,
        output_dir_path=args.output_dir_path,
        model_name=args.model_name,
        evaluation_dataset=args.evaluation_dataset,
        evaluation_dataset_version=args.evaluation_dataset_version,
    )

    return inferences_filepath


if __name__ == "__main__":
    parser = get_inference_parser()
    args = parser.parse_args()
    main(args)

{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SiloGen AI Workloads","text":"<p>This repository contains workloads, tools, and utilities for AI development and testing in the Silogen ecosystem.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>The documentation for this project is built using MkDocs with the Material theme.</p>"},{"location":"#setting-up-documentation-locally","title":"Setting Up Documentation Locally","text":"<ol> <li> <p>Install documentation dependencies:    <pre><code>pip install -r requirements-docs.txt\n</code></pre></p> </li> <li> <p>Start the documentation server:    <pre><code>mkdocs serve\n</code></pre></p> </li> <li> <p>View the documentation at http://localhost:8000</p> </li> </ol>"},{"location":"#building-documentation","title":"Building Documentation","text":"<p>To build the documentation site: <pre><code>mkdocs build\n</code></pre></p> <p>The built site will be in the <code>site</code> directory.</p>"},{"location":"#development","title":"Development","text":"<p>See the Contributing guide for development setup instructions.</p>"},{"location":"docker/","title":"Index","text":"<p>Dockerfiles related to workloads (e.g. if not available elsewhere)</p>"},{"location":"docker/axolotl/","title":"Axolotl ROCm image","text":"<p>Axolotl is an LLM continued pretraining and finetuning framework. This Dockerfile builds an image that can run axolotl on the ROCm platform.</p>"},{"location":"docker/axolotl/#building","title":"Building","text":"<p>Currently, the build requires you to have access to the image <code>ghcr.io/silogen/rocm6.2-vllm0.6.3-flash-attn2.6.3-wheels:static</code>, which is private. Once you have access, to build the image, simply run <pre><code>docker build -f axolotl-rocm.Dockerfile -t ghcr.io/silogen/rocm-silogen-axolotl-worker:YOUR_VERSION .\n</code></pre></p>"},{"location":"docker/axolotl/#missing-functionality","title":"Missing functionality","text":"<p>Currently, certain dependencies are not installed due to incompatibility issues: - bitsandbytes, which is built and installed, but currently has some unsolved issues. It is installed so that axolotl   launch works, but avoid functionality that actually uses bitsandbytes. - mamba-ssm, which should be possible to run on ROCm with a patch - nvidia-ml-py, as Nvidia management functionality is not used on ROCm</p>"},{"location":"docker/llm-evaluation/","title":"LLM Evaluation","text":"<p>Evaluation package used to run LLM evaluations (metrics evaluation, e.g. BERT-score, BLEU-score, and LLM-as-a-Judge evaluation). To be installed in a docker image used for running evaluation workloads using Kubernetes.</p> <p>We have a default image available at <code>ghcr.io/silogen/evaluation-workloads-metrics:v0.1</code>, which can be used to run the evaluation workloads included in this repository. Please refer to <code>workloads/llm-evaluation-judge</code> and <code>workloads/llm-evaluation-metrics</code> for instructions on running the workloads and examples on setting up the docker images.</p>"},{"location":"docker/llm-evaluation/#building-your-own-image","title":"Building your own image","text":"<p>We provide a <code>Makefile</code> to build and push a Docker image to a user image registry. Edit the Makefile for the desired image registry URL. See a tutorial here</p> <p>The <code>Makefile</code> uses the <code>Dockerfile</code> to build a Docker image. For further info about Dockerfiles, see here</p>"},{"location":"docker/logistics/","title":"Logistics image with separate root target","text":"<p>The purpose of the logistics image is to manage data movement from/to various data sources, e.g. Huggingface registry, Minio tenants etc. It contains all the necessary libraries and clients required for downloading and uploading data from/to supported storage providers.</p> <p>For the interaction between the logistics and main workload containers, both containers usually need access to files created by the other one. This requires that the containers run under the same user. Megatron training images currently require a root user, so we must use an accompanying logistics container which is similarly setup for the root user. To build the logistics image with the root user based target include <code>--target base</code> in the <code>docker build</code> command.</p> <pre><code>DOCKER_BUILDKIT=1 \\\ndocker build -f docker/logistics/logistics.Dockerfile \\\n             --target base \\\n             -t ghcr.io/silogen/logistics:v0.1r .\n</code></pre> <p>If the intent is to build the image with the user having uid <code>1000</code>, just skip the <code>--target base</code> option.</p>"},{"location":"docs/contributing/","title":"Contributing","text":"<p>Thank you for considering contributing to the SiloGen AI Workloads development!</p>"},{"location":"docs/contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork the repository</li> <li> <p>Clone your fork:    <pre><code>git clone https://github.com/silogen/ai-workloads\ncd ai-workloads\n</code></pre></p> </li> <li> <p>Set up development environment:    <pre><code>python -m venv .venv\nsource .venv/bin/activate\n# install packages you need\n</code></pre></p> </li> </ol>"},{"location":"docs/contributing/#pre-commit-setup","title":"Pre-commit setup","text":"<p>We use pre-commit for consistent formatting and cleaner code. Hooks are specified in <code>ai-workloads-dev/.pre-commit-config.yaml</code>.</p> <p>To install: <code>cd ai-workloads-dev</code> (this is necessary for <code>pre-commit install</code>, which runs particular to a git repository) <code>source your_venv</code> <code>pip install pre-commit</code> <code>pre-commit install --config .pre-commit-config.yaml</code> <code>git commit -m \"test commit\"</code></p> <p>With the final command, pre-commit should run automatically, with output something like the following:</p> <p>check json...........................................(no files to check)Skipped    check yaml...........................................(no files to check)Skipped    fix end of files.....................................(no files to check)Skipped    fix requirements.txt.................................(no files to check)Skipped    trim trailing whitespace.............................(no files to check)Skipped    black................................................(no files to check)Skipped    flake8...............................................(no files to check)Skipped    isort (python).......................................(no files to check)Skipped    mypy.................................................(no files to check)Skipped    helmlint.............................................(no files to check)Skipped</p> <p>It's also possible to manually run pre-commit using</p> <p><code>pre-commit run --all-files</code></p>"},{"location":"docs/contributing/#troubleshooting-pre-commit","title":"Troubleshooting pre-commit","text":"<p>Many pre-commit bugs come from having an incorrect version of pre-commit active. Pre-commit can hang around as a system-wide version, in python venvs, or in your pre-commit cache.</p> <p>It's easiest to use pre-commit as part of a python virtual environment. To check that the right pre-commit is being found, run <code>which pre-commit</code> and confirm that the binaries inside your venv are shown. For example: <code>/../../venvs/your_venv/bin/pre-commit</code>. A different path could indicate that your system is choosing the wrong pre-commit install.</p> <p>From system: <code>brew uninstall pre-commit</code> (mac) <code>sudo apt remove pre-commit</code> (linux)</p> <p>From venv: <code>pip uninstall pre-commit</code></p> <p>Just the pre-commit hooks uninstall: <code>pre-commit uninstall</code> <code>pre-commit clean</code></p> <p>Then reinstall pre-commit from scratch as described above.</p>"},{"location":"docs/contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Create a branch for your feature or bugfix:    <pre><code>git checkout -b feature-name\n</code></pre></p> </li> <li> <p>Make your changes and write tests</p> </li> <li> <p>Run tests:    <pre><code>pytest\n</code></pre></p> </li> <li> <p>Submit a pull request</p> </li> </ol>"},{"location":"docs/contributing/#documentation","title":"Documentation","text":"<p>Update the documentation when adding new features:</p> <ol> <li>Add new pages in the <code>docs/</code> directory</li> <li>Update the <code>nav</code> section in <code>mkdocs.yml</code> if necessary</li> <li>Test the documentation locally:    <pre><code>mkdocs serve\n</code></pre></li> </ol>"},{"location":"docs/contributing/#code-style","title":"Code Style","text":"<p>Follow PEP 8 guidelines. We use <code>black</code> for code formatting and <code>flake8</code> for linting.</p>"},{"location":"docs/getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with the SiloGen AI Workloads.</p>"},{"location":"docs/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>pip</li> <li>Virtual environment (optional but recommended)</li> </ul>"},{"location":"docs/getting-started/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/silogen/ai-workloads\ncd ai-workloads\n</code></pre></p> </li> <li> <p>Set up a virtual environment (optional):    <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows, use: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"docs/getting-started/#next-steps","title":"Next Steps","text":"<p>Explore the tutorials and workloads documentation to learn about various workload types and configurations.</p>"},{"location":"docs/tutorials/","title":"Usage","text":"<p>Learn how to use various features of the AI workloads.</p>"},{"location":"docs/tutorials/#tutorials","title":"Tutorials","text":"<p>The tutorials start with a shared setup: Tutorial 0. It covers the tools, but also has a required cluster setup step that needs to be run.</p> <p>For practical examples and step-by-step guides on using the SiloGen AI Workloads, check out our tutorials:</p> <ul> <li>Tutorial 0: Shared setup for tutorials.</li> <li>Deliver Resources and Finetune: Learn how to download models and data to cluster MinIO storage, run finetuning jobs, and deploy inference services.</li> <li>Language extension: Odia-finetuning: Learn how to finetune a continued pretraining basemodel to make it an instruction following model, then deploy models and compare.</li> <li>Deliver Resources and Run Megatron-LM Continuous Pretraining of Llama-3.1-8B: Learn how to download a dataset and base model to cluster\u202fMinIO, convert them to Megatron\u2011LM format, run a multi\u2011node continuous\u2011pretraining job, and validate the final checkpoint with an inference workload.</li> <li>Deliver Resources and Run Megatron-LM Continuous Pretraining of Llama-3.1-70B: Learn how to download a dataset and base model to cluster\u202fMinIO, convert them to Megatron\u2011LM format, run a multi\u2011node continuous\u2011pretraining job, and validate the final checkpoint with an inference workload.</li> </ul>"},{"location":"docs/workloads/","title":"Getting Started","text":"<p>This guide will help you get started with the SiloGen AI Workloads.</p>"},{"location":"docs/tutorials/tutorial-00-prerequisites/","title":"Tutorial 0: Prerequisites for running the tutorials","text":""},{"location":"docs/tutorials/tutorial-00-prerequisites/#required-program-installs","title":"Required program installs","text":"<p>Programs that are used in this tutorial:</p> <ul> <li>kubectl</li> <li>helm</li> <li>k9s</li> <li>jq</li> <li>curl</li> </ul> <p>At least curl and often jq too are commonly installed in many distributions out of the box.</p>"},{"location":"docs/tutorials/tutorial-00-prerequisites/#namespace-setup","title":"Namespace setup","text":"<p>In order to run the workloads you need to have a Kubernetes cluster with sufficient resources configured. This includes storage, secrets, namespace, and a HuggingFace token. These should be installed and configured as part of the installation process, but if this is not the case you can use the following command to set up these. The command does the following:</p> <ul> <li> <p>Adds a namespace, where we will conduct all our work. We will setup the <code>silo-tutorial</code> namespace, but this also works in the default namespace in your cluster.</p> </li> <li> <p>Adds an External Secret to get the credentials to access the MinIO storage from our namespace.</p> </li> <li> <p>This depends on a ClusterSecretStore called <code>k8s-secret-store</code> being already setup by a cluster admin, and the MinIO API credentials being secret there. The cluster should have these by default.</p> </li> <li> <p>Adds a LocalQueue so that our jobs schedule intelligently.</p> </li> <li> <p>This references the ClusterQueue <code>kaiwo</code> which should already be setup by a cluster admin.</p> </li> </ul> <p>We will use the helm chart in <code>workloads/k8s-namespace-setup/helm</code> and the overrides in <code>workloads/k8s-namespace-setup/helm/overrides/</code>.</p> <p>This first creates a new namespace and sets the current context to use it from now on:</p> <pre><code>kubectl create namespace \"silo-tutorial\"\nkubectl config set-context --current --namespace silo-tutorial\nhelm template workloads/k8s-namespace-setup/helm \\\n  --values workloads/k8s-namespace-setup/helm/overrides/tutorial-prereqs-local-queue.yaml \\\n  --values workloads/k8s-namespace-setup/helm/overrides/tutorial-prereqs-storage-access-external-secret.yaml \\\n  | kubectl apply -f -\n</code></pre> <p>HuggingFace token: In addition to running the command above you also need to add your HuggingFace Token as a secret called <code>hf-token</code> with the key <code>hf-token</code> in your namespace.</p>"},{"location":"docs/tutorials/tutorial-00-prerequisites/#monitoring-progress-logs-and-gpu-utilization-with-k9s","title":"Monitoring progress, logs, and GPU utilization with k9s","text":"<p>We're interested to see a progress bar of the fine-tuning training, seeing any messages that a workload logs, and we also want to verify that our GPU jobs are consuming our compute relatively effectively. This information can be fetched from our Kubernetes cluster in many ways, but one convenient and recommended way us using k9s.</p> <p>We recommend the official documentation for more thorough guidance, but this section shows some basic commands to get what we want here.</p> <p>To get right to the jobs view in the namespace we're using in this walk-through, we can run:</p> <pre><code>k9s --command Jobs\n</code></pre> <p>Choose a job using <code>arrow keys</code> and <code>Enter</code> to see the Pod that it spawned, then <code>Enter</code> again to see the Container in the Pod. From here, we can do three things:</p> <ul> <li> <p>Look at the logs by pressing <code>l</code>. The logs show any output messages produced during the workload runtime.</p> </li> <li> <p>Attach to the output of the container by pressing <code>a</code>. This is particularly useful to see the interactive progress bar of a fine-tuning run.</p> </li> <li> <p>Spawn a shell inside the container by pressing <code>s</code>. Inside the shell we can run <code>watch -n0.5 rocm-smi</code> to get a view of the GPU utilization that updates every 0.5s.</p> </li> </ul> <p>Return from any regular <code>k9s</code> view with <code>Esc</code>.</p>"},{"location":"docs/tutorials/tutorial-01-deliver-resources-and-finetune/","title":"Tutorial 01: Deliver model and data to cluster MinIO, then run fine-tune","text":"<p>This tutorial shows how to download a model and some data from HuggingFace Hub to a cluster-internal MinIO storage server, and then launch fine-tuning jobs that use those resources. The checkpoints are also synced into the same cluster-internal MinIO storage. Finally, an inference workload is spawned to make it possible to discuss with the newly fine-tuned model. At the end of the tutorial, there are some instructions on changing the model and the data.</p> <p>The fine-tuning work in this tutorial is meant for demonstration purposes, small enough to be run live. We start from Tiny-Llama 1.1B Chat, a small LLM. This is already a chat-fine-tuned model.</p> <p>We train it with some additional instruction data in the form of single prompt-and-answer pairs. The prompts in this data were gathered from real human prompts to LLMs, mostly ones that were shared on the now deprecated sharegpt.com site. The answers to those human prompts were generated with the Mistral Large model. In essence, training on this data makes our model respond more like Mistral Large. And there's another thing that this training accomplishes, which is to change the chat template, meaning the way the input to the model is formatted. More specifically, this adds special tokens that signal the start and end of message. Our experience is that such special tokens make the inference time message end signaling and message formatting a bit more robust.</p>"},{"location":"docs/tutorials/tutorial-01-deliver-resources-and-finetune/#1-setup","title":"1. Setup","text":"<p>Follow the setup in the tutorial pre-requisites section.</p>"},{"location":"docs/tutorials/tutorial-01-deliver-resources-and-finetune/#2-run-workloads-to-deliver-data-and-a-model","title":"2. Run workloads to deliver data and a model","text":"<p>We will use the helm charts in <code>workloads/download-huggingface-model-to-bucket/helm</code> and <code>workloads/download-data-to-bucket/helm</code>. We will use them to deliver a Tiny-Llama 1.1B parameter model, and an Argilla single-turn response supervised fine-tuning dataset respectively.</p> <p>Our user input files are in <code>workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-01-tiny-llama-to-minio.yaml</code>, and <code>workloads/download-data-to-bucket/helm/overrides/tutorial-01-argilla-to-minio.yaml</code>.</p> <pre><code>helm template workloads/download-huggingface-model-to-bucket/helm \\\n  --values workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-01-tiny-llama-to-minio.yaml \\\n  --name-template \"deliver-tiny-llama-model\" \\\n  | kubectl apply -f -\nhelm template workloads/download-data-to-bucket/helm \\\n  --values workloads/download-data-to-bucket/helm/overrides/tutorial-01-argilla-to-minio.yaml \\\n  --name-template \"deliver-argilla-data\" \\\n  | kubectl apply -f -\n</code></pre> <p>The logs will show a model staging download and upload for the model delivery workload, and data download, preprocessing, and upload for the data delivery.</p>"},{"location":"docs/tutorials/tutorial-01-deliver-resources-and-finetune/#3-scaling-fine-tuning-hyperparameter-tuning-with-parallel-jobs","title":"3. Scaling fine-tuning: hyperparameter tuning with parallel jobs","text":"<p>At the hyperparameter tuning stage, we run many parallel jobs while varying a hyperparameter to find the best configuration. Here we are going to look for the best rank parameter <code>r</code> for LoRA.</p> <p>To define the fine-tuning workload, we will use the helm chart in <code>workloads/llm-finetune-silogen-engine/helm</code>. Our user input file is <code>workloads/llm-finetune-silogen-engine/overrides/tutorial-01-finetune-lora.yaml</code>. This also includes the fine-tuning hyperparameters - you can change them in the file to experiment, or use <code>--set</code> with helm templating to change an individual value.</p> <p>Let's create ten different fine-tuning jobs to try out different LoRA ranks:</p> <pre><code>run_id=alpha\nfor r in 4 6 8 10 12 16 20 24 32 64; do\n  name=\"tiny-llama-argilla-r-sweep-$run_id-$r\"\n  helm template workloads/llm-finetune-silogen-engine/helm \\\n    --values workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-01-finetune-lora.yaml \\\n    --name-template $name \\\n    --set finetuning_config.peft_conf.peft_kwargs.r=$r \\\n    --set \"checkpointsRemote=default-bucket/experiments/$name\" \\\n    | kubectl apply -f -\ndone\n</code></pre> <p>For each job we can see logs, a progress bar, and that job's GPU utilization following the instructions above. If these jobs get relaunched, they are setup to continue from the existing checkpoints. If we instead want to re-run from scratch, we can just change the <code>run_id</code> variable that is defined before the for loop.</p>"},{"location":"docs/tutorials/tutorial-01-deliver-resources-and-finetune/#4-scaling-fine-tuning-multi-gpu-training","title":"4. Scaling fine-tuning: multi-GPU training","text":"<p>Beside parallel jobs, we can also take advantage of multiple GPUs by using them for parallel compute. This can be helpful for more compute demanding jobs, and necessary with larger models.</p> <p>Let's launch an 8-GPU run of full-parameter fine-tuning:</p> <pre><code>name=\"tiny-llama-argilla-v1\"\nhelm template workloads/llm-finetune-silogen-engine/helm \\\n  --values workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-01-finetune-full-param.yaml \\\n  --name-template $name \\\n  --set \"checkpointsRemote=default-bucket/experiments/$name\" \\\n  --set \"finetuningGpus=8\" \\\n  | kubectl apply -f -\n</code></pre> <p>We can see logs, a progress bar, and the full 8-GPU compute utilization following the instructions above. The training steps of this multi-gpu training run take merely 75 seconds, which reflects the nature of fine-tuning: fast, iterative, with a focus on flexible experimentation.</p> <p>If we want to compare to an equivalent single-GPU run, we can run:</p> <pre><code>name=\"tiny-llama-argilla-v1-singlegpu\"\nhelm template workloads/llm-finetune-silogen-engine/helm \\\n  --values workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-01-finetune-full-param.yaml \\\n  --name-template $name \\\n  --set \"checkpointsRemote=default-bucket/experiments/$name\" \\\n  --set \"finetuningGpus=1\" \\\n  | kubectl apply -f -\n</code></pre> <p>The training steps for this single-GPU run take around 340 seconds. Thus the full-node training yields a speedup ratio of around 0.22 (4.5x speed). Even higher speedups are achieved in pretraining, which benefits hugely from optimizations.</p>"},{"location":"docs/tutorials/tutorial-01-deliver-resources-and-finetune/#5-inference-with-a-fine-tuned-model","title":"5. Inference with a fine-tuned model","text":"<p>After training the model, we'll want to discuss with it. For this we will use the helm chart in <code>workloads/llm-inference-vllm/helm</code>.</p> <p>Let's deploy the full-parameter fine-tuned model:</p> <pre><code>name=\"tiny-llama-argilla-v1\"\nhelm template workloads/llm-inference-vllm/helm \\\n  --set \"model=s3://default-bucket/experiments/$name/checkpoint-final\" \\\n  --set \"vllm_engine_args.served_model_name=$name\" \\\n  --name-template \"$name\" \\\n  | kubectl apply -f -\n</code></pre> <p>We can change the <code>name</code> to different experiment names to deploy other models. Note that discussing with the LoRA adapter models with these workloads requires us to merge the final adapter. This can be achieved during fine-tuning by adding <code>--set mergeAdapter=true</code>. The final full model is saved in the same path of <code>\"$experiment/checkpoint-final\"</code>, whether it is an adapter or a full-parameter training run.</p> <p>To discuss with the model, we first need to setup a connection to it. Since this is not a public-internet deployment, we'll do this simply by starting a background port-forwarding process:</p> <pre><code>name=\"tiny-llama-argilla-v1\"\nkubectl port-forward services/llm-inference-vllm-$name 8080:80 &gt;/dev/null &amp;\nportforwardPID=$!\n</code></pre> <p>Now we can discuss with the model, using curl:</p> <pre><code>name=\"tiny-llama-argilla-v1\"\nquestion=\"What are the top five benefits of eating a large breakfast?\"\ncurl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -X POST \\\n    -d '{\n        \"model\": \"'$name'\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"'\"$question\"'\"}\n        ]\n    }' | jq \".choices[0].message.content\" --raw-output\n</code></pre> <p>We can test the limits of the model with our own questions. Since this is a model with a relatively limited capacity, its answers are often delightful nonsense.</p> <p>When we want to stop port-forwarding, we can just run:</p> <p><pre><code>kill $portforwardPID\n</code></pre> and to stop the deployment, we run: <pre><code>name=\"tiny-llama-argilla-v1\"\nkubectl delete deployments/llm-inference-vllm-$name\n</code></pre></p>"},{"location":"docs/tutorials/tutorial-01-deliver-resources-and-finetune/#next-steps-how-to-use-your-own-model-and-data","title":"Next Steps: How to use your own model and data","text":"<p>This tutorial has shown the basic steps in running fine-tuning and chatting with the resulting model. For many, the next step may be to use our own models and data.</p> <p>This section should get us started, but ultimately, this opens the whole topic of how to do fine-tuning, which is too large to cover here. One more comprehensive view point is provided by the T\u00fcl\u00fc 3 paper.</p>"},{"location":"docs/tutorials/tutorial-01-deliver-resources-and-finetune/#preparing-your-own-model-and-data","title":"Preparing your own model and data","text":"<p>The workload <code>workloads/download-huggingface-model-to-bucket/helm</code> delivers HuggingFace Hub models. To get models from elsewhere, we may for instance do it manually by downloading them to our own computers and uploading to our bucket storage from there. The data delivery workload <code>workloads/download-data-to-bucket/helm</code> uses a free script to download and preprocess the data, so it is more flexible in this regard.</p> <p>The bucket storage used in this tutorial is a MinIO server hosted inside the cluster itself. To use some other S3-compatible bucket storage, we need to change the <code>bucketStorageHost</code> field, add our credentials (HMAC keys) as a Secret in our namespace (this is generally achieved via an External Secret that in turn fetches the info from some secret store that we have access to), and then refer to that bucket storage credentials Secret in the <code>bucketCredentialsSecret</code> nested fields.</p> <p>To prepare our own model, we create a values file that is similar to <code>workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-01-tiny-llama-to-minio.yaml</code>. The key field is <code>modelID</code>, which defines which model is downloaded. The field <code>bucketModelPath</code> determines where the model is stored in the bucket storage.</p> <p>To prepare our own data, we structure our values file like <code>workloads/download-data-to-bucket/helm/overrides/tutorial-01-argilla-to-minio.yaml</code>. It may be easiest to write a Python script separately, potentially test it locally, and then put the script as a block text value for <code>dataScript</code>. The dataset upload location is set with the <code>bucketDataDir</code> field.</p>"},{"location":"docs/tutorials/tutorial-01-deliver-resources-and-finetune/#data","title":"Data","text":"<p>The <code>dataScript</code> is a script instead of just a dataset identifier, because the datasets on HuggingFace hub don't have a standard format that can be always directly passed to our fine-tuning engine. The data script should format the data into the format that the silogen fine-tuning engine expects. For supervised fine-tuning, this is JSON lines, where each line has a JSON dictionary formatted as follows:</p> <p><pre><code>{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"This is a user message\"},\n    {\"role\": \"assistant\", \"content\": \"The is an assistant answer\"}\n  ]\n}\n</code></pre> There can be an arbitrary number of messages. Additionally, each dictionary can contain a <code>dataset</code> field that has the dataset identifier, and an <code>id</code> field that identifies the data point uniquely. For Direct Preference Optimization, the data format is as follows:</p> <pre><code>{\n  \"prompt_messages\": [\n    {\"role\": \"user\", \"content\": \"This is a user message\"},\n  ],\n  \"chosen_messages\": [\n    {\"role\": \"assistant\", \"content\": \"This is a preferred answer\"}\n  ],\n  \"rejected_messages\": [\n    {\"role\": \"assistant\", \"content\": \"This is a rejected answer\"}\n  ]\n}\n</code></pre> <p>The JSON lines output of the data script should be saved to the <code>/downloads/datasets/</code>. This is easy with the approach taken in the tutorial file:</p> <pre><code>dataset.to_json(\"/downloads/datasets/&lt;name of your dataset file.jsonl&gt;\")\n</code></pre> <p>The dataset is uploaded to the directory pointed to by <code>bucketDataDir</code>, with the same filename as it had under <code>/downloads/datasets</code>.</p>"},{"location":"docs/tutorials/tutorial-01-deliver-resources-and-finetune/#model","title":"Model","text":"<p>Preparing a model is simple than data. We simply set the <code>modelID</code> to the HuggingFace Hub ID of the model (in the <code>Organization/ModelName</code> format). The model is the uploaded to the path pointed to by <code>bucketModelPath</code>.</p>"},{"location":"docs/tutorials/tutorial-01-deliver-resources-and-finetune/#setting-fine-tuning-parameters","title":"Setting fine-tuning parameters","text":"<p>For fine-tuning, we create a values file that is similar to <code>workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-01-finetune-lora.yaml</code> (for LoRA adapter training) or <code>workloads/llm-finetune-silogen-engine/overrides/tutorial-01-finetune-full-param.yaml</code> (for full parameter training). We'll want to inject our data in the field:</p> <pre><code>finetuning_config:\n  data_conf:\n    training_data:\n      datasets:\n        - path: \"bucketName/path/to/file.jsonl\n</code></pre> <p>The datasets array can contain any number of datasets, and they're concatenated in training.</p> <p>The model is set in the top level field <code>basemodel</code>, where the value should be a name of a bucket followed by the path to the model directory in the bucket, formatted like:</p> <pre><code>basemodel: bucketName/path/to/modelDir\n</code></pre> <p>All fine-tuning configurations are not sensible with all models, and some settings might even fail for unsupported models. Ultimately we need to understand the particular model we're using to set the parameters correctly. Suitable hyperparameters also depend on the data.</p> <p>One key model compatibility parameter to look at is the chat template, which is set by</p> <pre><code>finetuning_config:\n  data_conf:\n    chat_template_name: \"&lt;name of template&gt;\"\n</code></pre> <p>If the model we start from already has a chat template, we should usually set this to <code>\"keep-original\"</code>. Otherwise, <code>\"chat-ml\"</code> is usually a reasonable choice. Another set of parameters that often needs to be changed between models is the set of PEFT target layers, if doing LoRA training. These are set in the following field:</p> <pre><code>finetuning_config:\n  peft_conf:\n    peft_kwargs:\n      target_modules:\n        - q_proj\n        - k_proj\n        - v_proj\n        - o_proj\n        - up_proj\n        - down_proj\n</code></pre> <p>One setting that can be used is</p> <pre><code>finetuning_config:\n  peft_conf:\n    peft_kwargs:\n      target_modules: \"all-linear\"\n</code></pre> <p>which targets all linear layers on the model and doesn't require knowing the names of the layers.</p>"},{"location":"docs/tutorials/tutorial-02-language-extension-finetune/","title":"Fine-tuning: adding language-capabilities to a model","text":"<p>Adding a new language to model usually follows at least two stages: first, continued pretraining to build understanding and basic capacity to generate that language, and second, fine-tuning to make the model, for example, follow instructions. This tutorial handles that second stage of instruction-tuning for Odia language.</p> <p>The original basemodel is Qwen1.5-7B. The first stage of continued pretraining to add general Odia understanding and generation abilities to Qwen have already been done by the OdiaGenAI organization. That continued pretrained model is available openly here. A relevant comparison point for this tutorial is the chat-fine-tuned version of the Qwen1.5 basemodel, which should have the capability to follow instructions, but is not specifically meant for Odia.</p> <p>Note that access to the Odia continued pretraining version of the Qwen model requires signing the request on Huggingface. This also means that for downloading the model, we'll need to use a HuggingFace access token. See instructions here.</p> <p>Note</p> <p>This tutorial does not add the HuggingFace token to the cluster yet. You need to add it yourself.</p> <p>The tutorial includes cluster setup, model and data downloads, fine-tuning, and finally inference. We should start with a working cluster, setup by a cluster administrator using Cluster Forge. The access to that cluster is provided with a suitable Kubeconfig file.</p>"},{"location":"docs/tutorials/tutorial-02-language-extension-finetune/#1-setup","title":"1: Setup","text":"<p>Follow the setup in the tutorial pre-requisites section.</p> <p>Warning</p> <p>This tutorial does not handle adding the HuggingFace token to the cluster yet. Coming soon. Before then, to run this tutorial, you are responsible for adding your HuggingFace token as a secret called <code>hf-token</code> with the key <code>hf-token</code> in the <code>silo-tutorial</code> namespace.</p>"},{"location":"docs/tutorials/tutorial-02-language-extension-finetune/#2-fetch-data-and-models","title":"2. Fetch data and models","text":"<p>First we'll fetch the model and data for the fine-tuning.</p> <p>We will use the helm charts in <code>workloads/download-huggingface-model-to-bucket/helm</code> and <code>workloads/download-data-to-bucket/helm</code>. We will download a Qwen1.5 7B-parameter model, one English instruction dataset and five different Odia-language single-turn instruction fine-tuning datasets. These datasets cover slightly different areas, including open instructions, context-based question-answering, translation, and identity answers</p> <p>The identity answers aim to make our model call itself Olive, and tell that it is from the OdiaGenAI project. Our user input files are in <code>workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-02-qwen-odia.yaml</code> and <code>workloads/download-data-to-bucket/helm/overrides/tutorial-02-odia-data.yaml</code></p> <pre><code>helm template workloads/download-huggingface-model-to-bucket/helm \\\n  --values workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-02-qwen-odia.yaml \\\n  --name-template \"download-odia-qwen-odia\" \\\n  | kubectl apply -f -\nhelm template workloads/download-data-to-bucket/helm \\\n  --values workloads/download-data-to-bucket/helm/overrides/tutorial-02-odia-data.yaml \\\n  --name-template \"download-odia-data\" \\\n  | kubectl apply -f -\n</code></pre> <p>The logs will show a model staging download and upload, then data download, preprocessing, and upload.</p>"},{"location":"docs/tutorials/tutorial-02-language-extension-finetune/#3-fine-tune-an-odia-model-on-8-gpus","title":"3. Fine-tune an Odia model on 8 GPUs","text":"<p>We will run our Odia language fine-tuning using 8 GPUs in a data parallel setup. For fine-tuning, we'll use the helm chart in  <code>workloads/llm-finetune-silogen-engine/helm</code>, and the user input file in <code>workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-02-qwen-odia-instruct-v1.yaml</code>.</p> <p>This training job takes around 13 hours, because the combination of 6 datasets is large and we go through that combination 6 times. We have found that a large number of training steps is necessary to enable the model to learn act reasonably in Odia language. We suspect this has something to do with the Odia script, which is not well covered by the Qwen tokenizer, and leads to very long sequences produced character or even diacritic at a time.</p> <p>Let's launch the fine-tuning job:</p> <pre><code>name=\"qwen-odia-instruct-v1\"\nhelm template workloads/llm-finetune-silogen-engine/helm \\\n  --values workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-02-qwen-odia-instruct-v1.yaml \\\n  --name-template $name \\\n  --set \"checkpointsRemote=default-bucket/experiments/$name\" \\\n  --set \"finetuningGpus=8\" \\\n  | kubectl apply -f -\n</code></pre> <p>We can see logs, a progress bar, and the full 8-GPU compute utilization following the instructions .</p>"},{"location":"docs/tutorials/tutorial-02-language-extension-finetune/#4-compare-the-official-qwen15-7b-chat-model-the-odia-continued-pretraining-basemodel-and-the-odia-fine-tuned-model","title":"4. Compare the official Qwen1.5-7B-Chat model, the Odia continued pretraining basemodel, and the Odia-fine-tuned model:","text":"<p>The Qwen1.5-7B-Chat is a general chat-fine-tuned version of the same Qwen basemodel that our Odia continued pretraining started from. Thus it is a good point of comparison. Additionally we'll deploy the Odia continued pretraining basemodel, to see how the instruct-fine-tuning changed it.</p> <p>For inference deployments, we will use the helm chart in  <code>workloads/llm-inference-vllm/helm/chart</code>.</p> <p>Deploy the models with the following commands. Note that the Qwen1.5-7B-Chat model we're getting directly from HuggingFace, while the other two models we're fetching from the cluster internal bucket storage.</p> <pre><code>name=\"qwen-base-chat\"\nhelm template workloads/llm-inference-vllm/helm \\\n  --set \"model=Qwen/Qwen1.5-7B-Chat\" \\\n  --set \"vllm_engine_args.served_model_name=$name\" \\\n  --name-template \"$name\" \\\n  | kubectl create -f -\nname=\"qwen-odia-base\"\nhelm template workloads/llm-inference-vllm/helm \\\n  --set \"model=s3://default-bucket/models/OdiaGenAI/LLM_qwen_1.5_odia_7b\" \\\n  --set \"vllm_engine_args.served_model_name=$name\" \\\n  --name-template \"$name\" \\\n  | kubectl create -f -\nname=\"qwen-odia-instruct-v1\"\nhelm template workloads/llm-inference-vllm/helm \\\n  --set \"model=s3://default-bucket/experiments/$name/checkpoint-final\" \\\n  --set \"vllm_engine_args.served_model_name=$name\" \\\n  --name-template \"$name\" \\\n  | kubectl create -f -\n</code></pre> <p>To discuss with the models, we need to setup connections to them. Since these are not public-internet deployments, we'll do this simply by starting background port-forwarding processes:</p> <pre><code>name=\"qwen-base-chat\"\nkubectl port-forward services/llm-inference-vllm-$name 8080:80 &gt;/dev/null &amp;\nqwenchatPID=$!\nname=\"qwen-odia-base\"\nkubectl port-forward services/llm-inference-vllm-$name 8090:80 &gt;/dev/null &amp;\nodiabasePID=$!\nname=\"qwen-odia-instruct-v1\"\nkubectl port-forward services/llm-inference-vllm-$name 8100:80 &gt;/dev/null &amp;\nodiainstructPID=$!\n</code></pre> <p>Now we can talk to the models. We'll ask them about the difference between physics and chemistry, in Odia:</p> <p><pre><code>question=\"\u0b2a\u0b26\u0b3e\u0b30\u0b4d\u0b25 \u0b2c\u0b3f\u0b1c\u0b4d\u0b1e\u0b3e\u0b28 \u0b0f\u0b2c\u0b02 \u0b30\u0b38\u0b3e\u0b5f\u0b28 \u0b2c\u0b3f\u0b1c\u0b4d\u0b1e\u0b3e\u0b28 \u0b2e\u0b27\u0b4d\u0b5f\u0b30\u0b47 \u0b15\u2019\u0b23 \u0b2a\u0b3e\u0b30\u0b4d\u0b25\u0b15\u0b4d\u0b5f \u0b05\u0b1b\u0b3f?\"\ntemperature=0.0\nmax_tokens=2048\npresence_penalty=1.2\nmin_p=0.05\necho -e \"\\n\\nQwen 1.5 7B Chat:\"\nname=\"qwen-base-chat\"\ncurl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -X POST \\\n    -d '{\n        \"model\": \"'$name'\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"'\"$question\"'\"}\n        ],\n        \"temperature\": '$temperature',\n        \"max_tokens\": '$max_tokens',\n        \"presence_penalty\": '$presence_penalty',\n        \"min_p\": '$min_p'\n    }' 2&gt;/dev/null | jq \".choices[0].message.content\" --raw-output | fold -s | sed 's/^/  /'\necho -e \"\\n\\nQwen 1.5 7B Odia-CPT Base:\"\nname=\"qwen-odia-base\"\ncurl http://localhost:8090/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -X POST \\\n    -d '{\n        \"model\": \"'$name'\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"'\"$question\"'\"}\n        ],\n        \"temperature\": '$temperature',\n        \"max_tokens\": '$max_tokens',\n        \"presence_penalty\": '$presence_penalty',\n        \"min_p\": '$min_p'\n    }' 2&gt;/dev/null | jq \".choices[0].message.content\" --raw-output | fold -s | sed 's/^/  /'\necho -e \"\\n\\nQwen 1.5 Odia-CPT Instruction-tuned model v1:\"\nname=\"qwen-odia-instruct-v1\"\ncurl http://localhost:8100/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -X POST \\\n    -d '{\n        \"model\": \"'$name'\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"'\"$question\"'\"}\n        ],\n        \"temperature\": '$temperature',\n        \"max_tokens\": '$max_tokens',\n        \"presence_penalty\": '$presence_penalty',\n        \"min_p\": '$min_p'\n    }' 2&gt;/dev/null | jq \".choices[0].message.content\" --raw-output | fold -s | sed 's/^/  /'\n</code></pre> We'll find that the general Qwen chat-model does not keep to Odia, it easily switches to e.g. Hindi or Chinese. The Odia continued pretraining basemodel does answer in Odia, but since it is just a continuation model, the text it produces is not really answer to the question, but rather rambling. The fine-tuned model answers the question in Odia.</p> <p>When we're done chatting with the models, we can kill the port-forwards with:</p> <p><pre><code>kill $qwenchatPID $odiabasePID $odiainstructPID\n</code></pre> and we can shut down the inference deployments with: <pre><code>for name in qwen-base-chat qwen-odia-base qwen-odia-instruct-v1; do\n  kubectl delete deployment llm-inference-vllm-$name\ndone\n</code></pre></p>"},{"location":"docs/tutorials/tutorial-03-deliver-resources-and-run-megatron-cpt/","title":"Tutorial 03: Deliver model and data to cluster MinIO, then run Megatron-LM continuous pretraining","text":"<p>This tutorial involves the following steps: 1. Download a model from the HuggingFace Hub in HuggingFace Transformers format, convert it to the Megatron-LM compatible format, and save it to the cluster-internal MinIO storage server. 2. Download a sample dataset from the HuggingFace Hub in <code>jsonl</code> format, preprocess it into the Megatron-LM compatible format, and store it in a cluster-internal MinIO storage server. 3. Execute a multi-node Megatron-LM continuous pretraining job using the base model and dataset prepared in steps 1 and 2, and saving the resulting checkpoints to the cluster-internal MinIO storage. 4. Perform an inference workload using the final checkpoint from step 3 in Megatron-LM format to validate the results.</p>"},{"location":"docs/tutorials/tutorial-03-deliver-resources-and-run-megatron-cpt/#1-setup","title":"1. Setup","text":"<p>Follow the setup in the tutorial 0 prerequisites section.</p>"},{"location":"docs/tutorials/tutorial-03-deliver-resources-and-run-megatron-cpt/#2-run-workloads","title":"2. Run workloads","text":""},{"location":"docs/tutorials/tutorial-03-deliver-resources-and-run-megatron-cpt/#21-prepare-model-in-megatron-lm-format","title":"2.1 Prepare model in Megatron-LM format","text":""},{"location":"docs/tutorials/tutorial-03-deliver-resources-and-run-megatron-cpt/#211-download-model","title":"2.1.1 Download model","text":"<p>To download the <code>meta-llama/llama-3.1-8B</code> model from the HuggingFace Hub and upload it to the in-cluster MinIO bucket, use the Helm chart located at <code>workloads/download-huggingface-model-to-bucket/helm</code>.</p> <pre><code>helm template workloads/download-huggingface-model-to-bucket/helm \\\n  --values workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-03-llama-3.1-8b.yaml \\\n  --name-template \"download-llama3-1-8b\" \\\n  | kubectl apply -f -\n</code></pre> <p>The model will be stored in the remote MinIO bucket at the path <code>default-bucket/models/meta-llama/Llama-3.1-8B</code> after being downloaded from the HuggingFace Hub.</p>"},{"location":"docs/tutorials/tutorial-03-deliver-resources-and-run-megatron-cpt/#212-convert-model-checkpoints-to-megatron-lm-format","title":"2.1.2 Convert model checkpoints to Megatron-LM format","text":"<p>To convert the model checkpoints into the Megatron-LM compatible format, use the Helm chart located at <code>workloads/llm-megatron-ckpt-conversion/helm</code>.</p> <pre><code>helm template workloads/llm-megatron-ckpt-conversion/helm \\\n  --values workloads/llm-megatron-ckpt-conversion/helm/overrides/tutorial-03-llama-3.1-8b.yaml \\\n  --name-template \"llama3-1-8b\" \\\n  | kubectl create -f -\n</code></pre> <p>The conversion process begins by copying the model checkpoint files from the MinIO storage to the workload's working directory. These checkpoint files are then processed within the conversion container to transform them into the Megatron-LM compatible format. Once the conversion is complete, the transformed checkpoint is uploaded back to the internal MinIO storage at the location <code>default-bucket/megatron-models/meta-llama/Llama-3.1-8B/</code> for subsequent use.</p>"},{"location":"docs/tutorials/tutorial-03-deliver-resources-and-run-megatron-cpt/#22-prepare-data-in-megatron-lm-format","title":"2.2 Prepare data in Megatron-LM format","text":"<p>We will use the Helm chart located at <code>workloads/prepare-data-for-megatron-lm/helm</code> to download and preprocess a sample of the <code>HuggingFaceFW/fineweb-edu</code> dataset using the HuggingFace tokenizer downloaded during the previous step Download model.</p> <p>The user input file is <code>workloads/prepare-data-for-megatron-lm/helm/overrides/tutorial-03-fineweb-data-sample.yaml</code>.</p> <pre><code>helm template workloads/prepare-data-for-megatron-lm/helm \\\n  --values workloads/prepare-data-for-megatron-lm/helm/overrides/tutorial-03-fineweb-data-sample.yaml \\\n  --name-template \"prepare-fineweb-data\" \\\n  | kubectl apply -f -\n</code></pre> <p>Refer to the Monitoring progress, logs, and GPU utilization with k9s section to track data and tokenizer downloads, data preprocessing, and uploads to the in-cluster MinIO bucket.</p>"},{"location":"docs/tutorials/tutorial-03-deliver-resources-and-run-megatron-cpt/#23-run-multi-node-megatron-lm-continuous-pretraining-job","title":"2.3 Run multi-node Megatron-LM continuous pretraining job","text":"<p>To launch the Megatron-LM pretraining job use the Helm chart located at <code>workloads/llm-pretraining-megatron-lm-ray/helm</code>. Use the following command:</p> <pre><code>helm template workloads/llm-pretraining-megatron-lm-ray/helm \\\n  --values workloads/llm-pretraining-megatron-lm-ray/helm/overrides/tutorial-03-values-llama-8b-16ddp.yaml \\\n  | kubectl apply -f -\n</code></pre>"},{"location":"docs/tutorials/tutorial-03-deliver-resources-and-run-megatron-cpt/#24-run-inference-workload-with-the-final-checkpoint-23-and-query-it-using-sample-prompts-on-llama-31-8b","title":"2.4 Run inference workload with the final checkpoint (2.3) and query it using sample prompts on Llama-3.1-8B","text":"<p>In order to perform inference with the just trained Llama-3.1-8B model and verify it's quality, follow the steps:</p> <ol> <li>Execute the Llama-3.1-8B single-node Megatron-LM inference workload. This step verifies that the model is correctly deployed and can respond to basic prompts.</li> <li>Query the model with a simple prompt to confirm it generates coherent responses.</li> </ol>"},{"location":"docs/tutorials/tutorial-03-deliver-resources-and-run-megatron-cpt/#241-run-megatron-lm-inference-workload","title":"2.4.1 Run Megatron-LM inference workload","text":"<pre><code>helm template workloads/llm-inference-megatron-lm/helm/ \\\n  --values workloads/llm-inference-megatron-lm/helm/overrides/tutorial-03-llama-3-1-8b.yaml \\\n  | kubectl apply -f -\n</code></pre>"},{"location":"docs/tutorials/tutorial-03-deliver-resources-and-run-megatron-cpt/#242-monitoring-progress-logs-and-gpu-utilization-with-k9s","title":"2.4.2 Monitoring progress, logs, and GPU utilization with k9s","text":"<p>To monitor training progress, view workload logs, and observe GPU utilization, we recommend using k9s. Refer to the official documentation for detailed guidance. Below are basic commands for this tutorial:</p> <p>To access the Pods view in the your namespace, run:</p> <pre><code>k9s --command pods\n</code></pre> <p>Navigate using the <code>arrow keys</code> to select a the pod containg the keyword \"inference\" and  and press <code>Enter</code> to view the pod running the inference server. View logs by pressing <code>l</code>. Logs display output messages generated during runtime. Press <code>Esc</code> to return to the previous <code>k9s</code> view.</p>"},{"location":"docs/tutorials/tutorial-03-deliver-resources-and-run-megatron-cpt/#243-connect-to-the-inference-service-and-query-it-to-sample-prompt-continuations","title":"2.4.3 Connect to the inference service and query it to sample prompt continuations","text":"<p>First, check the deployment status:</p> <p><pre><code>kubectl get deployment\n</code></pre> You should see a deployment with a name in the format <code>llm-inference-megatron-lm-YYYYMMDD-HHMM</code> (e.g. <code>llm-inference-megatron-lm-20250811-1229</code>) in ready state.</p> <p>Get the name of the respective service deployed by the workload with</p> <pre><code>kubectl get svc\n</code></pre> <p>The service should have the same name as the deployment from above with the format <code>llm-inference-megatron-lm-YYYYMMDD-HHMM</code>. Note the port exposed by the service, it is expected to be the port <code>80</code>.</p> <p>Forward the service port to your local machine, e.g., in the example below, remote port <code>80</code> to local port <code>5000</code>. For example, use the following command, and do not forget to replace <code>llm-inference-megatron-lm-YYYYMMDD-HHMM</code> with your real service name:</p> <pre><code>kubectl port-forward svc/llm-inference-megatron-lm-YYYYMMDD-HHMM 5000:80\n</code></pre> <p>Now the inference API is available at <code>http://localhost:5000</code>.</p> <p>You can use <code>curl</code> to send requests to the inference API. Make sure you have the service port forwarded as shown above. Send a simple prompt to the model to check if it responds coherently. For example:</p> <pre><code>curl -X PUT -H \"Content-Type: application/json\" \\\n  -d '{\"prompts\": [\"What is the capital of France?\"], \"tokens_to_generate\": 32}' \\\n  http://localhost:5000/api\n</code></pre> <p>You should receive a JSON response with the model\u2019s answer. For a healthy model, the answer should be <code>\"Paris\"</code> or similar with some extra text.</p> <p>Try a few more prompts to check basic reasoning and language ability:</p> <pre><code>curl -X PUT -H \"Content-Type: application/json\" \\\n  -d '{\"prompts\": [\"2 + 2 = ?\"], \"tokens_to_generate\": 8}' \\\n  http://localhost:5000/api\n\ncurl -X PUT -H \"Content-Type: application/json\" \\\n  -d '{\"prompts\": [\"Write a short greeting.\"], \"tokens_to_generate\": 16}' \\\n  http://localhost:5000/api\n</code></pre>"},{"location":"docs/tutorials/tutorial-04-deliver-llama70b-and-run-megatron-cpt-with-tp8-ddp2/","title":"Tutorial 04: Deliver Llama 70B and data to cluster MinIO, then run Megatron-LM continuous pretraining with DDP=2 (distributed data parallelism) and TP=8 (tensor parallelism)","text":"<p>This tutorial involves the following steps: 1. Download Llama 3.1 70B from the HuggingFace Hub in HuggingFace Transformers format, convert it to the Megatron-LM compatible format, and save it to the cluster-internal MinIO storage server. 2. Download a sample dataset from the HuggingFace Hub in <code>jsonl</code> format, preprocess it into the Megatron-LM compatible format, and store it in a cluster-internal MinIO storage server. 3. Execute a multi-node Megatron-LM continued pretraining job using distributed data parallelism (DDP=2) and tensor parallelism (TP=8), and saving the resulting checkpoints to the cluster-internal MinIO storage. 4. Perform an inference workload using the final checkpoint from step 3 in Megatron-LM format to validate the results.</p>"},{"location":"docs/tutorials/tutorial-04-deliver-llama70b-and-run-megatron-cpt-with-tp8-ddp2/#1-setup","title":"1. Setup","text":"<p>Follow the setup in the tutorial 0 prerequisites section.</p>"},{"location":"docs/tutorials/tutorial-04-deliver-llama70b-and-run-megatron-cpt-with-tp8-ddp2/#2-run-workloads","title":"2. Run workloads","text":""},{"location":"docs/tutorials/tutorial-04-deliver-llama70b-and-run-megatron-cpt-with-tp8-ddp2/#21-prepare-model-in-megatron-lm-format","title":"2.1 Prepare model in Megatron-LM format","text":""},{"location":"docs/tutorials/tutorial-04-deliver-llama70b-and-run-megatron-cpt-with-tp8-ddp2/#211-download-model","title":"2.1.1 Download model","text":"<p>To download the <code>meta-llama/llama-3.1-70B</code> model from the HuggingFace Hub and upload it to the in-cluster MinIO bucket, use the Helm chart located at <code>workloads/download-huggingface-model-to-bucket/helm</code>:</p> <pre><code>helm template workloads/download-huggingface-model-to-bucket/helm \\\n  --values workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-04-llama-3.1-70b.yaml \\\n  --name-template \"download-llama3-1-70b\" \\\n  | kubectl apply -f -\n</code></pre> <p>The model will be stored in the remote MinIO bucket at the path <code>default-bucket/models/meta-llama/Llama-3.1-70B</code> after being downloaded from the HuggingFace Hub.</p>"},{"location":"docs/tutorials/tutorial-04-deliver-llama70b-and-run-megatron-cpt-with-tp8-ddp2/#212-convert-model-checkpoints-to-megatron-lm-format","title":"2.1.2 Convert model checkpoints to Megatron-LM format","text":"<p>To convert the model checkpoints into the Megatron-LM compatible format, use the Helm chart located at <code>workloads/llm-megatron-ckpt-conversion/helm</code>:</p> <pre><code>helm template workloads/llm-megatron-ckpt-conversion/helm \\\n  --values workloads/llm-megatron-ckpt-conversion/helm/overrides/tutorial-04-llama-3.1-70b.yaml \\\n  --name-template \"llama3-1-70b\" \\\n  | kubectl create -f -\n</code></pre> <p>The conversion process begins by copying the model checkpoints into the minio container. These checkpoints are then processed within the conversion container to transform them into the Megatron-LM compatible format. Once the conversion is complete, the transformed checkpoints are uploaded back to the internal MinIO storage at the location <code>default-bucket/megatron-models/meta-llama/Llama-3.1-70B/</code> for subsequent use.</p>"},{"location":"docs/tutorials/tutorial-04-deliver-llama70b-and-run-megatron-cpt-with-tp8-ddp2/#22-prepare-data-in-megatron-lm-format","title":"2.2 Prepare data in Megatron-LM format","text":"<p>We will use the Helm chart located at <code>workloads/prepare-data-for-megatron-lm/helm</code> to download and preprocess a sample of the <code>HuggingFaceFW/fineweb-edu</code> dataset and use the tokenizer of the <code>meta-llama/Llama-3.1-70B</code> model.</p> <p>The user input file is <code>workloads/prepare-data-for-megatron-lm/helm/overrides/tutorial-04-fineweb-data-sample.yaml</code>.</p> <pre><code>helm template workloads/prepare-data-for-megatron-lm/helm \\\n  --values workloads/prepare-data-for-megatron-lm/helm/overrides/tutorial-04-fineweb-data-sample.yaml \\\n  --name-template \"prepare-fineweb-data\" \\\n  | kubectl apply -f -\n</code></pre> <p>Refer to the Monitoring progress, logs, and GPU utilization with k9s section to track data and tokenizer downloads, data preprocessing, and uploads to the in-cluster MinIO bucket.</p>"},{"location":"docs/tutorials/tutorial-04-deliver-llama70b-and-run-megatron-cpt-with-tp8-ddp2/#23-run-multi-node-megatron-lm-continuous-pretraining-job","title":"2.3 Run multi-node Megatron-LM continuous pretraining job","text":"<p>To launch the Megatron-LM pretraining job use the Helm chart located at <code>workloads/llm-pretraining-megatron-lm-ray/helm</code>. Use the following command:</p> <pre><code>helm template workloads/llm-pretraining-megatron-lm-ray/helm \\\n  --values workloads/llm-pretraining-megatron-lm-ray/helm/overrides/tutorial-04-values-llama-70b-2ddp.yaml \\\n  | kubectl apply -f -\n</code></pre>"},{"location":"docs/tutorials/tutorial-04-deliver-llama70b-and-run-megatron-cpt-with-tp8-ddp2/#24-run-inference-workload-with-the-final-checkpoint-23-and-query-it-using-sample-prompts-on-llama-31-70b","title":"2.4 Run inference workload with the final checkpoint (2.3) and query it using sample prompts on Llama-3.1-70B","text":"<p>In order to perform inference with the just trained Llama-3.1-70B model and verify it's quality, follow the steps:</p> <ol> <li>Execute the Llama-3.1-70B single-node Megatron-LM inference workload. This step verifies that the model is correctly deployed and can respond to basic prompts.</li> <li>Query the model with a simple prompt to confirm it generates coherent responses.</li> </ol>"},{"location":"docs/tutorials/tutorial-04-deliver-llama70b-and-run-megatron-cpt-with-tp8-ddp2/#241-run-megatron-lm-inference-workload","title":"2.4.1 Run Megatron-LM inference workload","text":"<pre><code>helm template workloads/llm-inference-megatron-lm/helm/ \\\n  --values workloads/llm-inference-megatron-lm/helm/overrides/tutorial-04-llama-3-1-70b.yaml \\\n  | kubectl apply -f -\n</code></pre>"},{"location":"docs/tutorials/tutorial-04-deliver-llama70b-and-run-megatron-cpt-with-tp8-ddp2/#242-monitoring-progress-logs-and-gpu-utilization-with-k9s","title":"2.4.2 Monitoring progress, logs, and GPU utilization with k9s","text":"<p>To monitor training progress, view workload logs, and observe GPU utilization, we recommend using k9s. Refer to the official documentation for detailed guidance. Below are basic commands for this tutorial:</p> <p>To access the Pods view in the your namespace, run:</p> <pre><code>k9s --command pods\n</code></pre> <p>Navigate using the <code>arrow keys</code> to select a the pod containg the keyword \"inference\" and  and press <code>Enter</code> to view the pod running the inference server. View logs by pressing <code>l</code>. Logs display output messages generated during runtime. Press <code>Esc</code> to return to the previous <code>k9s</code> view.</p>"},{"location":"docs/tutorials/tutorial-04-deliver-llama70b-and-run-megatron-cpt-with-tp8-ddp2/#243-connect-to-the-inference-service-and-query-it-to-sample-prompt-continuations","title":"2.4.3 Connect to the inference service and query it to sample prompt continuations","text":"<p>First, check the deployment status:</p> <p><pre><code>kubectl get deployment\n</code></pre> You should see a deployment with a name in the format <code>llm-inference-megatron-lm-YYYYMMDD-HHMM</code> (e.g. <code>llm-inference-megatron-lm-20250811-1229</code>) in ready state.</p> <p>Get the name of the respective service deployed by the workload with</p> <pre><code>kubectl get svc\n</code></pre> <p>The service should have the same name as the deployment from above with the format <code>llm-inference-megatron-lm-YYYYMMDD-HHMM</code>. Note the port exposed by the service, it is expected to be the port <code>80</code>.</p> <p>Forward the service port to your local machine, e.g., in the example below, remote port <code>80</code> to local port <code>5000</code>. For example, use the following command, and do not forget to replace <code>llm-inference-megatron-lm-YYYYMMDD-HHMM</code> with your real service name:</p> <pre><code>kubectl port-forward svc/llm-inference-megatron-lm-YYYYMMDD-HHMM 5000:80\n</code></pre> <p>Now the inference API is available at <code>http://localhost:5000</code>.</p> <p>You can use <code>curl</code> to send requests to the inference API. Make sure you have the service port forwarded as shown above. Send a simple prompt to the model to check if it responds coherently. For example:</p> <pre><code>curl -X PUT -H \"Content-Type: application/json\" \\\n  -d '{\"prompts\": [\"What is the capital of France?\"], \"tokens_to_generate\": 32}' \\\n  http://localhost:5000/api\n</code></pre> <p>You should receive a JSON response with the model\u2019s answer. For a healthy model, the answer should be <code>\"Paris\"</code> or similar with some extra text.</p> <p>Try a few more prompts to check basic reasoning and language ability:</p> <pre><code>curl -X PUT -H \"Content-Type: application/json\" \\\n  -d '{\"prompts\": [\"2 + 2 = ?\"], \"tokens_to_generate\": 8}' \\\n  http://localhost:5000/api\n\ncurl -X PUT -H \"Content-Type: application/json\" \\\n  -d '{\"prompts\": [\"Write a short greeting.\"], \"tokens_to_generate\": 16}' \\\n  http://localhost:5000/api\n</code></pre>"},{"location":"workloads/","title":"Workloads","text":"<p>This directory contains all the workloads. Each workload has its own directory, with different formats defined in subdirectories.</p> <p>To create a new workload, you can duplicate an existing workload and adapt as needed.</p> <p>The files needed to create different instantiations of a workload depends on the format. For reference, see the existing workloads, and the documentation of the respective formats.</p> <p>For an introduction to AI workloads see the Workloads overview</p>"},{"location":"workloads/workloads-overview/","title":"Working with AI workloads on Kubernetes","text":""},{"location":"workloads/workloads-overview/#overview","title":"Overview","text":"<p>This document provides a general introduction to deploying AI workloads on Kubernetes using Helm charts and Kubernetes manifests. While each specific AI workload in our solution has its own dedicated documentation, this guide explains the common concepts, structure, and deployment patterns that apply across all our AI workload Helm charts.</p> <p>Each AI workload is defined as a separate Helm chart that can be submitted to run on a Kubernetes platform.</p>"},{"location":"workloads/workloads-overview/#what-is-helm","title":"What is Helm?","text":"<p>Helm is the package manager for Kubernetes, often referred to as \"the apt/yum for K8s.\" It simplifies the deployment and management of applications by:</p> <ul> <li>Packaging Kubernetes resources into reusable charts (pre-configured templates).</li> <li>Managing dependencies, versions, and configurations.</li> <li>Supporting easy upgrades, rollbacks, and customization.</li> </ul> <p>Think of it as a blueprint that defines how an application should be installed and run in a Kubernetes cluster.</p>"},{"location":"workloads/workloads-overview/#how-does-helm-relate-to-kubernetes-manifests","title":"How does Helm relate to Kubernetes manifests?","text":"<p>Helm doesn't replace manifests - it generates them dynamically using: - Templates: Manifest files with variables (in templates/ directory) - Values: Configuration that fills the template variables</p>"},{"location":"workloads/workloads-overview/#why-use-helm-for-ai-workloads","title":"Why Use Helm for AI Workloads?","text":"<p>Deploying AI/ML workloads (inference, training, batch processing) on Kubernetes can be complex due to:</p> <ul> <li>Dependencies (models, datasets, GPU drivers).</li> <li>Configuration variability (resource limits, scaling, model versions).</li> <li>Reproducibility (consistent deployments across dev/test/prod).</li> </ul> <p>Helm addresses these challenges by:</p> <ol> <li>Standardizing Deployments:</li> <li> <p>AI components (model servers, preprocessing, monitoring) are defined in a Helm chart (deployment.yaml, service.yaml, etc.).</p> </li> <li> <p>Managing Configurations:</p> </li> <li> <p>values.yaml centralizes tunable parameters (e.g., replicaCount, modelPath, GPU limits). Overrides allow specifying specific models, datasets or environment-specific setups (e.g., dev vs. prod):</p> </li> <li> <p>Supporting AI-Specific Needs</p> </li> <li>GPU/accelerator support: Define resource requests in values.yaml.</li> <li>Model storage: Mount PersistentVolumes or download models at runtime.</li> <li>Scaling: Pre-configure Horizontal Pod Autoscaler (HPA) for inference workloads.</li> </ol>"},{"location":"workloads/workloads-overview/#structure-of-the-workloads","title":"Structure of the workloads","text":"<pre><code>&lt;workload-name&gt;/\n\u251c\u2500\u2500 Chart.yaml          # Metadata about the chart, such as the release name.\n\u251c\u2500\u2500 values.yaml         # Default configuration values for the workload.\n\u251c\u2500\u2500 templates/          # Kubernetes manifest templates that Helm uses to generate actual manifests\n\u2502   \u251c\u2500\u2500 deployment.yaml # Configuration of the AI workload deployment\n\u2502   \u251c\u2500\u2500 service.yaml    # Configuration of the Kubernetes service\n\u2514\u2500\u2500 overrides           # Customization of the AI workload without modifying the original chart\n</code></pre>"},{"location":"workloads/workloads-overview/#understanding-the-valuesyaml-file","title":"Understanding the values.yaml file","text":"<p>The values.yaml file is a YAML-formatted configuration file that contains key-value pairs representing the parameters required for your model inference deployment. These parameters include: - Image name: name of the docker image to use</p> <ul> <li> <p>Resources: Define GPU, CPU and memory requirements for your deployment.</p> </li> <li> <p>Model: Provide the name of the model.</p> </li> <li> <p>Storage details</p> </li> <li> <p>Environment Variables: Set environment-specific configurations, such as secrets and storage host location.</p> </li> <li> <p>Service Configuration: Customize service settings like ports, timeouts, and logging levels.</p> </li> </ul>"},{"location":"workloads/workloads-overview/#overrides","title":"Overrides","text":"<p>Overrides allow customization of the AI workload without modifying the original chart. This includes changing the model and data sets.</p>"},{"location":"workloads/workloads-overview/#running-ai-workloads-on-k8s","title":"Running AI workloads on k8s","text":"<p>It is recommended to use <code>helm template</code> and pipe the result to <code>kubectl create</code> , rather than using <code>helm install</code>. Generally, a command looks as follows</p> <p><pre><code>helm template &lt;release-name&gt; . -f &lt;override.yaml&gt; | kubectl apply -f -\n</code></pre> or <pre><code>helm template &lt;release-name&gt; . --set &lt;parameter&gt;=&lt;value&gt; | kubectl apply -f -\n</code></pre></p>"},{"location":"workloads/workloads-overview/#how-to-use-overrides-to-customize-the-workload","title":"How to use overrides to customize the workload","text":"<p>There are multiple options you can use to customize the workload by applying overrides.</p> <p>Alternative 1: Deploy a Specific Model Configuration</p> <p>To deploy a specific model along with its settings, use the following command from the <code>helm</code> directory:</p> <pre><code>helm template tiny-llama . -f overrides/models/tinyllama_tinyllama-1.1b-chat-v1.0.yaml | kubectl apply -f -\n</code></pre> <p>Alternative 2: Override the Model</p> <p>You can also override the model on the command line:</p> <pre><code>helm template qwen2-0-5b . --set model=Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre> <p>Helm merges overrides in this order (last takes precedence):</p> <ol> <li>values.yaml (in chart)</li> <li>-f files (in command order)</li> <li>--set arguments</li> </ol>"},{"location":"workloads/workloads-overview/#verifying-the-deployment","title":"Verifying the Deployment","text":"<p>Check pods <pre><code>kubectl get pods -n &lt;namespace&gt;\n</code></pre> Check services <pre><code>kubectl get svc -n &lt;namespace&gt;\n</code></pre> View logs (for a specific pod) <pre><code>kubectl logs -f &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre></p>"},{"location":"workloads/workloads-overview/#monitoring-progress-logs-and-gpu-utilization-with-k9s","title":"Monitoring progress, logs, and GPU utilization with k9s","text":"<p>We're interested to see a progress bar of the finetuning training, seeing any messages that a workload logs, and we also want to verify that our GPU Jobs are consuming our compute relatively effectively. This information can be fetched from our Kubernetes cluster in many ways, but one convenient and recommended way us using k9s. We recommend the official documentation for more thorough guidance, but this section shows some basic commands to get what we want here.</p> <p>To get right to the Jobs view in the namespace <code>&lt;namespace&gt;</code>, we can run:</p> <pre><code>k9s --namespace &lt;namespace&gt; --command Jobs\n</code></pre> <p>Choose a Job using <code>arrow keys</code> and <code>Enter</code> to see the Pod that it spawned, then <code>Enter</code> again to see the Container in the Pod. From here, we can do three things:</p> <ul> <li> <p>Look at the logs by pressing <code>l</code>. The logs show any output messages produced during the workload runtime.</p> </li> <li> <p>Attach to the output of the container by pressing <code>a</code>. This is particularly useful to see the interactive progress bar of a finetuning run.</p> </li> <li> <p>Spawn a shell inside the container by pressing <code>s</code>. Inside the shell we can run <code>watch -n0.5 rocm-smi</code> to get a view of the GPU utilization that updates every 0.5s.</p> </li> </ul> <p>Return from any regular <code>k9s</code> view with <code>Esc</code> .</p>"},{"location":"workloads/workloads-overview/#editing-reusing-workloads","title":"Editing &amp; reusing workloads","text":"<p>To create a new workload, you can duplicate an existing workload and adapt as needed.</p> <p>Example: Using your own model and data in the workloads</p>"},{"location":"workloads/dev-chatui-aiaio/helm/","title":"dev-chatui-aiaio Helm Chart","text":"<p>This Helm chart is used to deploy dev-chatui-aiaio workload, i.e. the aiaio Chat UI. It is a simple application, making it a good example of how to package an application as a workload.</p>"},{"location":"workloads/dev-chatui-aiaio/helm/#installation","title":"Installation","text":"<p>To install the chart with the release name <code>my-release</code>:</p> <pre><code>helm template my-release ./helm | kubectl apply -f -\n</code></pre>"},{"location":"workloads/dev-chatui-aiaio/helm/#configuration","title":"Configuration","text":"<p>The following table lists the configurable parameters of the <code>dev-chatui-aiaio</code> chart and their default values.</p> Parameter Description Default <code>image</code> Image repository <code>ghcr.io/silogen/aiaio:v20250221</code> <code>image.pullPolicy</code> Image pull policy <code>Always</code> <code>env_vars</code> Environment variables <code>{}</code> <code>storage.ephemeral.quantity</code> Ephemeral storage quantity <code>1Gi</code> <code>storage.ephemeral.storageClassName</code> Storage class name <code>mlstorage</code> <code>storage.ephemeral.accessModes</code> Access modes <code>[ ReadWriteOnce ]</code> <code>storage.dshm.sizeLimit</code> Shared memory size limit <code>1Gi</code> <code>deployment.port</code> Deployment port <code>9000</code> <code>entrypoint</code> Entrypoint command <code>\"\"</code>"},{"location":"workloads/dev-chatui-aiaio/helm/#example","title":"Example","text":"<p>To deploy the chart with a custom values file <code>values_override.yaml</code>:</p> <pre><code>helm template my-release ./helm -f values_override.yaml | kubectl apply -f -\n</code></pre>"},{"location":"workloads/dev-chatui-aiaio/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/dev-chatui-openwebui/helm/","title":"Open WebUI for LLM Services","text":"<p>This Helm Chart deploys a WebUI for aggregating deployed LLM services within the cluster.</p>"},{"location":"workloads/dev-chatui-openwebui/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>The basic configurations for deployment are defined in the <code>values.yaml</code> file.</p> <p>To deploy the service, execute the following command from the Helm folder:</p> <pre><code>helm template &lt;release-name&gt; . | kubectl apply -f -\n</code></pre>"},{"location":"workloads/dev-chatui-openwebui/helm/#automatic-discovery-and-health-checks-for-llm-services","title":"Automatic Discovery and Health Checks for LLM Services","text":"<p>OpenAI-compatible endpoints can by specified by the user through the <code>env_vars.OPENAI_API_BASE_URLS</code> environment variable. Additionally, service discovery is used to include all OpenAI-compatible LLM inference services running in the same namespace.</p>"},{"location":"workloads/dev-chatui-openwebui/helm/#client-side-service-discovery-optional","title":"Client-Side Service Discovery (Optional)","text":"<p>Client-side discovery can be performed using the <code>--dry-run=server</code> flag:</p> <pre><code>helm template &lt;release-name&gt; . --dry-run=server | kubectl apply -f -\n</code></pre> <p>For a service to be included in <code>OPENAI_API_BASE_URLS_AUTODISCOVERY</code> during client-side discovery: - The service must be running in the same namespace. - The service name must start with <code>llm-inference-</code>.</p>"},{"location":"workloads/dev-chatui-openwebui/helm/#server-side-service-discovery","title":"Server-Side Service Discovery","text":"<p>The system performs server-side discovery of LLM inference services automatically. For a service to be included, the following conditions must be met: - The service must be running in the same namespace. - The service name must start with <code>llm-inference-</code>. - The pod's service account must have the necessary permissions to check running services in the namespace (configured via role-binding).</p>"},{"location":"workloads/dev-chatui-openwebui/helm/#health-checks-and-filtering","title":"Health Checks and Filtering","text":"<p>Before finalizing <code>OPENAI_API_BASE_URLS</code> and starting the service, the URLs specified by the user and the auto-discovered services are merged, and filtered based on a health check.</p> <p>For a service to be included in the final <code>OPENAI_API_BASE_URLS</code>: - The service must respond successfully to the <code>/v1/models</code> endpoint with an HTTP status code of 200.</p> <p>The final <code>OPENAI_API_BASE_URLS</code> determines what services/models are included in Open WebUI interface.</p>"},{"location":"workloads/dev-chatui-openwebui/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/dev-chatui-rag-example-app/helm/","title":"RAG Chat App (Infinity + VLLM + Gradio)","text":"<p>This Helm chart deploys a Retrieval-Augmented Generation (RAG) Chat Application that combines:</p> <p>Infinity for document embeddings VLLM for LLM-based text generation Chroma for vector storage Gradio for a user-friendly chat interface</p>"},{"location":"workloads/dev-chatui-rag-example-app/helm/#features","title":"Features","text":"<p>Embed your own .txt and .pdf documents (supports flat and nested folders) Retrieve and synthesize answers using a VLLM-hosted LLM Simple Gradio chat UI for interactive Q&amp;A All application code and knowledge base are packaged and deployed via Kubernetes ConfigMaps</p>"},{"location":"workloads/dev-chatui-rag-example-app/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<ol> <li>Deploy Infinity and VLLM Services Make sure you have the Infinity and VLLM services running in your cluster (see your infra repo or use provided Helm charts).</li> <li>Deploy the RAG Chat App Render and apply the Helm chart:</li> </ol> <pre><code>helm template my-app . | kubectl apply -f -\n</code></pre> <p>To use specific embedding or generation model (if you are running non-default infinity and vllm workloads):</p> <pre><code>helm template my-app . \\\n  --set infinity.host=\"rag-embedding-infinity-e5-base\" \\\n  --set infinity.port=80 \\\n  --set infinity.model=\"intfloat/multilingual-e5-base\" \\\n  --set vllm.host=\"llm-inference-vllm-qwen\" \\\n  --set vllm.port=80 \\\n  --set vllm.model=\"Qwen/Qwen2-0.5B-Instruct\" \\\n  | kubectl apply -f -\n</code></pre> <p>If \"host\" is a name of the service that is running the corresponding workload you should use port 80 (default).</p>"},{"location":"workloads/dev-chatui-rag-example-app/helm/#user-input-values","title":"User Input Values","text":"<p>See <code>values.yaml</code> for all configurable options, such as:</p> <p>Infinity/VLLM host and port</p> <p>Model names</p> <p>Resource requests/limits</p>"},{"location":"workloads/dev-chatui-rag-example-app/helm/#interacting-with-the-app","title":"Interacting with the App","text":"<ol> <li>Verify Deployment</li> </ol> <pre><code>kubectl get svc\nkubectl get deployment\nkubectl get pod\n</code></pre> <ol> <li>Port Forward to Access Gradio UI <pre><code>kubectl port-forward svc/my-app-rag-chat-app 8080:80\n</code></pre></li> </ol> <p>Then open http://localhost:8080 in your browser.</p> <ol> <li>Chat with Your Documents Upload your documents using UI function. Use the UI to ask questions about your uploaded documents. The app will embed, retrieve, and synthesize answers using your configured models.</li> </ol>"},{"location":"workloads/dev-chatui-rag-example-app/helm/#how-it-works","title":"How It Works","text":"<p>Documents are chunked and embedded, with results saved into ChromaDB. Embeddings are generated via the Infinity service. Relevant chunks are retrieved from a Chroma vectorstore (persisted in an ephemeral volume). Answers are generated by VLLM and displayed in the Gradio chat UI.</p>"},{"location":"workloads/dev-chatui-rag-example-app/helm/#cleanup","title":"Cleanup","text":"<p>After using the app, please delete the resources created.</p> <p>You can run the same <code>helm template</code> command, only replacing <code>kubectl apply</code> with <code>kubectl delete</code>, e.g.:</p> <p>```bash helm template my-app . | kubectl delete -f -</p>"},{"location":"workloads/dev-text2image-comfyui/helm/","title":"ComfyUI Text-to-Image/Video Workload","text":"<p>This Helm Chart deploys a ComfyUI web app for text-to-image/video generation. ComfyUI is a powerful node-based interface for stable diffusion that provides advanced workflows for AI image and video generation.</p>"},{"location":"workloads/dev-text2image-comfyui/helm/#features","title":"Features","text":"<ul> <li>Pre-configured ComfyUI Environment: Automatically installs and configures ComfyUI with ROCm support</li> <li>Model Management: Support for downloading models from HuggingFace or MinIO/S3 storage</li> <li>ComfyUI Manager: Includes ComfyUI Manager for easy extension management</li> </ul>"},{"location":"workloads/dev-text2image-comfyui/helm/#configuration-parameters","title":"Configuration Parameters","text":"<p>You can configure the following parameters in the <code>values.yaml</code> file or override them via the command line:</p> Parameter Description Default <code>image</code> Container image repository and tag <code>rocm/dev-ubuntu-22.04:6.2.4</code> <code>gpus</code> Number of GPUs to allocate <code>1</code> <code>model</code> HuggingFace model path (e.g., <code>Comfy-Org/flux1-dev</code>) Not set <code>tag</code> Specific model binaries (*tag*.safetensors)  to download (optional) Clone the repo when not set <code>storage.ephemeral.quantity</code> Ephemeral storage size <code>200Gi</code> <code>kaiwo.enabled</code> Enable Kaiwo operator management <code>false</code>"},{"location":"workloads/dev-text2image-comfyui/helm/#environment-variables","title":"Environment Variables","text":"<p>The following environment variables are configured for MinIO/S3 integration:</p> Variable Description Default <code>BUCKET_STORAGE_HOST</code> MinIO/S3 endpoint URL <code>http://minio.minio-tenant-default.svc.cluster.local:80</code> <code>BUCKET_STORAGE_ACCESS_KEY</code> MinIO/S3 access key (from secret) From <code>minio-credentials</code> secret <code>BUCKET_STORAGE_SECRET_KEY</code> MinIO/S3 secret key (from secret) From <code>minio-credentials</code> secret <code>PIP_DEPS</code> Additional Python packages to install via pip (space or newline separated URLs/packages) ROCm-compatible torchaudio wheel <code>COMFYUI_PATH</code> ComfyUI installation path <code>/workload/ComfyUI</code> <code>MODEL_BIN_URL</code> Direct URL to download an additional model checkpoint (optional) Not set"},{"location":"workloads/dev-text2image-comfyui/helm/#model-configuration","title":"Model Configuration","text":""},{"location":"workloads/dev-text2image-comfyui/helm/#using-huggingface-models","title":"Using HuggingFace Models","text":"<p>Configure models from HuggingFace by setting the <code>model</code> parameter:</p> <pre><code># Example: FLUX.1-dev model\nmodel: \"Comfy-Org/flux1-dev\"\ntag: \"flux1-dev-fp8\"\n</code></pre>"},{"location":"workloads/dev-text2image-comfyui/helm/#using-s3minio-models","title":"Using S3/MinIO Models","text":"<p>For models stored in S3/MinIO, use the s3:// prefix:</p> <pre><code>model: \"s3://models/Comfy-Org/flux1-dev\"\n</code></pre>"},{"location":"workloads/dev-text2image-comfyui/helm/#using-direct-download-urls","title":"Using Direct Download URLs","text":"<p>For direct model downloads, use the <code>MODEL_BIN_URL</code> environment variable:</p> <pre><code>env_vars:\n  MODEL_BIN_URL: \"https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/resolve/main/all_in_one/lumina_2.safetensors\"\n</code></pre>"},{"location":"workloads/dev-text2image-comfyui/helm/#pre-configured-model-overrides","title":"Pre-configured Model Overrides","text":"<p>The workload includes several pre-configured model overrides in the <code>overrides/models/</code> directory:</p>"},{"location":"workloads/dev-text2image-comfyui/helm/#deploying-the-workload","title":"Deploying the Workload","text":""},{"location":"workloads/dev-text2image-comfyui/helm/#basic-deployment","title":"Basic Deployment","text":"<p>To deploy the service with default settings, run the following command within the <code>helm</code> folder:</p> <pre><code>helm template . | kubectl apply -f -\n</code></pre>"},{"location":"workloads/dev-text2image-comfyui/helm/#deployment-with-model-override","title":"Deployment with Model Override","text":"<p>To deploy with a specific model configuration:</p> <pre><code>helm template flux . -f overrides/models/comfy-org_flux1-dev-fp8.yaml | kubectl apply -f -\n</code></pre>"},{"location":"workloads/dev-text2image-comfyui/helm/#custom-deployment","title":"Custom Deployment","text":"<p>To deploy with custom parameters:</p> <pre><code>helm template flux . --set model=\"Comfy-Org/flux1-dev\" | kubectl apply -f -\n</code></pre>"},{"location":"workloads/dev-text2image-comfyui/helm/#accessing-the-workload","title":"Accessing the Workload","text":""},{"location":"workloads/dev-text2image-comfyui/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment and service status:</p> <pre><code>kubectl get deployment\nkubectl get service\n</code></pre>"},{"location":"workloads/dev-text2image-comfyui/helm/#port-forwarding","title":"Port Forwarding","text":"<p>To access the service locally on port <code>8188</code>, forward the port of the service/deployment:</p> <pre><code>kubectl port-forward services/dev-text2image-comfyui 8188:80\n</code></pre> <p>Then open a web-browser and navigate to http://localhost:8188 to access ComfyUI.</p>"},{"location":"workloads/dev-text2image-comfyui/helm/#accessing-the-workload-via-url","title":"Accessing the Workload via URL","text":"<p>To access the workload through a URL, you can enable either an Ingress or HTTPRoute in the <code>values.yaml</code> file. The following parameters are available:</p> Parameter Description Default <code>ingress.enabled</code> Enable Ingress resource <code>false</code> <code>httproute.enabled</code> Enable HTTPRoute resource <code>false</code> <p>See the corresponding template files in the <code>templates/</code> directory. For more details on configuring Ingress or HTTPRoute, refer to the Ingress documentation and HTTPRoute documentation, or documentation of the particular gateway implementation you may use, like KGateway. Check with your cluster administrator for the correct configuration for your environment.</p>"},{"location":"workloads/dev-text2image-comfyui/helm/#health-checks-and-monitoring","title":"Health Checks and Monitoring","text":"<p>The workload includes comprehensive health monitoring:</p> <ul> <li>Startup Probe: Allows up to 10 minutes for ComfyUI to start (checks <code>/queue</code> endpoint)</li> <li>Liveness Probe: Monitors if ComfyUI is running properly</li> <li>Readiness Probe: Ensures ComfyUI is ready to serve requests</li> </ul>"},{"location":"workloads/dev-text2image-comfyui/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/dev-tracking-mlflow/helm/","title":"MLflow Tracking Server","text":"<p>This Helm Chart deploys an MLflow tracking server for experiment tracking and model management. The server provides a centralized location for logging metrics, parameters, and artifacts from machine learning experiments.</p>"},{"location":"workloads/dev-tracking-mlflow/helm/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following prerequisites are met before deploying this workload:</p> <ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li>MinIO Storage (recommended): Create the following secret in the namespace for artifact storage:</li> <li><code>minio-credentials</code> with keys <code>minio-access-key</code> and <code>minio-secret-key</code></li> </ol>"},{"location":"workloads/dev-tracking-mlflow/helm/#configuration-parameters","title":"Configuration Parameters","text":"<p>You can configure the following parameters in the <code>values.yaml</code> file or override them via the command line:</p>"},{"location":"workloads/dev-tracking-mlflow/helm/#backend-store-configuration","title":"Backend Store Configuration","text":"Parameter Description Default <code>backendStore.type</code> Backend storage type (sqlite, postgres, mysql) <code>sqlite</code> <code>backendStore.dbpath</code> SQLite database file path <code>/workload/mlflow.db</code> <code>backendStore.host</code> Database host (for postgres/mysql) <code>\"\"</code> <code>backendStore.port</code> Database port (for postgres/mysql) <code>\"\"</code> <code>backendStore.database</code> Database name (for postgres/mysql) <code>\"\"</code> <code>backendStore.driver</code> Database driver (for mysql) <code>\"\"</code> <code>backendStore.secret.name</code> Secret name for database credentials <code>mlflow-db-credentials</code> <code>backendStore.secret.userKey</code> Key in secret for username <code>username</code> <code>backendStore.secret.passwordKey</code> Key in secret for password <code>password</code>"},{"location":"workloads/dev-tracking-mlflow/helm/#artifact-storage-configuration","title":"Artifact Storage Configuration","text":"Parameter Description Default <code>env_vars.MLFLOW_S3_ENDPOINT_URL</code> S3-compatible storage endpoint for artifacts MinIO service URL <code>env_vars.MLFLOW_ARTIFACTS_DESTINATION</code> Artifact storage destination <code>s3://mlflow/mlartifacts</code> <code>env_vars.AWS_ACCESS_KEY_ID</code> AWS access key ID configuration from secret <code>minio-credentials</code> secret <code>env_vars.AWS_SECRET_ACCESS_KEY</code> AWS secret access key configuration from secret <code>minio-credentials</code> secret <p>For more details see the <code>values.yaml</code> file.</p>"},{"location":"workloads/dev-tracking-mlflow/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>It is recommended to use <code>helm template</code> and pipe the result to <code>kubectl apply</code>, rather than using <code>helm install</code>.</p> <p>To deploy the chart with the release name <code>mlflow-server</code>, run the following command from the <code>helm/</code> directory:</p> <pre><code>helm template mlflow-server . | kubectl apply -f -\n</code></pre>"},{"location":"workloads/dev-tracking-mlflow/helm/#custom-configuration","title":"Custom Configuration","text":"<p>You can override configuration values using command line parameters:</p>"},{"location":"workloads/dev-tracking-mlflow/helm/#database-backend-configuration","title":"Database Backend Configuration","text":"<pre><code>helm template mlflow-server . \\\n  --set backendStore.type=\"postgres\" \\\n  --set backendStore.host=\"postgres.example.com\" \\\n  --set backendStore.port=\"5432\" \\\n  --set backendStore.database=\"mlflow\" \\\n  --set backendStore.secret.name=\"my-db-credentials\" | kubectl apply -f -\n</code></pre>"},{"location":"workloads/dev-tracking-mlflow/helm/#using-local-storage-override","title":"Using Local Storage Override","text":"<pre><code>helm template mlflow-server . \\\n  -f overrides/backends/local_artifacts.yaml | kubectl apply -f -\n</code></pre>"},{"location":"workloads/dev-tracking-mlflow/helm/#custom-local-storage-path","title":"Custom Local Storage Path","text":"<pre><code>helm template mlflow-server . \\\n  --set env_vars.MLFLOW_ARTIFACTS_DESTINATION=\"/workload/custom/artifacts\" \\\n  --set storage.ephemeral.quantity=\"1Ti\" | kubectl apply -f -\n</code></pre>"},{"location":"workloads/dev-tracking-mlflow/helm/#accessing-the-mlflow-web-ui","title":"Accessing the MLflow Web UI","text":""},{"location":"workloads/dev-tracking-mlflow/helm/#local-access-via-port-forwarding","title":"Local Access via Port Forwarding","text":"<p>To access the MLflow UI from your local machine:</p> <ol> <li>Forward the service port to your local machine:</li> </ol> <pre><code>kubectl port-forward services/mlflow-server 8080:80\n</code></pre> <ol> <li> <p>Open the MLflow UI in your browser:</p> </li> <li> <p>When using HTTPRoute or Ingress, the URL path prefix (<code>&lt;project_id&gt;/[&lt;user_id&gt;/]&lt;workload_id&gt;/</code>) is automatically handled by the routing layer. The <code>user_id</code> segment is omitted from the path when it's not specified, i.e. a project-wise deployment.</p> </li> <li>For direct access via port-forward, no path prefix is needed since you're connecting directly to the MLflow service</li> </ol>"},{"location":"workloads/dev-tracking-mlflow/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment status:</p> <pre><code>kubectl get deployment\nkubectl get service\n</code></pre>"},{"location":"workloads/dev-tracking-mlflow/helm/#view-logs","title":"View Logs","text":"<p>To view the MLflow server logs:</p> <pre><code>kubectl logs -f deployment/mlflow-server\n</code></pre>"},{"location":"workloads/dev-tracking-mlflow/helm/#storage-configuration","title":"Storage Configuration","text":""},{"location":"workloads/dev-tracking-mlflow/helm/#sqlite-backend-default","title":"SQLite Backend (Default)","text":"<p>By default, MLflow uses SQLite for the backend store, with the database file stored in ephemeral storage.</p>"},{"location":"workloads/dev-tracking-mlflow/helm/#postgresqlmysql-backend","title":"PostgreSQL/MySQL Backend","text":"<p>For production deployments, configure a PostgreSQL or MySQL backend:</p> <pre><code>backendStore:\n  type: postgres  # or mysql\n  host: \"your-db-host\"\n  port: \"5432\"\n  database: \"mlflow\"\n  driver: \"\"  # required for mysql, e.g., \"pymysql\"\n  secret:\n    name: \"mlflow-db-credentials\"\n    userKey: \"username\"\n    passwordKey: \"password\"\n</code></pre> <p>Create the required secret for database credentials:</p> <pre><code>kubectl create secret generic mlflow-db-credentials \\\n  --from-literal=username=myuser \\\n  --from-literal=password=mypassword\n</code></pre>"},{"location":"workloads/dev-tracking-mlflow/helm/#artifact-storage","title":"Artifact Storage","text":"<p>Artifacts are stored in S3-compatible storage (MinIO) by default. The configuration supports:</p> <ul> <li>Local filesystem storage</li> <li>S3-compatible object storage (MinIO, AWS S3, etc.)</li> </ul>"},{"location":"workloads/dev-tracking-mlflow/helm/#persistent-storage","title":"Persistent Storage","text":"<p>The chart supports optional persistent storage volumes:</p> <pre><code>persistent_storage:\n  enabled: true\n  volumes:\n    pvc-user:\n      pvc_name: \"pvc-user-{{ .Values.metadata.user_id }}\"\n      mount_path: \"/workload/{{ .Values.metadata.user_id }}\"\n</code></pre> <p>When enabled, this creates persistent volumes that can be shared across workload restarts and used for storing user data or models.</p>"},{"location":"workloads/dev-tracking-mlflow/helm/#health-checks","title":"Health Checks","text":"<p>The deployment includes comprehensive health checks:</p> <ul> <li>Startup Probe: Checks if the container has successfully started. It disables liveness and readiness probes until it succeeds, useful for slow-starting applications.</li> <li>Liveness Probe: Checks if the container is still alive. If it fails, Kubernetes restarts the container to recover from failure.</li> <li>Readiness Probe: Checks if the container is ready to serve traffic. If it fails, the container is removed from the service's endpoints but remains running.</li> </ul> <p>All probes use the <code>/health</code> endpoint on the HTTP port. The startup probe has a higher failure threshold (20) to accommodate longer startup times.</p>"},{"location":"workloads/dev-tracking-mlflow/helm/#kaiwo-integration","title":"Kaiwo Integration","text":"<p>The chart supports integration with Kaiwo for advanced workload management:</p> <pre><code>kaiwo:\n  enabled: true\n</code></pre> <p>When enabled, this uses Kaiwo CRDs to have the Kaiwo operator manage the workload lifecycle.</p>"},{"location":"workloads/dev-tracking-mlflow/helm/#using-mlflow-in-your-ml-projects","title":"Using MLflow in Your ML Projects","text":"<p>Once deployed, you can use this MLflow tracking server in your machine learning experiments:</p>"},{"location":"workloads/dev-tracking-mlflow/helm/#basic-usage","title":"Basic Usage","text":"<pre><code>import mlflow\n\n# Set the tracking server URI, assuming the release name as \"mlflow-server-service\"\nmlflow.set_tracking_uri(\"http://mlflow-server-service/\")\n\n# Start an experiment\nmlflow.set_experiment(\"my-experiment\")\n\n# Log parameters, metrics, and artifacts\nwith mlflow.start_run():\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_metric(\"accuracy\", 0.95)\n    mlflow.log_artifact(\"model.pkl\")\n</code></pre>"},{"location":"workloads/dev-tracking-mlflow/helm/#accessing-via-url","title":"Accessing via URL","text":"<p>To access the workload through a URL, you can enable either an Ingress or HTTPRoute in the <code>values.yaml</code> file by setting <code>ingress.enabled: true</code> or <code>http_route.enabled: true</code>.</p>"},{"location":"workloads/dev-tracking-mlflow/helm/#access-urls","title":"Access URLs","text":"<p>The MLflow tracking server can be accessed via different methods depending on your deployment (assuming the release name as \"mlflow-server-service\"):</p> <ul> <li>Port Forward: <code>http://localhost:8080</code> (after running <code>kubectl port-forward services/mlflow-server-service 8080:80</code>)</li> <li>Ingress/HTTPRoute: <code>https://your-domain.com/&lt;project_id&gt;/[&lt;user_id&gt;/]&lt;workload_id&gt;/</code> (when ingress is enabled)</li> <li>Internal Cluster Access:</li> <li>Within the same namespace: <code>http://mlflow-server-service</code></li> <li>From different namespaces: <code>http://mlflow-server-service.&lt;namespace&gt;.svc.cluster.local:80</code></li> <li>Used for service-to-service communication within the Kubernetes cluster</li> <li>Example usage in application code:     <pre><code>mlflow.set_tracking_uri(\"http://mlflow-server-service\")\n</code></pre></li> </ul>"},{"location":"workloads/dev-tracking-mlflow/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/dev-workspace-jupyterlab/helm/","title":"JupyterLab Workload","text":"<p>This workload deploys a basic JupyterLab environment on top of any image with Python (pip) pre-installed. It is ideal for interactive development sessions and experimentation with other workloads.</p>"},{"location":"workloads/dev-workspace-jupyterlab/helm/#configuration-parameters","title":"Configuration Parameters","text":"<p>You can configure the following parameters in the <code>values.yaml</code> file or override them via the command line:</p> Parameter Description Default <code>image</code> Container image repository and tag <code>rocm/pytorch:rocm6.4_ubuntu24.04_py3.12_pytorch_release_2.6.0</code> <code>imagePullPolicy</code> Image pull policy <code>Always</code> <code>gpus</code> Number of GPUs to allocate (set to 0 for CPU-only mode) <code>1</code> <code>memory_per_gpu</code> Memory allocated per GPU (in Gi) <code>128</code> <code>cpu_per_gpu</code> CPU cores allocated per GPU <code>4</code> <code>storage.ephemeral</code> Ephemeral storage configuration <code>128Gi</code>, <code>mlstorage</code>, <code>ReadWriteOnce</code> <code>deployment.ports.http</code> HTTP port exposed by the service <code>8080</code> <code>entrypoint</code> Custom entrypoint script See <code>values.yaml</code> for details <p>For more details see the <code>values.yaml</code> file.</p>"},{"location":"workloads/dev-workspace-jupyterlab/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>To deploy the chart with the release name <code>example</code>, run the following command from the <code>helm/</code> directory:</p> <pre><code>helm template example . | kubectl apply -f -\n</code></pre> <p>Note: If you set the <code>gpus</code> value greater than 0, ensure you specify a GPU-capable image to utilize the allocated resources properly.</p>"},{"location":"workloads/dev-workspace-jupyterlab/helm/#accessing-the-workload-locally","title":"Accessing the Workload Locally","text":"<p>To access JupyterLab locally, forward the service port to your machine:</p> <pre><code>kubectl port-forward services/dev-workspace-jupyterlab-example 8080:80\n</code></pre> <p>The complete url can be found by checking the container logs. You should see a message like: [.... ServerApp] Jupyter Server 2.16.0 is running at: [.... ServerApp] http://dev-workspace-jupyterlab-... [.... ServerApp]     http://127.0.0.1:8080/silogen/user/dev-workspace-jupyterlab-.../lab</p> <p>Open your browser and navigate to the second url, <code>http://localhost:8080/silogen...</code>.</p>"},{"location":"workloads/dev-workspace-jupyterlab/helm/#accessing-the-workload-via-url","title":"Accessing the Workload via URL","text":"<p>To access the workload through a URL, you can enable either an Ingress or HTTPRoute in the <code>values.yaml</code> file. The following parameters are available:</p> Parameter Description Default <code>ingress.enabled</code> Enable Ingress resource <code>false</code> <code>httproute.enabled</code> Enable HTTPRoute resource <code>false</code> <p>See the corresponding template files in the <code>templates/</code> directory. For more details on configuring Ingress or HTTPRoute, refer to the Ingress documentation and HTTPRoute documentation, or documentation of the particular gateway implementation you may use, like KGateway. Check with your cluster administrator for the correct configuration for your environment.</p>"},{"location":"workloads/dev-workspace-jupyterlab/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/dev-workspace-vscode/helm/","title":"Visual Studio Code Workload","text":"<p>This workload deploys a basic Visual Studio Code environment on top of any image with Python (pip) pre-installed. It is ideal for interactive development sessions and experimentation with other workloads.</p>"},{"location":"workloads/dev-workspace-vscode/helm/#persistent-settings","title":"Persistent Settings","text":"<p>By default, this workload is configured to persist VSCode settings, extensions, and workspace configurations across pod restarts using a Persistent Volume Claim (PVC). This means:</p> <ul> <li>Settings: Your VSCode settings, themes, and preferences are preserved</li> <li>Extensions: Installed extensions persist and don't need to be reinstalled</li> <li>Workspace: Your workspace configurations and project settings are maintained</li> <li>User Data: All user customizations and data are stored persistently</li> </ul> <p>The persistent configuration is stored in <code>/workload/{user_id}/.vscode-server/</code> on the PVC.</p>"},{"location":"workloads/dev-workspace-vscode/helm/#disabling-persistence","title":"Disabling Persistence","text":"<p>If you prefer ephemeral storage (settings reset on pod restart), you can disable persistence by setting:</p> <pre><code>persistent_storage:\n  enabled: false\n</code></pre>"},{"location":"workloads/dev-workspace-vscode/helm/#configuration-parameters","title":"Configuration Parameters","text":"<p>You can configure the following parameters in the <code>values.yaml</code> file or override them via the command line:</p> Parameter Description Default <code>image</code> Container image repository and tag <code>rocm/pytorch:rocm6.4_ubuntu24.04_py3.12_pytorch_release_2.6.0</code> <code>imagePullPolicy</code> Image pull policy <code>Always</code> <code>gpus</code> Number of GPUs to allocate (set to 0 for CPU-only mode) <code>1</code> <code>memory_per_gpu</code> Memory allocated per GPU (in Gi) <code>128</code> <code>cpu_per_gpu</code> CPU cores allocated per GPU <code>4</code> <code>storage.ephemeral</code> Ephemeral storage configuration <code>128Gi</code>, <code>mlstorage</code>, <code>ReadWriteOnce</code> <code>persistent_storage.enabled</code> Enable persistent storage for VSCode settings and extensions <code>false</code> <code>deployment.ports.http</code> HTTP port exposed by the service <code>8080</code> <code>entrypoint</code> Custom entrypoint script See <code>values.yaml</code> for details <p>For more details see the <code>values.yaml</code> file.</p>"},{"location":"workloads/dev-workspace-vscode/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>To deploy the chart with the release name <code>example</code>, run the following command from the <code>helm/</code> directory:</p> <pre><code>helm template example . | kubectl apply -f -\n</code></pre> <p>Note: If you set the <code>gpus</code> value greater than 0, ensure you specify a GPU-capable image to utilize the allocated resources properly.</p>"},{"location":"workloads/dev-workspace-vscode/helm/#accessing-the-workload-locally","title":"Accessing the Workload Locally","text":"<p>To access Visual Studio Code locally, forward the service port to your machine:</p> <pre><code>kubectl port-forward services/dev-workspace-vscode-example 8080:80\n</code></pre> <p>Then, open your browser and navigate to <code>http://localhost:8080</code>.</p>"},{"location":"workloads/dev-workspace-vscode/helm/#accessing-the-workload-via-url","title":"Accessing the Workload via URL","text":"<p>To access the workload through a URL, you can enable either an Ingress or HTTPRoute in the <code>values.yaml</code> file. The following parameters are available:</p> Parameter Description Default <code>ingress.enabled</code> Enable Ingress resource <code>false</code> <code>httproute.enabled</code> Enable HTTPRoute resource <code>false</code> <p>See the corresponding template files in the <code>templates/</code> directory. For more details on configuring Ingress or HTTPRoute, refer to the Ingress documentation and HTTPRoute documentation, or documentation of the particular gateway implementation you may use, like KGateway. Check with your cluster administrator for the correct configuration for your environment.</p>"},{"location":"workloads/dev-workspace-vscode/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/download-data-to-bucket/helm/","title":"workload helm template to download and preprocess data to bucket storage","text":"<p>This is an workload which downloads data, potentially preprocesses it, and uploads it to bucket storage. Since the <code>helm install</code> semantics are centered around on-going installs, not jobs that run once, it's best to just run <code>helm template</code> and pipe the result to <code>kubectl create</code> (<code>create</code> maybe more appropriate than apply for this Job as we don't expect to modify existing entities). Example: <pre><code>helm template workloads/download-data-to-bucket/helm \\\n    -f workloads/download-data-to-bucket/overrides/argilla-mistral-large-human-prompts.yaml \\\n    --name-template download-argilla \\\n    | kubectl create -f -\n</code></pre></p>"},{"location":"workloads/download-data-to-bucket/helm/#user-input-values","title":"User input values","text":"<p>See the <code>values.yaml</code> file for all user input values that you can provide, with instructions. In values.yaml, the <code>dataScript</code> is a script instead of just a dataset identifier, because the datasets on HuggingFace hub don't have a standard format that can be always directly passed to any training framework. The data script should format the data into the format that the training framework expects.</p> <p>Any data files output of the data script should be saved to the <code>/downloads/datasets/</code>. The files are uploaded to the directory pointed to by <code>bucketDataDir</code>, with the same filename as they had under <code>/downloads/datasets</code>.</p>"},{"location":"workloads/download-data-to-bucket/helm/#silogen-finetuning-engine-format","title":"Silogen finetuning engine format","text":"<p>For the silogen finetuning engine, the data format is JSON lines. For supervised finetuning, each line has a JSON dictionary formatted as follows: <pre><code>{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"This is a user message\"},\n    {\"role\": \"assistant\", \"content\": \"The is an assistant answer\"}\n  ]\n}\n</code></pre> There can be an arbitrary number of messages. Additionally, each dictionary can contain a <code>dataset</code> field that has the dataset identifier, and an <code>id</code> field that identifies the data point uniquely. For Direct Preference Optimization, the data format is as follows: <pre><code>{\n  \"prompt_messages\": [\n    {\"role\": \"user\", \"content\": \"This is a user message\"},\n  ],\n  \"chosen_messages\": [\n    {\"role\": \"assistant\", \"content\": \"This is a preferred answer\"}\n  ],\n  \"rejected_messages\": [\n    {\"role\": \"assistant\", \"content\": \"This is a rejected answer\"}\n  ]\n}\n</code></pre></p> <p>The HuggingFace datasets library can save the output in JSON lines format with the <code>to_json</code> function: <pre><code>dataset.to_json(\"/downloads/datasets/&lt;name of your dataset file.jsonl&gt;\")\n</code></pre></p>"},{"location":"workloads/download-huggingface-model-to-bucket/helm/","title":"workload helm template to download a model to bucket storage","text":"<p>This is an workload which downloads a model and uploads it to bucket storage. Since the <code>helm install</code> semantics are centered around on-going installs, not jobs that run once, it's best to just run <code>helm template</code> and pipe the result to <code>kubectl create</code> (<code>create</code> maybe more appropriate than apply for this Job as we don't expect to modify existing entities). Example: <pre><code>helm template workloads/download-huggingface-model-to-bucket/helm \\\n    -f workloads/download-huggingface-model-to-bucket/helm/overrides/llama-3.1-tiny-random-to-google.yaml \\\n    --name-template download-huggingface \\\n    | kubectl create -f -\n</code></pre></p>"},{"location":"workloads/download-huggingface-model-to-bucket/helm/#user-input-values","title":"User input values","text":"<p>See the <code>values.yaml</code> file for the user input values that you can provide, with instructions.</p>"},{"location":"workloads/download-wandb-model-to-bucket/helm/","title":"Download a model from Weights and Biases to bucket storage","text":"<p>This is an workload which downloads a model from weights and biases and uploads it to bucket storage.</p> <p>Run example: <pre><code>helm template \"dl-from-wandb\" workloads/download-wandb-model-to-bucket/helm \\\n    -f workloads/download-wandb-model-to-bucket/helm/overrides/example-model-to-minio.yaml \\\n    | kubectl create -f -\n</code></pre></p>"},{"location":"workloads/download-wandb-model-to-bucket/helm/#user-input-values","title":"User input values","text":"<p>See the <code>values.yaml</code> file for the user input values that you can provide, with instructions.</p>"},{"location":"workloads/k8s-namespace-setup/helm/","title":"k8s-namespace-setup","text":"<p>A Helm chart for setting up Kubernetes namespaces. This chart allows you to configure the following components in a namespace:</p> <ul> <li>External Secrets: Pull in secrets from within or outside the cluster, such as bucket storage credentials or Hugging Face tokens.</li> <li>Kueue: Set up a local queue for Kueue in the namespace to submit jobs.</li> <li>Role / Role Binding: Configure permissions for service accounts, like the default service account, to access the Kubernetes API from within a container.</li> </ul> <p>If you are not sure if you need any of these, then this workload is probably not needed for you.</p>"},{"location":"workloads/k8s-namespace-setup/helm/#installation","title":"Installation","text":"<p>To apply a configuration to the active namespace, use:</p> <pre><code>helm template . -f overrides/rename-secret-names.yaml | kubectl apply -f -\n</code></pre> <p>To specify a different namespace:</p> <pre><code>helm template . -f overrides/rename-secret-names.yaml | kubectl apply -n &lt;namespace&gt; -f -\n</code></pre> <p>Control which components to set up using command line parameters:</p> <pre><code>helm template . --set kueue.setup=true --set role.setup=true | kubectl apply -f -\n</code></pre>"},{"location":"workloads/k8s-namespace-setup/helm/#configuration","title":"Configuration","text":"<p>The following table lists the configurable parameters of the <code>k8s-namespace-setup</code> chart and their default values.</p>"},{"location":"workloads/k8s-namespace-setup/helm/#external-secret","title":"External Secret","text":"Parameter Description Default <code>external_secret.setup</code> Enable external secret <code>false</code> <code>external_secret.external_secret_name</code> External secret name <code>minio-credentials-fetcher</code> <code>external_secret.src.secret_store_name</code> Secret store name <code>k8s-secret-store</code> <code>external_secret.src.remote_secret_name</code> Remote secret name <code>default-user</code> <code>external_secret.src.access_key_name</code> Access key name <code>API_ACCESS_KEY</code> <code>external_secret.src.secret_key_name</code> Secret key name <code>API_SECRET_KEY</code> <code>external_secret.dest.k8s_secret_name</code> Kubernetes secret name <code>minio-credentials</code> <code>external_secret.dest.access_key_name</code> Kubernetes access key name <code>minio-access-key</code> <code>external_secret.dest.secret_key_name</code> Kubernetes secret key name <code>minio-secret-key</code>"},{"location":"workloads/k8s-namespace-setup/helm/#kueue","title":"Kueue","text":"Parameter Description Default <code>kueue.setup</code> Enable kueue <code>false</code> <code>kueue.cluster_queue_name</code> Cluster queue name <code>kaiwo</code>"},{"location":"workloads/k8s-namespace-setup/helm/#roles","title":"Roles","text":"Parameter Description Default <code>role.setup</code> Enable roles setup <code>false</code> <code>role.name</code> Role name <code>default-role</code> <code>role.bindingName</code> Role binding name <code>default-role-binding</code> <code>role.rules</code> Role rules See <code>values.yaml</code>"},{"location":"workloads/llm-deepspeed-estimate-ram-vram/helm/","title":"Deepspeed RAM and VRAM estimate","text":"<p>Deepspeed can require a large amount of RAM, and save on the VRAM, depending on the configuration. Deepspeed includes a memory estimation utility, and this chart exposes that utility in a convenient package.</p>"},{"location":"workloads/llm-deepspeed-estimate-ram-vram/helm/#specifying-the-inputs","title":"Specifying the inputs","text":"<p>You can get estimates for a specifc model on bucket storage or on HuggingFace Hub. In that case, the model config is downloaded to the container and a model loaded onto a meta device, meaning that no parameters are actually constructed. The number of parameters is computed based on the loaded model, but note that any shared parameters will be counted multiple times, since the meta device cannot distinguish between shared layers and non-shared ones. That may slightly inflate the estimates. Specify the model in the <code>modelPath</code> field. If the model is on an S3-bucket, prefix the bucket path with <code>s3://</code>.</p> <p>You can also simply specify the number of parameters. Then, the estimates are based on your given number. Note that if you give both <code>modelPath</code> and <code>numParameters</code>, then <code>modelPath</code> is used. If you're using stage 3, you must also specify <code>largestLayerParameters</code>.</p> <p>The estimates depend on the number of nodes and GPUs per node. These are set with <code>nodes</code> and <code>gpusPerNode</code> respectively. Additionally, a buffer factor is used. It's set by <code>bufferFactor</code>.</p> <p>Finally, the estimates depend on the Deepspeed stage. Stages 1 and 2 have the same estimate, and stage 3 has its own. Specify the Deepspeed stage with <code>stage</code>.</p> <p>See <code>values.yaml</code> for other inputs, which determine the image, the bucket storage secret naming, the hf-token secret naming, and whether to use a KaiwoJob or a regular Job.</p>"},{"location":"workloads/llm-deepspeed-estimate-ram-vram/helm/#getting-the-results","title":"Getting the results","text":"<p>The estimates are printed to the logs of the pod. You simply run the workload, e.g. <pre><code>helm template . --set modelPath=meta-llama/Llama-3.1-70B --values overrides/hf-token --name-template \"llama-70b-ds-estimate\" | kubectl apply -f -\n</code></pre> or <pre><code>helm template . --set modelPath=s3://default-bucket/models/meta-llama/Llama-3.1-70B --name-template \"llama-70b-ds-estimate\" | kubectl apply -f -\n</code></pre> or <pre><code>helm template . --set numParameters=70e9 --name-template \"generic-70b-ds-estimate\" | kubectl apply -f -\n</code></pre></p> <p>then let the workload finish and check the logs with <pre><code>kubectl logs llama-70b-ds-estimate-POD_UNIQUE_ID_PART_HERE\n</code></pre></p> <p>The logs end in something along the lines of: <code>Estimated memory needed for params, optim states and gradients for a: HW: Setup with 1 node, 8 GPUs per node. SW: Model with 69503M total params.   per CPU  |  per GPU |   Options  3107.03GB | 129.46GB | offload_optimizer=cpu  3107.03GB | 275.10GB | offload_optimizer=none</code></p>"},{"location":"workloads/llm-evaluation-judge/helm/","title":"LLM-as-a-Judge Workloads","text":"<p>This helm chart implements evaluation of LLMs using LLM-as-a-Judge --- having an LLM provide inferences over an evaluation dataset, and letting another LLM judge the quality of the outputs.</p> <p>The necessary Kubernetes and Helm files are stored here in <code>/workloads/llm-evaluation-judge/helm</code>, while the evaluation package source code and docker image build files are stored in <code>/docker/llm-evaluation</code>.</p>"},{"location":"workloads/llm-evaluation-judge/helm/#helm-and-kubernetes-files","title":"Helm and Kubernetes files","text":"<p>The Helm templates are stored in <code>/workloads/llm-evaluation-judge/helm/templates</code>, the main template workload template being <code>evaluation_judge_template.yaml</code>. Default values can be found in <code>values.yaml</code>, with user-defined configurations stored in <code>/overrides</code>. We have included a few example override files for typical use cases.</p> <p>A few extra resources are defined in <code>templates/</code>. We use a <code>ConfigMap</code> (<code>templates/configmap.yaml</code>) to mount files directly to the cluster when running the workload. Anything stored in the <code>mount/</code> directory will be mounted.</p>"},{"location":"workloads/llm-evaluation-judge/helm/#docker-container","title":"Docker Container","text":"<p>We define an associated evaluation package in <code>/docker/llm-evaluation</code>. This contains code to call the inference container over the evaluation dataset, and subsequently judge the outputs using a second judge container, writing results to MinIO storage.</p> <p>This package is installed into a docker image, which can be used to run the evaluation container in the helm template. We use a Makefile to push new images to our GitHub registry. (<code>&gt; make push</code>)</p>"},{"location":"workloads/llm-evaluation-judge/helm/#running","title":"Running","text":"<p>To run this evaluation workload with helm, use the template command and pipe it to kubectl apply:</p> <pre><code>cd workloads/llm-evaluation-judge\n</code></pre> <pre><code>helm template helm -f overrides/prometheus-llama_3_8b-cnn_dailymail.yaml | kubectl apply -f - -n &lt;your-namespace&gt;\n</code></pre>"},{"location":"workloads/llm-evaluation-metrics/helm/","title":"Metrics Evaluation Workloads","text":"<p>This helm chart implements evaluation of LLMs using the BERTscore metric, comparing generated answers to a gold standard.</p> <p>The necessary Kubernetes and Helm files are stored here in <code>/workloads/llm-evaluation-judge/helm</code>, while the evaluation package source code and docker image build files are stored in <code>/docker/llm-evaluation</code>.</p>"},{"location":"workloads/llm-evaluation-metrics/helm/#helm-and-kubernetes-files","title":"Helm and Kubernetes files","text":"<p>The Helm templates are stored in <code>/workloads/llm-evaluation-metrics/helm/templates</code>, the main template workload template being <code>metrics_evaluation_template_with_download.yaml</code>. Default parameters can be found in <code>values.yaml</code>, with user-defined configurations stored in <code>/overrides</code>. We have included a few example override files for typical use cases.</p> <p>A few extra resources are defined in <code>templates/</code>. We use a <code>ConfigMap</code> (<code>templates/configmap.yaml</code>) to mount files directly to the cluster when running the workload. Anything stored in the <code>mount/</code> directory will be mounted.</p>"},{"location":"workloads/llm-evaluation-metrics/helm/#docker-container","title":"Docker Container","text":"<p>We define an associated evaluation package in <code>/docker/llm-evaluation</code>. This contains code to call the inference container, and subsequently run the BERTscore metric evaluation, writing results to MinIO storage.</p> <p>This package is installed into a docker image, which can be used to run the evaluation container in the helm template. We use a Makefile to push new images to our GitHub registry. See the <code>/docker/llm-evaluation/README.md</code> for more details.</p>"},{"location":"workloads/llm-evaluation-metrics/helm/#running","title":"Running","text":"<p>To run this evaluation workload with helm, use the template command and pipe it to kubectl apply:</p> <pre><code>cd workloads/llm-evaluation-metrics\n</code></pre> <pre><code>helm template helm -f overrides/bertscore_llama-3.1-8B_cnn-dailymail_values.yaml | kubectl apply -f - -n &lt;your-namespace&gt;\n</code></pre>"},{"location":"workloads/llm-finetune-axolotl/helm/","title":"Finetuning with Axolotl","text":"<p>This is a Helm Chart for running an Axolotl finetuning job.</p> <p>Currently the base model and input data are assumed to be from HuggingFace, or some other source directly supported by Axolotl. The output is saved with MinIO in the directory specified by <code>checkpointsRemote</code>. If any checkpoints already exist in the directory, the training can be resumed from there (by setting <code>auto_resume_from_checkpoints</code>)</p> <p>The provided example task is based on the Llama-3.2-1B LoRA config from the Axolotl repo. The only changes are:</p> <ul> <li>set the optimizer to <code>adamw_torch</code> in order to avoid using bitsandbytes</li> <li>turn on <code>auto_resume_from_checkpoints</code></li> <li>not specifying an output directory (the chart takes care of uploading the checkpoints)</li> </ul>"},{"location":"workloads/llm-finetune-axolotl/helm/#limitations","title":"Limitations","text":"<ul> <li>bitsandbytes does not work. It is built and installed so that axolotl launch works, but currently has some unsolved issues. Avoid functionality that actually uses bitsandbytes.</li> <li>ray also does not work as expected, so multi-node training is not supported.</li> </ul>"},{"location":"workloads/llm-finetune-axolotl/helm/#configuration","title":"Configuration","text":"<p>Create an Axolotl config file in the <code>mount/</code> directory, and set <code>configFile</code> in the overrides to point to your config file. See the Axolotl docs for how to specify a config file.</p>"},{"location":"workloads/llm-finetune-axolotl/helm/#running-the-workload","title":"Running the workload","text":"<p>Then the simplest is to run <code>helm template</code> and pipe the result to <code>kubectl create</code>.</p> <p>Example command:</p> <pre><code>helm template workloads/llm-finetune-axolotl/helm \\\n  --values workloads/llm-finetune-axolotl/helm/overrides/finetune-lora.yaml \\\n  --name-template finetune-lora-axolotl \\\n  | kubectl create -f -\n</code></pre>"},{"location":"workloads/llm-finetune-llama-factory/helm/","title":"Finetuning with LLaMA-Factory","text":"<p>This is a Helm Chart for running a finetuning job using LLaMA-Factory</p> <p>The output is saved with MinIO in the directory specified by <code>checkpointsRemote</code>.</p>"},{"location":"workloads/llm-finetune-llama-factory/helm/#configuration","title":"Configuration","text":"<p>Include any parameters for LLaMA-Factory in the <code>llamaFactoryConfig</code> parameter. See the override file <code>overrides/finetune-lora.yaml</code> for an example and the LLaMA-Factory documentation for more details.</p>"},{"location":"workloads/llm-finetune-llama-factory/helm/#running-the-workload","title":"Running the workload","text":"<p>The simplest is to run <code>helm template</code> and pipe the result to <code>kubectl create</code>.</p> <p>Example command using the example override file <code>overrides/finetune-lora.yaml</code>:</p> <pre><code>helm template workloads/llm-finetune-llama-factory/helm \\\n  --values workloads/llm-finetune-llama-factory/helm/overrides/finetune-lora.yaml \\\n  --name-template finetune-lora-llama-factory \\\n  | kubectl create -f -\n</code></pre>"},{"location":"workloads/llm-finetune-llama-factory/helm/#data-specification","title":"Data specification","text":"<p>Specify the name of data set used for training as <code>dataset</code>. This can include datasets predefined in LLaMA-Factory or those defined in <code>datasetInfo</code>. Use commas to separate multiple data sets.</p> <p>To use other datasets, create an entry in <code>datasetInfo</code> following the LLaMA-Factory dataset info format. Note that LLaMA-Factory directly supports loading datasets from HuggingFace, ModelScope, or s3/gcs cloud storage by setting the urls according to the documentation.</p> <p>This workload adds a custom way to load data from MinIO. In <code>datasetInfo</code> specify the path to the dataset in the remote bucket as <code>pathRemote</code>, and the workload will load the file and update the configuration. See the override file <code>overrides/finetune-model_data_from_minio.yaml</code> for an example of finetuning where the data and model are loaded from MinIO.</p>"},{"location":"workloads/llm-finetune-llama-factory/helm/#model-specification","title":"Model specification","text":"<p>To use a base model from HuggingFace or other source directly supported by LLaMA-Factory, specify the model name in <code>modelName</code>.</p> <p>Alternatively to use a model from MinIO, specify the path to the model in <code>modelRemote</code>.</p> <p>Either <code>modelName</code> or <code>modelRemote</code> must be specified. If both are included, the model from <code>modelRemote</code> is used.</p>"},{"location":"workloads/llm-finetune-llama-factory/helm/#cleanup","title":"Cleanup","text":"<p>After the jobs are completed, please delete the resources created. In particular for multi-node ray jobs, a <code>PersistentVolumeClaim</code> is used as shared storage and persists on the cluster after the job is completed.</p> <p>To delete the resources, you can run the same <code>helm template</code> command, only replacing <code>kubectl create</code> with <code>kubectl delete</code>, e.g.:</p> <pre><code>helm template workloads/llm-finetune-llama-factory/helm \\\n  --values workloads/llm-finetune-llama-factory/helm/overrides/finetune-lora.yaml \\\n  --name-template finetune-lora-llama-factory \\\n  | kubectl delete -f -\n</code></pre>"},{"location":"workloads/llm-finetune-llama-factory/helm/#multi-node-finetuning-with-ray","title":"Multi-node finetuning with ray","text":"<p>The chart supports multi-node jobs by setting <code>nodes</code> to an integer greater than 1. Doing so enables ray and creates a RayJob instead. An example config is provided in <code>overrides/finetune-lora-ray.yaml</code>. The example also shows how to use DeepSpeed ZeRO Stage 2 to partition the gradients. To enable DeepSpeed, set the <code>deepspeed</code> parameter in the LLaMA-Factory config to point to one of the deepspeed configs included in LLaMA-Factory or a dictionary.</p> <p>When configuring ray jobs, the resources you are requesting (<code>nodes</code> and <code>gpusPerNode</code>) are automatically specified for LLaMA-Factory, and do not need to be included separately in the <code>llamaFactoryConfig</code>.</p>"},{"location":"workloads/llm-finetune-llama-factory/helm/#limitations","title":"Limitations","text":"<p><code>unsloth</code> and <code>bitsandbytes</code> are not installed in the currently used image, so any functionality using those libraries will not work.</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/","title":"Finetuning with the SiloGen finetuning engine","text":"<p>This is a Helm Chart for finetuning Jobs based on the SiloGen finetuning engine. The chart integrates the finetuning config as part of the <code>values.yaml</code> input.</p> <p>See the <code>values.yaml</code> file for the general structure (more documentation coming soon).</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/#running-the-workload","title":"Running the workload","text":"<p>Since the <code>helm install</code> semantics are centered around on-going installs, not jobs that run once, it's best to just run <code>helm template</code> and pipe the result to <code>kubectl create</code>.</p> <p>Example command: <pre><code>helm template . \\\n  -f overrides/llama-31-tiny-random-deepspeed-values.yaml \\\n  --name-template llama-31-tiny-random-deepspeed-alpha \\\n  | kubectl create -f -\n</code></pre></p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/#multiple-overlays-simplified-interface","title":"Multiple overlays, simplified interface","text":"<p>This chart supports two ways of specifying certain inputs, one on the top level and one as part of the finetuning_config: - Training data can be provided as <code>trainingData</code> or <code>finetuning_config.data_conf.training_data.datasets</code> - The total batch size target can be provided as <code>batchSize</code> or <code>finetuning_config.batchsize_conf.total_train_batch_size</code> - The number of epochs to run for can be provided as <code>numberOfEpochs</code> or <code>finetuning_config.training_args.num_train_epochs</code></p> <p>The top level inputs provide a simpler interface to run finetuning. However, they're not enough alone to fully specify a sensible training setup. The expectation is that these top-level inputs are used in conjuction with a set of override files that specify most arguments. This is the expected way that the chart is used in conjuction with the so called Silogen developer console. An example of such use is: <pre><code>helm template . \\\n  -f overrides/models/meta-llama_llama-3.1-8b.yaml \\\n  -f overrides/dev-console/default.yaml \\\n  --name-template llama-31-8b-argilla-alpha \\\n  | kubectl create -f -\n</code></pre></p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/#multiple-overlays-general-case","title":"Multiple overlays, general case","text":"<p>Multiple overlays can be useful for a CLI user as well. Here's an example that reproduces the output of tutorial-01-finetune-full-param.yaml: <pre><code>helm template . \\\n  -f overrides/models/tiny-llama_tinyllama-1.1b-chat-v1.0.yaml \\\n  -f overrides/additional-example-files/repro-tutorial-01-user-inputs.yaml \\\n  --name-template tiny-llama-argilla-alpha \\\n  | kubectl create -f -\n</code></pre> To check that the manifests match, we can run a diff and see that it is empty: <pre><code>diff \\\n  &lt;( \\\n    helm template . \\\n    -f overrides/models/tiny-llama_tinyllama-1.1b-chat-v1.0.yaml \\\n    -f overrides/additional-example-files/repro-tutorial-01-user-inputs.yaml \\\n    --name-template tiny-llama-argilla-alpha \\\n  ) \\\n  &lt;( \\\n    helm template . \\\n    -f overrides/tutorial-01-finetune-full-param.yaml \\\n    --name-template tiny-llama-argilla-alpha \\\n  )\n</code></pre></p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/#tensorboard","title":"Tensorboard","text":"<p>Specifying <code>runTensorboard: true</code> and <code>finetuning_config.trainings_args.report_to: [\"tensorboard\"]</code> logs the training progress to tensorboard and serves the Tensorboard web UI from the training container. The tensorboard logs are also uploaded to the bucket storage for later use.</p> <p>To connect to the Tensorboard web UI on the container, start a port-forward: <pre><code>kubectl port-forward --namespace YOUR_NAMESPACE pods/YOUR_POD_NAME 6006:6006\n</code></pre> Then browse to localhost:6006.</p> <p>Note that the logging frequency is set by the HuggingFace Transformers logging options.</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/#best-known-configuration-model-overrides","title":"Best-known-configuration model overrides","text":"<p>The directory <code>overrides/models</code> hosts finetuning recipes for various models. The files are named according to model canonical names, which is the huggingface pattern of <code>organization/model-name</code> just changed into <code>organization_model-name</code>. These configurations have been shown to work well in experiments, but that does not guarantee that these exact parameters are always optimal. The best parameters still depend on the data, too.</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/","title":"Finetuning config structure and parameters","text":"<p>This document describes the structure of the finetuning configuration, and the parameters and values that can be defined there.</p> <p>See the finetuning config section this config file for an example of a valid configuration. See the various sub-configs for their options. Additional properties are not allowed.</p> <p>Top-level properties:</p> Property Type Required Possible values Default Description data_conf <code>object</code> \u2705 ChatTrainValidConfig The data input config training_args <code>object</code> \u2705 SilogenTrainingArguments Transformer TrainingArguments with some restrictions batchsize_conf <code>object</code> \u2705 BatchsizeConfig Batch size configuration peft_conf <code>object</code> \u2705 GenericPeftConfig and/or NoPeftConfig and/or PretrainedPeftConfig Adapter configuration run_conf <code>object</code> \u2705 RunConfig Model related configuration sft_args <code>object</code> \u2705 SFTArguments SFT specific arguments method <code>const</code> <code>sft</code> <code>\"sft\"</code> overrides <code>object</code> Overrides <code>{\"lr_multiplier\": 1.0, \"lr_batch_size_scaling\": \"none\"}</code> Override options to simplify the config interface tracking <code>object</code> or <code>null</code> FinetuningTrackingConfig MLFlow tracking configuration quant_conf <code>object</code> BnBQuantizationConfig and/or NoQuantizationConfig <code>{\"quantization_type\": \"no-quantization\"}</code> Quantization configuration"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#definitions","title":"Definitions","text":""},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#autosplitdatainput","title":"AutoSplitDataInput","text":"<p>Automatic validation split from the training data</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description type <code>const</code> \u2705 <code>AUTO_SPLIT</code> data_type <code>string</code> string <code>\"ChatConversation\"</code> Generally, the data_type is automatically set based on the experiment config method. ratio <code>number</code> number <code>0.2</code> Ratio of the training data to use for validation seed <code>integer</code> integer <code>1289525893</code> Seed for the random number generator for splitting"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#batchsizeconfig","title":"BatchsizeConfig","text":"<p>Config for determining the total batch size</p> <p>Total batch size is the effective batch size for the complete training run. It is equal to number of processes * per-device batch size * accumulation.</p> <p>The maximum batch size per device is the maximum batch size that can be accommodated on a single device. This mostly limited by the memory capacity of the device.</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_1","title":"Type: <code>object</code>","text":"Property Type Required Possible values Description total_train_batch_size <code>integer</code> \u2705 integer The total batch size for the training run max_per_device_train_batch_size <code>integer</code> \u2705 integer The maximum training batch size per device per_device_eval_batch_size <code>integer</code> or <code>null</code> integer The maximum eval batch size per device, if not given, will use same as training batch size"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#bnbquantizationconfig","title":"BnBQuantizationConfig","text":"<p>Bits and Bytes configuration</p> <p>The options are from the BitsAndBytes config, see: https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_2","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description quantization_type <code>const</code> <code>bits-and-bytes</code> <code>\"bits-and-bytes\"</code> load_in_8bit <code>boolean</code> boolean <code>False</code> load_in_4bit <code>boolean</code> boolean <code>False</code> llm_int8_threshold <code>number</code> number <code>6.0</code> llm_int8_skip_modules <code>array</code> or <code>null</code> string llm_int8_enable_fp32_cpu_offload <code>boolean</code> boolean <code>False</code> llm_int8_has_fp16_weight <code>boolean</code> boolean <code>False</code> bnb_4bit_compute_dtype <code>string</code> or <code>null</code> string bnb_4bit_quant_type <code>const</code> <code>fp4</code> and/or <code>nf4</code> <code>\"fp4\"</code> bnb_4bit_use_double_quant <code>boolean</code> boolean <code>False</code> bnb_4bit_quant_storage <code>string</code> or <code>null</code> string"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#chattemplatename","title":"ChatTemplateName","text":"<p>Chat template to use.</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-string","title":"Type: <code>string</code>","text":"<p>Possible Values: <code>mistral-with-system</code> or <code>chat-ml</code> or <code>poro</code> or <code>keep-original</code> or <code>simplified-llama31</code></p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#chattrainvalidconfig","title":"ChatTrainValidConfig","text":"<p>Training time data configuration</p> <p>Always defines some DataInput for training data and can include validation DataInput, though a trivial NoneDataInput is also allowed for the validation side.</p> <p>Additionally includes chat template and padding configurations, as those are part of the data input pipeline.</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_3","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description training_data <code>object</code> \u2705 ConcatenationDataInput and/or WeightedMixDataInput validation_data <code>object</code> \u2705 AutoSplitDataInput and/or ConcatenationDataInput and/or NoneDataInput chat_template_name <code>string</code> ChatTemplateName <code>\"mistral-with-system\"</code> padding_side <code>string</code> string <code>\"right\"</code> Padding side, right is usually right. missing_pad_token_strategy <code>string</code> MissingPadTokenStrategy <code>\"bos-repurpose\"</code> See the MissingPadTokenStrategys for descriptions of the options"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#concatenationdatainput","title":"ConcatenationDataInput","text":"<p>A simple list of datasets</p> <p>These are simply concatenated, the same as sampling all with equal weight.</p> <p>The datasets themselves need to be in the finetuning supported JSONL formats. For SFT this means lines:</p> <pre><code>{\"messages\": {\"content\": \"string\", \"role\": \"string\"}}\n</code></pre> <p>For DPO this means lines of:</p> <pre><code>{\"prompt_messages\": {\"content\": \"string\", \"role\": \"string\"}, \"chosen_messages\": {\"content\": \"string\", \"role\": \"string\"}, \"rejected_messages\": {\"content\": \"string\", \"role\": \"string\"}}\n</code></pre>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_4","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description type <code>const</code> \u2705 <code>CONCATENATION</code> datasets <code>array</code> \u2705 DatasetDefinition data_type <code>string</code> string <code>\"ChatConversation\"</code> Generally, the data_type is automatically set based on the experiment config method."},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#datasetdefinition","title":"DatasetDefinition","text":"<p>Define how to load a dataset</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_5","title":"Type: <code>object</code>","text":"Property Type Required Possible values Description path <code>string</code> \u2705 string Local path to a JSONL file in the finetuning data format"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#finetuningtrackingconfig","title":"FinetuningTrackingConfig","text":"<p>Settings that define how run details are logged</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_6","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description mlflow_server_uri <code>string</code> \u2705 string MLflow server URI. Can be local path. experiment_name <code>string</code> \u2705 string Experiment name that is used for MLFlow tracking. run_id <code>string</code> or <code>null</code> string Run id, to resume logging to previously started run. run_name <code>string</code> or <code>null</code> string Run name, to give meaningful name to the run to be displayed in MLFlow UI. Used only when run_id is unspecified. hf_mlflow_log_artifacts <code>string</code> string <code>\"False\"</code> Whether to store model artifacts in MLFlow."},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#genericpeftconfig","title":"GenericPeftConfig","text":"<p>Config for any new initialized PEFT Adapter</p> <p>See https://huggingface.co/docs/peft/tutorial/peft_model_config for the possible kwargs and https://github.com/huggingface/peft/blob/v0.7.1/src/peft/utils/peft_types.py for the types.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; loaded_data = {'peft_type':'LORA', 'task_type': 'CAUSAL_LM',\n...         'peft_kwargs': {'r': 32, 'target_modules': ['v_proj']}}\n&gt;&gt;&gt; generic_conf = GenericPeftConfig(**loaded_data)\n&gt;&gt;&gt; # Then later in the code something like:\n&gt;&gt;&gt; model = transformers.AutoModel.from_pretrained('hf-internal-testing/tiny-random-MistralModel')\n&gt;&gt;&gt; peft.get_peft_model(model, generic_conf.get_peft_config())\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    ...\n  )\n)\n</code></pre>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_7","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description peft_type <code>string</code> \u2705 PeftType task_type <code>string</code> TaskType <code>\"CAUSAL_LM\"</code> peft_kwargs <code>object</code> object"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#missingpadtokenstrategy","title":"MissingPadTokenStrategy","text":"<p>Specifies the available missing pad token strategies.</p> <p>We've shown in a small set of experiments that repurposing EOS can start to hurt performance while the other options seem to work equally well.</p> <p>Repurposing EOS is the default in many online sources, but it is actually a bad idea if we want to predict EOS, as all the pad_token_ids get ignored in loss computation, and thus the model does not learn to predict the end of the text. However, for models that have additional tokens for end of message, end of turn, etc. this is not so dangerous.</p> <p>Repurposing BOS is similar to repurposing EOS, but since we do not need to predict BOS, this may be more sensible.</p> <p>Repurposing UNK can work with tokenizers that never produce UNKs in normal data (e.g. Mistral tokenizers should have a byte fall-back so that everything can be tokenized).</p> <p>UNK_CONVERT_TO_EOS uses a hack where the unk_token_id is initially used for padding, but in the collation phase the input-side UNKs (padding) gets set to EOS, so that the input-side padding looks like EOS. On the output-side, the UNKs (padding) still gets ignored. NOTE: This will leave the tokenizer's pad_token_id set to the unk_token_id; so any subsequent use of the model where padding is involved should somehow explicitly set the pad_token_id again.</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-string_1","title":"Type: <code>string</code>","text":"<p>Possible Values: <code>eos-repurpose</code> or <code>bos-repurpose</code> or <code>unk-repurpose</code> or <code>unk-convert-to-eos</code></p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#modelarguments","title":"ModelArguments","text":"<p>These are passed to AutoModelForCausalLM.from_pretrained</p> <p>See parameter docstrings and help at: https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained See below in \"Parameters for big model inference\" too, it affects training too. Also note that this link takes you to the transformers main branch version - be sure to compare with the installed version of transformers (that keeps changing over time, and it is difficult to keep this docstring up to date, so we wanted to link to the latest here).</p> <p>Some important parameters to consider are:</p> <ul> <li>device_map :     A map that specifies where each submodule should go. It doesn\u2019t need to be refined to each parameter/buffer     name, once a given module name is inside, every submodule of it will be sent to the same device. If we only pass     the device (e.g., \"cpu\", \"cuda:1\", \"mps\", or a GPU ordinal rank like 1) on which the model will be allocated,     the device map will map the entire model to this device. Passing device_map = 0 means put the whole model on GPU     0.</li> <li>attn_implementation :     The attention implementation to use in the model (if relevant). Can be any of \"eager\" (manual implementation of     the attention), \"sdpa\" (using F.scaled_dot_product_attention), or \"flash_attention_2\" (using     Dao-AILab/flash-attention). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is     otherwise the manual \"eager\" implementation.</li> </ul> <p>NOTE:     This does not include quantization_config. Quantization config is specified separately.</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_8","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description torch_dtype <code>const</code> <code>auto</code> <code>\"auto\"</code> device_map <code>object</code> or <code>string</code> or <code>null</code> object and/or string Custom device map so that you can manually override the choices that HuggingFace would make. This can also be a string to specify \"auto\", \"balanced_low_0\", or \"sequential\". max_memory <code>object</code> or <code>null</code> object low_cpu_mem_usage <code>boolean</code> boolean <code>False</code> attn_implementation <code>string</code> or <code>null</code> string Note: this can be set to \"sdpa\", \"flash_attention_2\", \"eager\". offload_folder <code>string</code> or <code>null</code> string offload_state_dict <code>boolean</code> or <code>null</code> boolean Default is True if offloading (otherwise no effect) offload_buffers <code>boolean</code> or <code>null</code> boolean use_cache <code>boolean</code> boolean <code>true</code> Saves generated hidden states to speed up generation, see: https://discuss.huggingface.co/t/what-is-the-purpose-of-use-cache-in-decoder/958 This is mutually exclusive with gradient_checkpointing. cache_dir <code>string</code> or <code>null</code> string force_download <code>boolean</code> boolean <code>False</code> local_files_only <code>boolean</code> boolean <code>False</code> proxies <code>object</code> or <code>null</code> object resume_download <code>boolean</code> boolean <code>False</code> revision <code>string</code> string <code>\"main\"</code> code_revision <code>string</code> string <code>\"main\"</code> subfolder <code>string</code> or <code>null</code> string token <code>string</code> or <code>null</code> string use_safetensors <code>boolean</code> or <code>null</code> boolean variant <code>string</code> or <code>null</code> string trust_remote_code <code>boolean</code> boolean <code>False</code> Warning: if set to True, allows execution of downloaded remote code."},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#nopeftconfig","title":"NoPeftConfig","text":"<p>A trivial config specifying that no peft is used</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_9","title":"Type: <code>object</code>","text":"Property Type Required Possible values Description peft_type <code>const</code> \u2705 <code>NO_PEFT</code>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#noquantizationconfig","title":"NoQuantizationConfig","text":"<p>A marker not to use quantization</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_10","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description quantization_type <code>const</code> <code>no-quantization</code> <code>\"no-quantization\"</code>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#nonedatainput","title":"NoneDataInput","text":"<p>A special type for not using data e.g. in validation</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_11","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description type <code>const</code> \u2705 <code>NONE</code> data_type <code>string</code> string <code>\"ChatConversation\"</code> Generally, the data_type is automatically set based on the experiment config method."},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#overrides","title":"Overrides","text":"<p>Override options</p> <p>These implement dynamic scaling for the learning rate.</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_12","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description lr_multiplier <code>number</code> number <code>1.0</code> Multiplier applied to the learning rate in the training_args lr_batch_size_scaling <code>string</code> <code>none</code> <code>sqrt</code> <code>linear</code> <code>\"none\"</code> Scales the learning rate in the training_args by a factor derived from the total training batch size.             'none': No scaling.             'sqrt': Multiplies learning rate by square root of batch size (a classic scaling rule).             'linear': Multiplies learning rate by the batch size (a more modern scaling rule)."},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#pefttype","title":"PeftType","text":"<p>Enum class for the different types of adapters in PEFT.</p> <p>Supported PEFT types: - PROMPT_TUNING - MULTITASK_PROMPT_TUNING - P_TUNING - PREFIX_TUNING - LORA - ADALORA - BOFT - ADAPTION_PROMPT - IA3 - LOHA - LOKR - OFT - XLORA - POLY - LN_TUNING - VERA - FOURIERFT - HRA</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-string_2","title":"Type: <code>string</code>","text":"<p>Possible Values: <code>PROMPT_TUNING</code> or <code>MULTITASK_PROMPT_TUNING</code> or <code>P_TUNING</code> or <code>PREFIX_TUNING</code> or <code>LORA</code> or <code>ADALORA</code> or <code>BOFT</code> or <code>ADAPTION_PROMPT</code> or <code>IA3</code> or <code>LOHA</code> or <code>LOKR</code> or <code>OFT</code> or <code>POLY</code> or <code>LN_TUNING</code> or <code>VERA</code> or <code>FOURIERFT</code> or <code>XLORA</code> or <code>HRA</code> or <code>VBLORA</code></p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#pretrainedpeftconfig","title":"PretrainedPeftConfig","text":"<p>PEFT adapter uses the config and initialisation from a pretrained adapter</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_13","title":"Type: <code>object</code>","text":"Property Type Required Possible values Description peft_type <code>const</code> \u2705 <code>PRETRAINED_PEFT</code> name_or_path <code>string</code> \u2705 string HF ID or path to the pretrained peft."},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#runconfig","title":"RunConfig","text":"<p>Experiment running configuration</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_14","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description model <code>string</code> string <code>\"/local_resources/basemodel\"</code> Local path to model to be fine-tuned. Normally this should be /local_resources/basemodel model_args <code>object</code> ModelArguments <code>{\"torch_dtype\": \"auto\", \"device_map\": \"auto\", \"max_memory\": null, \"low_cpu_mem_usage\": false, \"attn_implementation\": null, \"offload_folder\": null, \"offload_state_dict\": null, \"offload_buffers\": null, \"use_cache\": true, \"cache_dir\": null, \"force_download\": false, \"local_files_only\": false, \"proxies\": null, \"resume_download\": false, \"revision\": \"main\", \"code_revision\": \"main\", \"subfolder\": null, \"token\": null, \"use_safetensors\": null, \"variant\": null, \"trust_remote_code\": false}</code> tokenizer <code>string</code> or <code>null</code> string Model HuggingFace ID, or path, or None to use the one associated with the model use_fast_tokenizer <code>boolean</code> boolean <code>true</code> Use the Fast version of the tokenizer. The 'slow' version may be compatible with more features. resume_from_checkpoint <code>boolean</code> or <code>string</code> boolean and/or string Normally should be set to 'auto' to continue if a checkpoint exists.        Can set to True to always try to continue, False to never try, or a path to load from a specific path. final_checkpoint_name <code>string</code> string <code>\"checkpoint-final\"</code> Name of final checkpoint. Should be left as default determinism <code>string</code> <code>no</code> <code>half</code> <code>full</code> <code>\"no\"</code> Set the level of determinism in implementations. Deterministic implementations are not always available,            and when they are, they are usually slower than their non-deterministic counterparts. Recommended for            debugging only.            'no': No determinism.            'half': Prefer deterministic implementations.            'full': Only fully deterministic implementations, error out on operations that only have non-deterministic                    implementations."},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#sftarguments","title":"SFTArguments","text":"<p>Supervised fine-tuning arguments</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_15","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description max_seq_length <code>integer</code> integer <code>2048</code> Maximum length input sequence length. Longer sequences will be filtered out. save_name_if_new_basemodel <code>string</code> string <code>\"checkpoint-new-basemodel\"</code> If a new basemodel is saved, it will be saved with this name train_on_completions_only <code>boolean</code> boolean <code>False</code> Only compute loss on the assistant's turns."},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#silogentrainingarguments","title":"SilogenTrainingArguments","text":"<p>HuggingFace TrainingArguments as Config with additional SiloGen conventions</p> <p>The list of training arguments is best available online (the version might not be up-to-date here): https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments</p> <p>The TrainingArguments object does a lot of things besides specifying the training configuaration options (e.g. it has computed properties like true training batch size etc.)</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#tasktype","title":"TaskType","text":"<p>Enum class for the different types of tasks supported by PEFT.</p> <p>Overview of the supported task types: - SEQ_CLS: Text classification. - SEQ_2_SEQ_LM: Sequence-to-sequence language modeling. - CAUSAL_LM: Causal language modeling. - TOKEN_CLS: Token classification. - QUESTION_ANS: Question answering. - FEATURE_EXTRACTION: Feature extraction. Provides the hidden states which can be used as embeddings or features   for downstream tasks.</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-string_3","title":"Type: <code>string</code>","text":"<p>Possible Values: <code>SEQ_CLS</code> or <code>SEQ_2_SEQ_LM</code> or <code>CAUSAL_LM</code> or <code>TOKEN_CLS</code> or <code>QUESTION_ANS</code> or <code>FEATURE_EXTRACTION</code></p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#weighteddatasetdefinition","title":"WeightedDatasetDefinition","text":"<p>Define a dataset, with a weight for sampling</p>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_16","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description path <code>string</code> \u2705 string Local path to a JSONL file in the finetuning data format sampling_weight <code>number</code> number <code>1.0</code>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#weightedmixdatainput","title":"WeightedMixDataInput","text":"<p>A list of datasets where each is sampled by a certain weight</p> <p>These datasets are interleaved based on the sampling weights. The resulting dataset is fully precomputed, upto the point where every single sample in every dataset gets picked. This means that with small sampling weights, it can take a lot of draws to see every sample from a dataset and so the resulting dataset can be very large.</p> <p>The datasets themselves need to be in the finetuning supported JSONL formats. For SFT this means lines:</p> <pre><code>{\"messages\": {\"content\": \"string\", \"role\": \"string\"}}\n</code></pre> <p>For DPO this means lines of:</p> <pre><code>{\"prompt_messages\": {\"content\": \"string\", \"role\": \"string\"}, \"chosen_messages\": {\"content\": \"string\", \"role\": \"string\"}, \"rejected_messages\": {\"content\": \"string\", \"role\": \"string\"}}\n</code></pre>"},{"location":"workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_17","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description type <code>const</code> \u2705 <code>PRECOMPUTE_WEIGHTED_MIX</code> datasets <code>array</code> \u2705 WeightedDatasetDefinition data_type <code>string</code> string <code>\"ChatConversation\"</code> Generally, the data_type is automatically set based on the experiment config method. seed <code>integer</code> integer <code>19851243</code> Seed for the random number generator for interleaving draws"},{"location":"workloads/llm-finetune-verl/helm/","title":"Finetuning with VeRL","text":"<p>This is a Helm Chart for running a finetuning job using VeRL</p> <p>The output is saved with MinIO in the directory specified by <code>checkpointsRemote</code>.</p>"},{"location":"workloads/llm-finetune-verl/helm/#configuration","title":"Configuration","text":"<p>Include any parameters for VeRL in the <code>verlConfig</code> parameter. See the override file <code>overrides/ppo_qwen_gsm8k.yaml</code> for an example and the VeRL documentation for more details.</p>"},{"location":"workloads/llm-finetune-verl/helm/#running-the-workload","title":"Running the workload","text":"<p>The simplest is to run <code>helm template</code> and pipe the result to <code>kubectl create</code>.</p> <p>Example command using the example override file <code>overrides/ppo_qwen_gsm8k.yaml</code>:</p> <pre><code>helm template workloads/llm-finetune-verl/helm \\\n  --values workloads/llm-finetune-verl/helm/overrides/ppo_qwen_gsm8k.yaml \\\n  --name-template ppo-qwen-gsm8k-verl \\\n  | kubectl create -f -\n</code></pre>"},{"location":"workloads/llm-finetune-verl/helm/#data-specification","title":"Data specification","text":"<p>VeRL requires that the data is prepared for the policy training in a particular way.</p> <p>Some example data preprocess scripts are provided, to use one of these, specify the name of data set used for training as <code>dataset</code>. Available datasets are \"full_hh_rlhf\", \"geo3k\", \"gsm8k\", \"hellaswag\", \"math_dataset\".</p> <p>To use your own datasets from MinIO, specify the path as <code>datasetRemote</code>. It should point to a directory with files that have already been appropriately processed (<code>train.parquet</code> and <code>test.parquet</code>).</p>"},{"location":"workloads/llm-finetune-verl/helm/#model-specification","title":"Model specification","text":"<p>To use a base model from HuggingFace or other source directly supported by LLaMA-Factory, specify the model name in <code>modelName</code>.</p> <p>Alternatively to use a model from MinIO, specify the path to the model in <code>modelRemote</code>.</p> <p>Either <code>modelName</code> or <code>modelRemote</code> must be specified. If both are included, the model from <code>modelRemote</code> is used.</p>"},{"location":"workloads/llm-finetune-verl/helm/#cleanup","title":"Cleanup","text":"<p>After the jobs are completed, please delete the resources created. To delete the resources, you can run the same <code>helm template</code> command, only replacing <code>kubectl create</code> with <code>kubectl delete</code>, e.g.:</p> <pre><code>helm template workloads/llm-finetune-verl/helm \\\n  --values workloads/llm-finetune-verl/helm/overrides/ppo_qwen_gsm8k.yaml \\\n  --name-template ppo-qwen-gsm8k-verl \\\n  | kubectl delete -f -\n</code></pre>"},{"location":"workloads/llm-inference-llamacpp-mi300x/helm/","title":"LLM Inference Service with Llama.cpp","text":"<p>This Helm chart deploys a Large Language Model (LLM) inference service using llama.cpp. The chart clones the llama.cpp source code from GitHub and compiles it into optimized binaries based on the target GPU architecture. Upon deployment, the service downloads pre-trained GGUF models from Hugging Face and exposes an OpenAI-compatible API via an HTTP endpoint</p>"},{"location":"workloads/llm-inference-llamacpp-mi300x/helm/#prerequisites","title":"Prerequisites","text":"<p>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</p>"},{"location":"workloads/llm-inference-llamacpp-mi300x/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>Basic configurations are defined in the <code>values.yaml</code> file.</p> <p>The default model is 1.73-bit quantized DeepSeek-R1-UD-IQ1_M, which fits in one MI300X GPU (192GB VRAM) and can serve with a context length of 4K.</p> <p>For example: run the following command within the <code>helm/</code> folder to deploy the service:</p> <pre><code>helm template . --set env_vars.TEMP=\"0.8\" | kubectl apply -f -\n</code></pre> <p>Note: Compiling llama.cpp executables and downloading/merging the GGUF files of DeepSeek R1 (~200GB) from HuggingFace can take a significant amount of time. The deployment process may take over 30 minutes before the LLM inference service is ready.</p>"},{"location":"workloads/llm-inference-llamacpp-mi300x/helm/#interacting-with-the-deployed-model","title":"Interacting with the Deployed Model","text":""},{"location":"workloads/llm-inference-llamacpp-mi300x/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment and service status:</p> <pre><code>kubectl get deployment\nkubectl get service\n</code></pre>"},{"location":"workloads/llm-inference-llamacpp-mi300x/helm/#port-forwarding","title":"Port Forwarding","text":"<p>To access the service locally, forward the port using the following commands. This assumes the service name is <code>llm-inference-llamacpp</code>:</p> <pre><code>kubectl port-forward services/llm-inference-llamacpp 8080:80\n</code></pre> <p>You can access the Llama.cpp server's WebUI at <code>http://localhost:8080</code> using a web browser.</p> <p>Additionally, an OpenAI-compatible API endpoint is available at <code>http://localhost:8080/v1</code></p>"},{"location":"workloads/llm-inference-llamacpp-mi300x/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/llm-inference-megatron-lm/helm/","title":"llm-inference-megatron-lm","text":""},{"location":"workloads/llm-inference-megatron-lm/helm/#pretrained-llm-inference-with-megatron-lm-on-mi300x","title":"Pretrained LLM inference with Megatron-LM on MI300X","text":"<p>This Helm Chart deploys the LLM Inference Megatron-LM workload.</p>"},{"location":"workloads/llm-inference-megatron-lm/helm/#prerequisites","title":"Prerequisites","text":"<p>The following prerequisites should be met before deploying this workload::</p> <ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li>Secrets: Secret containing the S3 storage provider's HMAC credentials should be created in the namespace, where workload runs. Default secret has name <code>minio-credentials</code> with keys <code>minio-access-key</code> and <code>minio-secret-key</code>.</li> </ol>"},{"location":"workloads/llm-inference-megatron-lm/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>To deploy workload pipe the result of <code>helm template</code> command to <code>kubectl apply</code>. Generally, full command looks as follows</p> <p><pre><code>helm template [release-name] &lt;helm-chart-dir&gt; [-f &lt;path/to/overrides/xyz.yaml&gt;] [--set &lt;name&gt;=&lt;value&gt;] | kubectl apply -f - [-n namespace]\n</code></pre> where - release-name: is the optional name of the helm release of the deployed job. - helm-chart-dir: is the path to the directory where this helm chart is located. - -f path/to/overrides/xyz.yaml: optional, can be multiple and presents a path to helm values overrides. - --set &lt;name&gt;=&lt;value&gt;: optional, can be multiple and allows overriding single entry from the default values. - namespace: is an optional kubernetes namespace of the deployment, if different from the default one.</p> <p>An example of the command that will use default helm values and deploys workload to a default namespoace is shown below <pre><code>helm template workloads/llm-inference-megatron-lm/helm/ | kubectl apply -f -\n</code></pre></p>"},{"location":"workloads/llm-inference-megatron-lm/helm/#interacting-with-deployed-model","title":"Interacting with Deployed Model","text":""},{"location":"workloads/llm-inference-megatron-lm/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment status:</p> <p><pre><code>kubectl get deployment [-n namespace]\n</code></pre> You should see a deployment with a name starting with the prefix <code>llm-inference-megatron-lm-</code> up and running.</p> <p>To see service deployed by the workload run</p> <pre><code>kubectl get svc [-n namespace]\n</code></pre> <p>The service should have a name starting with <code>llm-inference-megatron-lm-</code>. Note the port exposed by the service, it is expected to be the port <code>80</code>.</p>"},{"location":"workloads/llm-inference-megatron-lm/helm/#port-forwarding","title":"Port Forwarding","text":"<p>Forward the port to access the service to the local machine at e.g. port <code>5000</code>. Assuming the service is named <code>llm-inference-megatron-lm-20250522-1521</code> and the port exposed is <code>80</code>, use the following command:</p> <pre><code>kubectl port-forward [-n namespace] svc/llm-inference-megatron-lm-20250522-1521 5000:80\n</code></pre>"},{"location":"workloads/llm-inference-megatron-lm/helm/#test-the-model-inference-service","title":"Test the model inference service","text":"<p>Send a request to the service to get a reply from the model using <code>curl</code> command:</p> <pre><code>curl -X PUT -H \"Content-Type: application/json\" -d '{\"prompts\": [\"This is a test prompt.\"], \"tokens_to_generate\": 50}' http://localhost:5000/api\n</code></pre> <p>Another way to interact with the inference service is to use the <code>test_manual.py</code> script located in the <code>llm-inference-megatron-lm/helm/mount</code> directory. This script prompts for multiple questions interactively. To run it, use the following command (assuming repo root is your current directory):</p> <pre><code>python workloads/llm-inference-megatron-lm/helm/mount/test_manual.py localhost:5000\n</code></pre> <p>You can also run a quick sanity check to evaluate the coherence of the model by using the <code>coherence.py</code> script located in the <code>llm-inference-megatron-lm/helm/mount</code> directory. This file contains multiple questions along with their corresponding expected answers. When the model's generated response matches the expected answer, a point is awarded. At the end, the user can assess the model's coherence performance based on the total score. To run the evaluation, use the following command:</p> <pre><code>python workloads/llm-inference-megatron-lm/helm/mount/coherence.py localhost:5000\n</code></pre>"},{"location":"workloads/llm-inference-megatron-lm/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/llm-inference-ollama/helm/","title":"LLM Inference Service with Llama.cpp","text":"<p>This Helm chart deploys a LLM inference service workload via Ollama</p>"},{"location":"workloads/llm-inference-ollama/helm/#prerequisites","title":"Prerequisites","text":"<p>Install <code>helm</code>. Refer to the Helm documentation for instructions.</p>"},{"location":"workloads/llm-inference-ollama/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>Basic configurations for the deployment are specified in the <code>values.yaml</code> file. By default, the service uses the quantized <code>Gemma3:4b</code> model. For a comprehensive list of available models, visit the Ollama Model Library.</p> <p>For example: run the following command within the <code>helm/</code> folder to deploy the service:</p> <pre><code>helm template . --set env_vars.MODEL=\"gemma3:27b-it-fp16\" | kubectl apply -f -\n</code></pre> <p>Note: Compiling Ollama executables and downloading models can take a significant amount of time. The deployment process may take over 10 minutes before the LLM inference service is ready.</p>"},{"location":"workloads/llm-inference-ollama/helm/#interacting-with-the-deployed-model","title":"Interacting with the Deployed Model","text":""},{"location":"workloads/llm-inference-ollama/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment and service status:</p> <pre><code>kubectl get deployment\nkubectl get service\n</code></pre>"},{"location":"workloads/llm-inference-ollama/helm/#port-forwarding","title":"Port Forwarding","text":"<p>To access the service locally, forward the port using the following commands. This assumes the service name is <code>llm-inference-ollama</code>:</p> <pre><code>kubectl port-forward services/llm-inference-ollama 8080:80\n</code></pre> <p>Note Ollama server provides both Ollama API <code>http://localhost:8080/api</code> and OpenAI-compatible API <code>http://localhost:8080/v1</code></p>"},{"location":"workloads/llm-inference-ollama/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/llm-inference-openai-benchmark-guidellm/helm/","title":"OpenAI-compatible Endpoint Benchmarking using GuideLLM","text":""},{"location":"workloads/llm-inference-openai-benchmark-guidellm/helm/#overview","title":"Overview","text":"<p>This Helm chart deploys a batch job that benchmarks LLM performance against an OpenAI-compatible API endpoint using GuideLLM.</p>"},{"location":"workloads/llm-inference-openai-benchmark-guidellm/helm/#prerequisites","title":"Prerequisites","text":"<ul> <li>Helm: Helm installed on your system</li> </ul>"},{"location":"workloads/llm-inference-openai-benchmark-guidellm/helm/#configuration","title":"Configuration","text":"Parameter Description Default <code>env_vars.OPENAI_API_BASE_URL</code> OpenAI-compatible API endpoint URL <code>\"http://example-open-ai-api-server.com/v1\"</code> <code>env_vars.TOKENIZER</code> HuggingFace model repository for token calculations <code>deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</code> <code>env_vars.HF_TOKEN</code> HuggingFace access token (required for gated models like Mistral or Llama) not set"},{"location":"workloads/llm-inference-openai-benchmark-guidellm/helm/#deployment","title":"Deployment","text":"<p>Deploy the benchmark job with the following command:</p> <pre><code>helm template . --set env_vars.OPENAI_API_BASE_URL=\"http://your-api-endpoint.com/v1/\" | kubectl apply -f -\n</code></pre> <p>Monitor job progress with <code>kubectl logs</code>, and review the benchmark results in the final output of the completed job.</p> <p>Example output:</p> <pre><code>\u256d\u2500 GuideLLM Benchmarks Report (stdout) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Benchmark Report 1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Backend(type=openai_server,                                              \u2502 \u2502\n\u2502 \u2502 target=http://llm-inference-vllm-llama-4-maverick/v1,                    \u2502 \u2502\n\u2502 \u2502 model=meta-llama/Llama-4-Maverick-17B-128E-Instruct)                     \u2502 \u2502\n\u2502 \u2502 Data(type=emulated, source=prompt_tokens=512,generated_tokens=128,       \u2502 \u2502\n\u2502 \u2502 tokenizer=meta-llama/Llama-4-Maverick-17B-128E-Instruct)                 \u2502 \u2502\n\u2502 \u2502 Rate(type=sweep, rate=None)                                              \u2502 \u2502\n\u2502 \u2502 Limits(max_number=None requests, max_duration=120 sec)                   \u2502 \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502 Requests Data by Benchmark                                               \u2502 \u2502\n\u2502 \u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2502 \u2502\n\u2502 \u2502 \u2503           \u2503 Requests  \u2503 Request   \u2503           \u2503 Start     \u2503          \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503 Benchmark \u2503 Completed \u2503 Failed    \u2503 Duration  \u2503 Time      \u2503 End Time \u2503 \u2502 \u2502\n\u2502 \u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 \u2502\n\u2502 \u2502 \u2502 synchron\u2026 \u2502 101/101   \u2502 0/101     \u2502 119.00    \u2502 08:21:17  \u2502 08:23:16 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502           \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 354/354   \u2502 0/354     \u2502 119.80    \u2502 08:23:43  \u2502 08:25:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 607/607   \u2502 0/607     \u2502 119.99    \u2502 08:25:43  \u2502 08:27:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 854/854   \u2502 0/854     \u2502 119.93    \u2502 08:27:43  \u2502 08:29:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 1099/1099 \u2502 0/1099    \u2502 119.96    \u2502 08:29:43  \u2502 08:31:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 1331/1331 \u2502 0/1331    \u2502 119.98    \u2502 08:31:43  \u2502 08:33:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 1578/1578 \u2502 0/1578    \u2502 119.49    \u2502 08:33:43  \u2502 08:35:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 1798/1798 \u2502 0/1798    \u2502 119.90    \u2502 08:35:43  \u2502 08:37:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 1885/1885 \u2502 0/1885    \u2502 119.92    \u2502 08:37:43  \u2502 08:39:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 2026/2026 \u2502 0/2026    \u2502 119.35    \u2502 08:39:44  \u2502 08:41:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 throughp\u2026 \u2502 505/505   \u2502 0/505     \u2502 25.14 sec \u2502 08:23:18  \u2502 08:23:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502 Tokens Data by Benchmark                                                 \u2502 \u2502\n\u2502 \u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2502 \u2502\n\u2502 \u2502 \u2503                 \u2503        \u2503 Prompt (1%,    \u2503        \u2503                 \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503                 \u2503        \u2503 5%, 50%, 95%,  \u2503        \u2503 Output (1%, 5%, \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503 Benchmark       \u2503 Prompt \u2503 99%)           \u2503 Output \u2503 50%, 95%, 99%)  \u2503 \u2502 \u2502\n\u2502 \u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 \u2502\n\u2502 \u2502 \u2502 synchronous     \u2502 512.49 \u2502 512.0, 512.0,  \u2502 127.84 \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@2\u2026 \u2502 512.33 \u2502 512.0, 512.0,  \u2502 127.70 \u2502 114.7, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@5\u2026 \u2502 512.37 \u2502 512.0, 512.0,  \u2502 127.61 \u2502 112.3, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@7\u2026 \u2502 512.37 \u2502 512.0, 512.0,  \u2502 127.73 \u2502 115.1, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@9\u2026 \u2502 512.34 \u2502 512.0, 512.0,  \u2502 127.86 \u2502 125.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 512.33 \u2502 512.0, 512.0,  \u2502 127.75 \u2502 119.3, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 512.34 \u2502 512.0, 512.0,  \u2502 127.75 \u2502 118.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 512.36 \u2502 512.0, 512.0,  \u2502 127.78 \u2502 119.9, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 512.36 \u2502 512.0, 512.0,  \u2502 127.79 \u2502 118.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@2\u2026 \u2502 512.36 \u2502 512.0, 512.0,  \u2502 127.79 \u2502 123.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 throughput      \u2502 512.38 \u2502 512.0, 512.0,  \u2502 127.72 \u2502 116.3, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502 Performance Stats by Benchmark                                           \u2502 \u2502\n\u2502 \u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2502 \u2502\n\u2502 \u2502 \u2503                 \u2503                 \u2503                \u2503 Inter Token     \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503                 \u2503 Request Latency \u2503 Time to First  \u2503 Latency [1%,    \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503                 \u2503 [1%, 5%, 10%,   \u2503 Token [1%, 5%, \u2503 5%, 10%, 50%,   \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503                 \u2503 50%, 90%, 95%,  \u2503 10%, 50%, 90%, \u2503 90% 95%, 99%]   \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503 Benchmark       \u2503 99%] (sec)      \u2503 95%, 99%] (ms) \u2503 (ms)            \u2503 \u2502 \u2502\n\u2502 \u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 \u2502\n\u2502 \u2502 \u2502 synchronous     \u2502 1.17, 1.17,     \u2502 60.2, 60.7,    \u2502 8.0, 8.6, 8.7,  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 1.17, 1.18,     \u2502 61.0, 62.1,    \u2502 8.7, 8.9, 9.0,  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 1.19, 1.19,     \u2502 63.6, 64.2,    \u2502 9.4             \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 1.20            \u2502 69.3           \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@2\u2026 \u2502 1.46, 1.59,     \u2502 57.7, 58.7,    \u2502 0.2, 10.2,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 1.59, 1.60,     \u2502 59.5, 63.1,    \u2502 10.3, 10.9,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 1.61, 1.62,     \u2502 68.5, 71.7,    \u2502 12.0, 15.4,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 1.65            \u2502 117.9          \u2502 48.1            \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@5\u2026 \u2502 1.71, 1.91,     \u2502 57.3, 58.2,    \u2502 0.2, 10.9,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 1.91, 1.93,     \u2502 59.1, 64.2,    \u2502 11.4, 11.8,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 1.96, 2.00,     \u2502 72.1, 115.5,   \u2502 16.8, 46.3,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 2.11            \u2502 138.1          \u2502 49.3            \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@7\u2026 \u2502 2.17, 2.39,     \u2502 58.4, 59.4,    \u2502 0.2, 11.8,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 2.42, 2.44,     \u2502 60.2, 64.3,    \u2502 12.7, 13.4,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 2.50, 2.57,     \u2502 74.6, 124.3,   \u2502 47.0, 48.9,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 2.96            \u2502 145.3          \u2502 50.5            \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@9\u2026 \u2502 2.78, 3.05,     \u2502 62.3, 63.4,    \u2502 0.2, 13.9,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 3.09, 3.16,     \u2502 64.3, 67.1,    \u2502 14.4, 18.5,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 3.21, 3.25,     \u2502 81.6, 130.7,   \u2502 48.8, 50.6,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 3.32            \u2502 201.6          \u2502 53.2            \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 3.56, 4.29,     \u2502 70.1, 71.5,    \u2502 0.2, 13.1,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 4.36, 4.66,     \u2502 72.2, 78.6,    \u2502 19.8, 27.7,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 4.83, 5.02,     \u2502 104.1, 176.4,  \u2502 53.8, 54.7,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 5.39            \u2502 356.6          \u2502 70.4            \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 4.25, 4.53,     \u2502 61.9, 74.5,    \u2502 0.2, 14.3,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 4.61, 4.75,     \u2502 81.1, 149.6,   \u2502 17.4, 25.6,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 5.04, 5.13,     \u2502 460.8, 538.6,  \u2502 59.9, 70.1,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 5.30            \u2502 712.5          \u2502 122.4           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 4.11, 4.27,     \u2502 92.2, 217.3,   \u2502 0.2, 13.8,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 4.35, 4.72,     \u2502 345.8, 710.8,  \u2502 15.9, 24.4,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 5.41, 6.05,     \u2502 1255.9,        \u2502 54.8, 57.7,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 6.28            \u2502 1559.3, 2121.3 \u2502 84.8            \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 4.11, 4.40,     \u2502 129.0, 666.0,  \u2502 0.1, 0.2, 0.2,  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 4.71, 8.49,     \u2502 1044.1,        \u2502 22.2, 55.1,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 13.42, 14.13,   \u2502 4703.2,        \u2502 61.2, 112.8     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 14.62           \u2502 9479.4,        \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502                 \u2502 10238.6,       \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502                 \u2502 10723.4        \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@2\u2026 \u2502 3.85, 4.35,     \u2502 137.9, 920.0,  \u2502 0.1, 0.2, 0.2,  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 4.86, 10.38,    \u2502 1499.8,        \u2502 19.8, 54.4,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 17.68, 18.69,   \u2502 6826.9,        \u2502 64.0, 149.5     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 19.42           \u2502 14005.3,       \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502                 \u2502 14994.2,       \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502                 \u2502 15808.3        \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 throughput      \u2502 3.02, 3.24,     \u2502 295.8, 497.8,  \u2502 0.1, 0.2, 0.2,  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 3.35, 6.70,     \u2502 692.8, 4160.1, \u2502 18.3, 44.3,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 9.33, 9.59,     \u2502 6786.1,        \u2502 48.8, 56.4      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 10.19           \u2502 7115.2, 7739.8 \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502 Performance Summary by Benchmark                                         \u2502 \u2502\n\u2502 \u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2502 \u2502\n\u2502 \u2502 \u2503           \u2503 Requests  \u2503           \u2503 Time to   \u2503 Inter     \u2503 Output   \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503           \u2503 per       \u2503 Request   \u2503 First     \u2503 Token     \u2503 Token    \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503 Benchmark \u2503 Second    \u2503 Latency   \u2503 Token     \u2503 Latency   \u2503 Through\u2026 \u2503 \u2502 \u2502\n\u2502 \u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 \u2502\n\u2502 \u2502 \u2502 synchron\u2026 \u2502 0.85      \u2502 1.18 sec  \u2502 62.29 ms  \u2502 8.72 ms   \u2502 108.50   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502           \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 2.96      \u2502 1.60 sec  \u2502 65.41 ms  \u2502 12.02 ms  \u2502 377.37   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 5.06      \u2502 1.93 sec  \u2502 69.15 ms  \u2502 14.61 ms  \u2502 645.58   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 7.12      \u2502 2.46 sec  \u2502 71.39 ms  \u2502 18.69 ms  \u2502 909.53   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 9.16      \u2502 3.15 sec  \u2502 76.51 ms  \u2502 23.99 ms  \u2502 1171.39  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 11.09     \u2502 4.61 sec  \u2502 91.56 ms  \u2502 35.39 ms  \u2502 1417.25  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 13.21     \u2502 4.78 sec  \u2502 212.79 ms \u2502 35.77 ms  \u2502 1687.03  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 15.00     \u2502 4.81 sec  \u2502 762.19 ms \u2502 31.71 ms  \u2502 1916.20  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 15.72     \u2502 8.74 sec  \u2502 4904.15   \u2502 30.04 ms  \u2502 2008.65  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502 ms        \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 16.98     \u2502 10.77 sec \u2502 7203.12   \u2502 27.90 ms  \u2502 2169.26  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502 ms        \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 throughp\u2026 \u2502 20.09     \u2502 6.79 sec  \u2502 4279.14   \u2502 19.54 ms  \u2502 2565.91  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502           \u2502 req/sec   \u2502           \u2502 ms        \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"workloads/llm-inference-openai-benchmark-guidellm/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/llm-inference-openai-benchmark-rocmblog/helm/","title":"OpenAI-compatible Endpoint Benchmarking","text":"<p>This Helm chart defines a batch job to benchmark LLM performance using vLLM's benchmarking script against OpenAI-compatible API endpoints. It follows the best practices for optimized inference on AMD Instinct GPUs.</p>"},{"location":"workloads/llm-inference-openai-benchmark-rocmblog/helm/#prerequisites-and-configuration","title":"Prerequisites and Configuration","text":"<ol> <li> <p>Helm: Ensure <code>helm</code> is installed. Refer to the Helm documentation for installation instructions.</p> </li> <li> <p>MinIO Storage: Required for saving benchmark results. Configure the following environment variables in <code>values.yaml</code>:</p> <ul> <li><code>BUCKET_STORAGE_HOST</code></li> <li><code>BUCKET_STORAGE_ACCESS_KEY</code></li> <li><code>BUCKET_STORAGE_SECRET_KEY</code></li> <li><code>BUCKET_RESULT_PATH</code></li> </ul> </li> <li> <p>API Endpoint: An OpenAI-compatible API endpoint is required. Configure this in <code>values.yaml</code> as <code>env_vars.OPENAI_API_BASE_URL</code> or override using the <code>--set</code> option with Helm.</p> </li> <li> <p>Tokenizer: Required for token calculations. Specify a HuggingFace model repository in <code>values.yaml</code> by setting <code>env_vars.TOKENIZER</code>. The default is <code>deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</code>.</p> </li> <li> <p>HuggingFace Token (optional): Set the <code>env_vars.HF_TOKEN</code> environment variable if using gated tokenizers (e.g., Mistral and Llama models) from HuggingFace.</p> </li> </ol>"},{"location":"workloads/llm-inference-openai-benchmark-rocmblog/helm/#benchmark-configuration","title":"Benchmark Configuration","text":"<p>The benchmark behavior can be customized using the following environment variables:</p> <ul> <li><code>INPUT_LENGTH</code> (default: <code>2048</code>): Sets the input token length for benchmark requests</li> <li><code>OUTPUT_LENGTH</code> (default: <code>2048</code>): Sets the output token length for benchmark requests</li> <li><code>QPS</code> (default: <code>inf</code>): Sets the queries per second rate. Can be:</li> <li>A single value: <code>\"10\"</code>, <code>\"inf\"</code> (unlimited)</li> <li>Multiple space-separated values: <code>\"1 5 10 inf\"</code> (runs tests at each rate)</li> </ul> <p>The benchmark automatically tests with request concurrency levels of 1, 2, 4, 8, 16, 32, 64, 128, and 256.</p>"},{"location":"workloads/llm-inference-openai-benchmark-rocmblog/helm/#deployment-example","title":"Deployment Example","text":"<p>To deploy the chart, run the following command in the <code>helm/</code> directory:</p> <pre><code>helm template . \\\n    --set env_vars.OPENAI_API_BASE_URL=\"http://example-open-ai-api-server.com/v1/\" \\\n    --set env_vars.TOKENIZER=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" \\\n    --set env_vars.INPUT_LENGTH=\"1024\" \\\n    --set env_vars.OUTPUT_LENGTH=\"512\" \\\n    --set env_vars.QPS=\"1 5 10 inf\" | \\\n    kubectl apply -f -\n</code></pre>"},{"location":"workloads/llm-inference-openai-benchmark-rocmblog/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/llm-inference-sglang/helm/","title":"LLM Inference with SGLang","text":"<p>This Helm Chart deploys the LLM Inference SGLang workload.</p>"},{"location":"workloads/llm-inference-sglang/helm/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following prerequisites are met before deploying any workloads:</p> <ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li>Secrets: Create the following secrets in the namespace:<ul> <li><code>minio-credentials</code> with keys <code>minio-access-key</code> and <code>minio-secret-key</code>.</li> <li><code>hf-token</code> with key <code>hf-token</code>.</li> </ul> </li> </ol>"},{"location":"workloads/llm-inference-sglang/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>It is recommended to use <code>helm template</code> and pipe the result to <code>kubectl apply</code> , rather than using <code>helm install</code>. Generally, a command looks as follows <pre><code>helm template [optional-release-name] &lt;helm-dir&gt; -f &lt;overrides/xyz.yaml&gt; --set &lt;name&gt;=&lt;value&gt; | kubectl apply -n &lt;namespace&gt; -f -\n</code></pre></p> <p>The chart provides three main ways to deploy models, detailed below.</p>"},{"location":"workloads/llm-inference-sglang/helm/#alternative-1-deploy-a-specific-model-configuration","title":"Alternative 1: Deploy a Specific Model Configuration","text":"<p>To deploy a specific model along with its settings, use the following command from the <code>helm</code> directory:</p> <pre><code>helm template tiny-llama . -f overrides/models/tinyllama_tinyllama-1.1b-chat-v1.0.yaml | kubectl apply -f -\n</code></pre>"},{"location":"workloads/llm-inference-sglang/helm/#alternative-2-override-the-model","title":"Alternative 2: Override the Model","text":"<p>You can also override the model on the command line:</p> <pre><code>helm template qwen2-0-5b . --set model=Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre>"},{"location":"workloads/llm-inference-sglang/helm/#alternative-3-deploy-a-model-from-bucket-storage","title":"Alternative 3: Deploy a Model from Bucket Storage","text":"<p>If you have downloaded your model to bucket storage, use:</p> <pre><code>helm template qwen2-0-5b . --set model=s3://models/Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre> <p>The model will be automatically downloaded before starting the inference server.</p>"},{"location":"workloads/llm-inference-sglang/helm/#user-input-values","title":"User Input Values","text":"<p>Refer to the <code>values.yaml</code> file for the user input values you can provide, along with instructions.</p>"},{"location":"workloads/llm-inference-sglang/helm/#interacting-with-deployed-model","title":"Interacting with Deployed Model","text":""},{"location":"workloads/llm-inference-sglang/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment status:</p> <pre><code>kubectl get deployment\n</code></pre>"},{"location":"workloads/llm-inference-sglang/helm/#port-forwarding","title":"Port Forwarding","text":"<p>Forward the port to access the service (assuming the service is named <code>llm-inference-sglang-tiny-llama</code> ):</p> <pre><code>kubectl port-forward services/llm-inference-sglang-tiny-llama 8080:80\n</code></pre>"},{"location":"workloads/llm-inference-sglang/helm/#test-the-deployment","title":"Test the Deployment","text":"<p>Send a test request to verify the service, assuming <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code> model:</p> <pre><code>curl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -X POST \\\n    -d '{\n        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n        ]\n    }'\n</code></pre>"},{"location":"workloads/llm-inference-sglang/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/llm-inference-tgi/helm/","title":"LLM Inference with TGI","text":"<p>This Helm Chart deploys the LLM Inference vLLM workload.</p>"},{"location":"workloads/llm-inference-tgi/helm/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following prerequisites are met before deploying any workloads:</p> <ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li>Secrets: Create the following secrets in the namespace:<ul> <li><code>minio-credentials</code> with keys <code>minio-access-key</code> and <code>minio-secret-key</code>.</li> <li><code>hf-token</code> with key <code>hf-token</code>.</li> </ul> </li> </ol>"},{"location":"workloads/llm-inference-tgi/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>It is recommended to use <code>helm template</code> and pipe the result to <code>kubectl create</code>, rather than using <code>helm install</code>. Generally, a command looks as follows <pre><code>helm template [optional-release-name] &lt;helm-dir&gt; -f &lt;overrides/xyz.yaml&gt; --set &lt;name&gt;=&lt;value&gt; | kubectl apply -f -\n</code></pre></p> <p>The chart provides three main ways to deploy models, detailed below.</p>"},{"location":"workloads/llm-inference-tgi/helm/#alternative-1-deploy-a-specific-model-configuration","title":"Alternative 1: Deploy a Specific Model Configuration","text":"<p>To deploy a specific model along with its settings, use the following command from the <code>helm</code> directory:</p> <pre><code>helm template tiny-llama . -f overrides/models/tinyllama_tinyllama-1.1b-chat-v1.0.yaml | kubectl apply -f -\n</code></pre>"},{"location":"workloads/llm-inference-tgi/helm/#alternative-2-override-the-model","title":"Alternative 2: Override the Model","text":"<p>You can also override the model on the command line:</p> <pre><code>helm template qwen2-0-5b . --set model=Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre>"},{"location":"workloads/llm-inference-tgi/helm/#alternative-3-deploy-a-model-from-bucket-storage","title":"Alternative 3: Deploy a Model from Bucket Storage","text":"<p>If you have downloaded your model to bucket storage, use:</p> <pre><code>helm template qwen2-0-5b . --set model=s3://models/Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre> <p>The model will be automatically downloaded before starting the inference server.</p>"},{"location":"workloads/llm-inference-tgi/helm/#user-input-values","title":"User Input Values","text":"<p>Refer to the <code>values.yaml</code> file for the user input values you can provide, along with instructions.</p>"},{"location":"workloads/llm-inference-tgi/helm/#interacting-with-deployed-model","title":"Interacting with Deployed Model","text":""},{"location":"workloads/llm-inference-tgi/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment status:</p> <pre><code>kubectl get deployment\n</code></pre>"},{"location":"workloads/llm-inference-tgi/helm/#port-forwarding","title":"Port Forwarding","text":"<p>Forward the port to access the service (assuming the service is named <code>llm-inference-tgi-tiny-llama</code> ):</p> <pre><code>kubectl port-forward services/llm-inference-tgi-tiny-llama 8080:80\n</code></pre>"},{"location":"workloads/llm-inference-tgi/helm/#test-the-deployment","title":"Test the Deployment","text":"<p>Send a test request to verify the service, assuming <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code> model:</p> <pre><code>curl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -X POST \\\n    -d '{\n        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n        ]\n    }'\n</code></pre>"},{"location":"workloads/llm-inference-tgi/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/llm-inference-vllm/helm/","title":"LLM Inference with vLLM","text":"<p>This Helm Chart deploys the LLM Inference vLLM workload.</p>"},{"location":"workloads/llm-inference-vllm/helm/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following prerequisites are met before deploying any workloads:</p> <ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li>Secrets: Create the following secrets in the namespace:<ul> <li><code>minio-credentials</code> with keys <code>minio-access-key</code> and <code>minio-secret-key</code>.</li> <li><code>hf-token</code> with key <code>hf-token</code>.</li> </ul> </li> </ol>"},{"location":"workloads/llm-inference-vllm/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>It is recommended to use <code>helm template</code> and pipe the result to <code>kubectl create</code> , rather than using <code>helm install</code>. Generally, a command looks as follows</p> <pre><code>helm template [optional-release-name] &lt;helm-dir&gt; -f &lt;overrides/xyz.yaml&gt; --set &lt;name&gt;=&lt;value&gt; | kubectl apply -f -\n</code></pre> <p>The chart provides three main ways to deploy models, detailed below.</p>"},{"location":"workloads/llm-inference-vllm/helm/#alternative-1-deploy-a-specific-model-configuration","title":"Alternative 1: Deploy a Specific Model Configuration","text":"<p>To deploy a specific model along with its settings, use the following command from the <code>helm</code> directory:</p> <pre><code>helm template tiny-llama . -f overrides/models/tinyllama_tinyllama-1.1b-chat-v1.0.yaml | kubectl apply -f -\n</code></pre>"},{"location":"workloads/llm-inference-vllm/helm/#alternative-2-override-the-model","title":"Alternative 2: Override the Model","text":"<p>You can also override the model on the command line:</p> <pre><code>helm template qwen2-0-5b . --set model=Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre>"},{"location":"workloads/llm-inference-vllm/helm/#alternative-3-deploy-a-model-from-bucket-storage","title":"Alternative 3: Deploy a Model from Bucket Storage","text":"<p>If you have downloaded your model to bucket storage, use:</p> <pre><code>helm template qwen2-0-5b . --set model=s3://models/Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre> <p>The model will be automatically downloaded before starting the inference server.</p>"},{"location":"workloads/llm-inference-vllm/helm/#alternative-4-deploy-with-custom-served-model-name","title":"Alternative 4: Deploy with Custom Served Model Name","text":"<p>You can decouple the served model name from the storage path by using the <code>served_model_name</code> parameter:</p> <pre><code>helm template qwen2-0-5b . \\\n    --set model=s3://default-bucket/engineering/models/OdiaGenAI-LLM/qwen_1.5_odia_7b \\\n    --set served_model_name=OdiaGenAI-LLM/qwen_1.5_odia_7b | kubectl apply -f -\n</code></pre> <p>This allows you to use clean, user-friendly model names in your API requests while keeping the actual storage path separate.</p>"},{"location":"workloads/llm-inference-vllm/helm/#user-input-values","title":"User Input Values","text":"<p>Refer to the <code>values.yaml</code> file for the user input values you can provide, along with instructions.</p>"},{"location":"workloads/llm-inference-vllm/helm/#interacting-with-deployed-model","title":"Interacting with Deployed Model","text":""},{"location":"workloads/llm-inference-vllm/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment status:</p> <pre><code>kubectl get deployment\n</code></pre>"},{"location":"workloads/llm-inference-vllm/helm/#port-forwarding","title":"Port Forwarding","text":"<p>Forward the port to access the service (assuming the service is named <code>llm-inference-vllm-tiny-llama</code> ):</p> <pre><code>kubectl port-forward services/llm-inference-vllm-tiny-llama 8080:80\n</code></pre>"},{"location":"workloads/llm-inference-vllm/helm/#test-the-deployment","title":"Test the Deployment","text":"<p>Send a test request to verify the service, assuming <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code> model:</p> <pre><code>curl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -X POST \\\n    -d '{\n        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n        ]\n    }'\n</code></pre> <p>If you deployed with a custom <code>served_model_name</code>, use that name instead:</p> <pre><code>curl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"OdiaGenAI-LLM/qwen_1.5_odia_7b\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n        ]\n    }'\n</code></pre>"},{"location":"workloads/llm-inference-vllm/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/llm-inference-vllm-benchmark-mad/helm/","title":"LLM Inference Benchmarking Workload","text":"<p>This Helm chart submits a job to benchmark the performance of vLLM running a model in the same container.</p>"},{"location":"workloads/llm-inference-vllm-benchmark-mad/helm/#prerequisites","title":"Prerequisites","text":"<ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li> <p>MinIO Storage (optional): To use pre-downloaded model weights from MinIO storage, the following environment variables must be set, otherwise models will be downloaded from HuggingFace. MinIO storage is also used for saving benchmark results.</p> <ul> <li><code>BUCKET_STORAGE_HOST</code></li> <li><code>BUCKET_STORAGE_ACCESS_KEY</code></li> <li><code>BUCKET_STORAGE_SECRET_KEY</code></li> <li><code>BUCKET_MODEL_PATH</code></li> </ul> </li> <li> <p>HF Token (optional): If you need to download gated models from HuggingFace (e.g., Mistral and LLaMA 3.x) that are not available locally, ensure a secret named <code>hf-token</code> exists in the namespace.</p> </li> </ol>"},{"location":"workloads/llm-inference-vllm-benchmark-mad/helm/#implementation","title":"Implementation","text":"<p>Basic configurations are defined in the <code>values.yaml</code> file, with key settings:</p> <ul> <li><code>env_vars.TESTOPT</code>: Must be set to either \"latency\" or \"throughput\"</li> <li><code>env_vars.USE_MAD</code>: Controls whether to apply the MAD approach (see below)</li> </ul> <p>Note: If the specified model cannot be found locally, the workload will attempt to download it from HuggingFace.</p>"},{"location":"workloads/llm-inference-vllm-benchmark-mad/helm/#a-scenario-specific-approach","title":"A. Scenario-specific approach","text":"<p>In this approach (<code>env_vars.USE_SCENARIO</code> is not \"false\"), scenarios are defined in the <code>mount/scenarios_{$TESTOPT}.csv</code> file. Modify this file to specify models, parameters, and environment variables for benchmarking. Each column defines a parameter or variable, and each row represents a unique scenario to benchmark.</p> <p>The default configuration benchmarks latency using benchmark_latency.py from vLLM. Setting <code>env_vars.TESTOPT</code> to \"throughput\" will use benchmark_throughput.py instead.</p> <p>Example 1: Benchmark latency scenarios (default) <pre><code>helm template . | kubectl apply -f -\n</code></pre></p> <p>Example 2: Benchmark throughput scenarios <pre><code>helm template . --set env_vars.TESTOPT=\"throughput\" | kubectl apply -f -\n</code></pre></p>"},{"location":"workloads/llm-inference-vllm-benchmark-mad/helm/#b-rocmmad-standalone-approach","title":"B. ROCm/MAD standalone approach","text":"<p>When <code>env_vars.USE_MAD</code> is not \"false\", the ROCm/MAD repository will be cloned. The specified model (<code>env_vars.MAD_MODEL</code>) will be benchmarked according to preset scripts.</p> <p>Example 3: Benchmark using MAD standalone approach with override settings <pre><code>helm template . -f overrides/methods/MAD-Qwen2.5_0.5B.yaml | kubectl apply -f -\n</code></pre></p>"},{"location":"workloads/llm-inference-vllm-benchmark-mad/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/llm-inference-vllm-benchmark-rocmblog/helm/","title":"LLM Inference Benchmarking Workload (ROCm Best Practices)","text":"<p>This Helm chart deploys a job to benchmark the performance of vLLM running a model within the same container. It follows the best practices for optimized inference on AMD Instinct GPUs.</p>"},{"location":"workloads/llm-inference-vllm-benchmark-rocmblog/helm/#prerequisites","title":"Prerequisites","text":"<ol> <li>Helm: Ensure <code>helm</code> is installed. Refer to the Helm documentation for installation instructions.</li> <li> <p>MinIO Storage (optional): To use pre-downloaded model weights from MinIO storage, set the following environment variables. If not set, models will be downloaded from HuggingFace. MinIO storage is also used for saving benchmark results:</p> <ul> <li><code>BUCKET_STORAGE_HOST</code></li> <li><code>BUCKET_STORAGE_ACCESS_KEY</code></li> <li><code>BUCKET_STORAGE_SECRET_KEY</code></li> <li><code>BUCKET_MODEL_PATH</code></li> </ul> </li> <li> <p>HuggingFace Token (optional): Required for downloading gated models (e.g., Mistral and LLaMA 3.x) from HuggingFace if they are not available locally.</p> </li> </ol>"},{"location":"workloads/llm-inference-vllm-benchmark-rocmblog/helm/#implementation","title":"Implementation","text":"<p>Basic configurations are defined in the <code>values.yaml</code> file. YAML files in the <code>overrides/models/</code> directory can be used to reproduce benchmarks for specific scenarios, such as models, tensor parallelism, data types, quantization, etc.</p>"},{"location":"workloads/llm-inference-vllm-benchmark-rocmblog/helm/#example-benchmarking-a-specific-model-configuration","title":"Example: Benchmarking a Specific Model Configuration","text":"<p>To benchmark a specific model (e.g., Mistral-7B-Instruct-v0.3-FP8) with its settings, run the following command from the <code>helm</code> directory:</p> <pre><code>helm template . -f overrides/models/Mistral-7B-Instruct-v0.3-FP8.TP2.yaml | kubectl apply -f -\n</code></pre> <p>The benchmark results will be displayed at the end of the job log. An example result as the following:</p> <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     256\nBenchmark duration (s):                  63.26\nTotal input tokens:                      524288\nTotal generated tokens:                  524288\nRequest throughput (req/s):              4.05\nOutput token throughput (tok/s):         8287.48\nTotal Token throughput (tok/s):          16574.96\n---------------Time to First Token----------------\nMean TTFT (ms):                          5749.02\nMedian TTFT (ms):                        5569.11\nP99 TTFT (ms):                           10835.37\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          28.06\nMedian TPOT (ms):                        28.15\nP99 TPOT (ms):                           30.52\n---------------Inter-token Latency----------------\nMean ITL (ms):                           28.06\nMedian ITL (ms):                         25.17\nP99 ITL (ms):                            40.60\n----------------End-to-end Latency----------------\nMean E2EL (ms):                          63192.59\nMedian E2EL (ms):                        63190.51\nP99 E2EL (ms):                           63229.48\n==================================================\n</code></pre>"},{"location":"workloads/llm-inference-vllm-benchmark-rocmblog/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/llm-megatron-ckpt-conversion/helm/","title":"Helm Template for Converting Hugging Face Checkpoints to Megatron Format","text":"<p>This Helm template converts Hugging Face model checkpoints to the Megatron format. The workload interacts with MinIO storage to manage model artifacts. Specifically, it reads the pre-downloaded model checkpoints, converts them, and writes the converted artifacts back to the remote storage.</p>"},{"location":"workloads/llm-megatron-ckpt-conversion/helm/#prerequisites","title":"Prerequisites","text":"<p>Ensure that the tokenizer files are located in the same directory as the model files. For example, the directory <code>minio-host/default-bucket/models/meta-llama/Llama-3.1-8B/</code> should contain both the model checkpoints and files like <code>tokenizer.json</code>.</p>"},{"location":"workloads/llm-megatron-ckpt-conversion/helm/#usage","title":"Usage","text":"<p>To deploy the workload, use the following command:</p> <pre><code>helm template workloads/llm-megatron-ckpt-conversion/helm \\\n    --name-template llama3.1-8B \\\n    | kubectl create -f -\n</code></pre> <p>You can pass overrides to the default values as following: <pre><code>helm template workloads/llm-megatron-ckpt-conversion/helm \\\n    -f workloads/llm-megatron-ckpt-conversion/helm/overrides/values-70b.yaml\n    --name-template llama3.1-70B \\\n    | kubectl create -f -\n</code></pre></p>"},{"location":"workloads/llm-megatron-ckpt-conversion/helm/#note","title":"Note","text":"<ul> <li>The <code>helm install</code> command is designed for ongoing installations, not one-time jobs. Therefore, it is recommended to use <code>helm template</code> and pipe the output to <code>kubectl create</code>. This approach is more suitable for jobs that do not require modifying existing entities.</li> <li>Use <code>kubectl create</code> instead of <code>kubectl apply</code> for this job, as it is intended to create new resources without updating existing ones.</li> </ul>"},{"location":"workloads/llm-megatron-ckpt-conversion/helm/#configuration","title":"Configuration","text":"<p>Refer to the <code>values.yaml</code> file for configurable user input values. The file includes instructions to help you customize the workload as needed.</p>"},{"location":"workloads/llm-pretraining-megatron-lm/helm/","title":"Megatron-LM CPT templates","text":""},{"location":"workloads/llm-pretraining-megatron-lm/helm/#helm","title":"Helm","text":"<p>To generate manifests and print them in standard output using the default <code>values.yaml</code>, for example, run: <pre><code>helm template ./ --name-template 8b\n</code></pre> This will generate a kubernetes manifest with a job and a configmap both named <code>llm-pretraining-megatron-lm-job-8b</code> in the user's active namespace.</p> <p>To override the default values, a specific file can be passed using <code>-f</code> flag, for example: <pre><code>helm template ./ --name-template 70b -f overrides/values-llama-70b-cpt.yaml\n</code></pre></p> <p>Multiple overriding yaml files can be used e.g.: <pre><code>helm template ./ --name-template 70b \\\n    -f overrides/values-llama-70b-cpt.yaml \\\n    -f overrides/labels/kaiwo-managed-true.yaml\n</code></pre></p> <p>Note: Anything overlapping with the default <code>values.yaml</code> file can be omitted from the specific files passed with the <code>-f</code> flag.</p>"},{"location":"workloads/llm-pretraining-megatron-lm/helm/#running","title":"Running","text":"<p>To run the workload, simply pipe the generated manifests to a <code>kubectl</code> command, like:</p> <pre><code>helm template ./ --name-template 8b |\u00a0kubectl apply -f -\n</code></pre>"},{"location":"workloads/llm-pretraining-megatron-lm/helm/#docker-image","title":"Docker image","text":"<p>We recommend using the <code>rocm/megatron-lm:v25.4</code> image or later versions of it.</p> <p>Note: this workload has been tested with images: - <code>rocm/megatron-lm:v25.4</code></p>"},{"location":"workloads/llm-pretraining-megatron-lm/helm/#assumptions","title":"Assumptions","text":"<p>Some assumptions for running the pretraining jobs are as follows: The initial model checkpoint and data, both in Megatron format, are located in an S3-compatible storage. Additionally, it is assumed that a secret containing the S3 storage provider's HMAC credentials (access key and secret key) is present in the namespace where the jobs are executed. The defaults (as viewed from the Kubernetes manifest's perspective) are:</p> <pre><code>- name: BUCKET_STORAGE_ACCESS_KEY\n    valueFrom:\n    secretKeyRef:\n        name: minio-credentials\n        key: minio-access-key\n- name: BUCKET_STORAGE_SECRET_KEY\n    valueFrom:\n    secretKeyRef:\n        name: minio-credentials\n        key: minio-secret-key\n</code></pre>"},{"location":"workloads/llm-pretraining-megatron-lm/helm/#cleanup","title":"Cleanup","text":"<p>Note that this chart, when applied with <code>kubectl</code>, will create Job and ConfigMap objects. After the Job has finished, there is a 3600-second grace period to remove the Job object from the namespace. However, the cleanup of the ConfigMap must be done manually. For example, to delete the ConfigMap <code>llm-pretraining-megatron-lm-job-8b</code> from the user's active namespace:</p> <pre><code>kubectl delete configmap llm-pretraining-megatron-lm-job-8b\n</code></pre> <p>Alternatively one can use the garbage collection utility script <code>../utils/gc.sh</code> to manage cleanup automatically. After the workload is submitted to the kubernetes cluster run <code>../utils/gc.sh &lt;job-name&gt; &lt;namespace&gt;</code> to attach configmap to the lifecycle of the job object, e.g., for the above example:</p> <pre><code>../utils/gc.sh llm-pretraining-megatron-lm-job-8b &lt;ACTIVE_NAMESPACE&gt;\n</code></pre>"},{"location":"workloads/llm-pretraining-megatron-lm/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/local-resources/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/llm-pretraining-megatron-lm-ray/helm/","title":"LLM Pretraining Megatron LM Multinode","text":"<p>Workload for running Megatron based pretraining workloads on multiple nodes using ray</p>"},{"location":"workloads/llm-pretraining-megatron-lm-ray/helm/#helm","title":"Helm","text":"<p>To generate manifests and print them in standard output using the default <code>values.yaml</code>, run: <pre><code>helm template workloads/llm-pretraining-megatron-lm-ray/helm\n</code></pre></p> <p>This will generate a kubernetes manifest with a RayJob, a ConfigMap and a PersistentVolumeClaim resources in the user's active namespace.</p> <p>To override the default values, a specific file can be passed using <code>--values</code> flag <pre><code>helm template workloads/llm-pretraining-megatron-lm-ray/helm --values workloads/llm-pretraining-megatron-lm-ray/helm/overrides/values-llama-8b-2x2-4ddp.yaml\n</code></pre></p> <p>Note: Anything overlapping with the default <code>values.yaml</code> file can be omitted from the specific files passed with the <code>--values</code> flag.</p>"},{"location":"workloads/llm-pretraining-megatron-lm-ray/helm/#running","title":"Running","text":"<p>To run the workload, simply pipe the generated manifests to a <code>kubectl apply</code> command, like:</p> <pre><code>helm template workloads/llm-pretraining-megatron-lm-ray/helm |\u00a0kubectl apply -f -\n</code></pre>"},{"location":"workloads/llm-pretraining-megatron-lm-ray/helm/#docker-image","title":"Docker image","text":"<p>We recommend using the <code>ghcr.io/silogen/megatron-lm-ray</code> image. Ray dependecies are  included in the <code>megatron-lm-ray</code> image.</p>"},{"location":"workloads/llm-pretraining-megatron-lm-ray/helm/#assumptions","title":"Assumptions","text":"<p>Some assumptions for running the pretraining jobs are as follows: The initial model checkpoint and data, both in Megatron format, are located in an S3-compatible storage. Additionally, it is assumed that a secret containing the S3 storage provider's HMAC credentials (access key and secret key) is present in the namespace where the jobs are executed. The defaults (as viewed from the Kubernetes manifest's perspective) are:</p> <pre><code>- name: BUCKET_STORAGE_ACCESS_KEY\n    valueFrom:\n    secretKeyRef:\n        name: minio-credentials\n        key: minio-access-key\n- name: BUCKET_STORAGE_SECRET_KEY\n    valueFrom:\n    secretKeyRef:\n        name: minio-credentials\n        key: minio-secret-key\n</code></pre>"},{"location":"workloads/llm-pretraining-megatron-lm-ray/helm/#cleanup","title":"Cleanup","text":"<p>Note that this chart, when run with <code>kubectl apply</code>, will create RayJob, PersistentVolumeClaim and ConfigMap objects. After the RayJob has finished, there is a 3600-second grace period to remove the RayJob object from the namespace. ConfigMap and PersistentVolumeClaim are attached to the lifecycle of the RayJob at the start of the workload and cleaned up automatically. However, if there is an issue during start up of the workload, there can be a situation, when ConfigMap and PersistentVolumeClaim are created but are not owned by the RayJob. In this case ConfigMap and PersistentVolumeClaim resources should be cleaned up manually using <code>kubectl delete</code> command.</p>"},{"location":"workloads/llm-pretraining-megatron-lm-ray/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at /local_resources/mount.</p> <p>Note: Subdirectories and binary files are not supported.</p>"},{"location":"workloads/prepare-data-for-megatron-lm/helm/","title":"Workload helm template to download and preprocess data for Megatron-LM and upload it to bucket storage","text":"<p>This workload downloads data and a Huggingface tokenizer, preprocesses the data to the format accepted by Megatron-LM framework, and uploads it to bucket storage. To launch the workload, resolve the helm template with <code>helm template</code> and pipe the result to <code>kubectl apply</code>:</p> <p>Example: <pre><code>helm template workloads/prepare-data-for-megatron-lm/helm \\\n    -f workloads/prepare-data-for-megatron-lm/helm/overrides/fineweb-data-sample.yaml \\\n    --name-template prepare-fineweb-data \\\n    | kubectl apply -f -\n</code></pre></p>"},{"location":"workloads/prepare-data-for-megatron-lm/helm/#user-inputs","title":"User inputs","text":"<p>See the <code>values.yaml</code> file for all user input values that you can provide, with instructions.</p> <p>The data download logic should be implemented in a user-provided script under the key <code>dataScript</code>. The script must accept <code>--target-dir</code> as an argument, and output a JSONL file in the directory specified by that argument. The only requirement for the JSONL file is that the same json key in each line contains text paragraphs to be trained on. The specific json key that is used for extracting text data is specified as parameter for the next preprocessing step. For example, for <code>HuggingFaceFW/fineweb-edu</code> dataset this json key name is <code>text</code>.</p> <p>Further preprocessing logic that transforms JSONL data to the format accepted by Megatron-LM framwework is implemented in the <code>mount/prepare_data.sh</code> script. This script accepts, in particular, <code>json_key</code> as its 4-th argument, that points to the json key that stores training text data in JSONL file from previous step.</p> <p>At the moment we choose to ask the user to provide the data download script instead of just a few parameters like dataset identifier, because HuggingFace datasets don't have a standard format that can be always directly applied for every use case.</p>"},{"location":"workloads/rag-embedding-infinity/helm/","title":"LLM Inference with vLLM","text":"<p>This Helm Chart deploys the embedding inference (via infinity) workload.</p>"},{"location":"workloads/rag-embedding-infinity/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<pre><code>helm template [optional-release-name] &lt;helm-dir&gt; -f &lt;overrides/xyz.yaml&gt; --set &lt;name&gt;=&lt;value&gt; | kubectl apply -f -\n</code></pre>"},{"location":"workloads/rag-embedding-infinity/helm/#example-commands","title":"Example commands","text":"<p>Use default settings:</p> <pre><code>helm template e5-large-instruct . | kubectl apply -f -\n</code></pre> <p>Use custom model (that works with infinity):</p> <pre><code>helm template bilingual-embedding-large . --set model=Lajavaness/bilingual-embedding-large | kubectl apply -f -\n</code></pre> <p>The model will be automatically downloaded before starting the inference server.</p>"},{"location":"workloads/rag-embedding-infinity/helm/#user-input-values","title":"User Input Values","text":"<p>Refer to the <code>values.yaml</code> file for the user input values you can provide, along with instructions.</p>"},{"location":"workloads/rag-embedding-infinity/helm/#interacting-with-deployed-model","title":"Interacting with Deployed Model","text":""},{"location":"workloads/rag-embedding-infinity/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment status:</p> <pre><code>kubectl get deployment\n</code></pre>"},{"location":"workloads/rag-embedding-infinity/helm/#port-forwarding","title":"Port Forwarding","text":"<p>Forward the port to access the service:</p> <pre><code>kubectl port-forward services/rag-embedding-infinity-e5-large-instruct 7997:80\n</code></pre>"},{"location":"workloads/rag-embedding-infinity/helm/#test-the-deployment","title":"Test the Deployment","text":"<p>Infinity server UI will be accessible at http://0.0.0.0:7997/docs.</p> <p>Send a test request to verify the service:</p> <pre><code>curl -X POST http://0.0.0.0:7997/embeddings \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"model\":\"intfloat/multilingual-e5-large-instruct\",\"input\":[\"Two cute cats.\"]}'\n</code></pre>"},{"location":"workloads/rag-embedding-infinity/helm/mount/","title":"Index","text":"<p>Files in this directory are mounted to the workload at <code>/workload/mount</code>.</p> <p>Note: Subdirectories and binary files are not supported.</p>"}]}